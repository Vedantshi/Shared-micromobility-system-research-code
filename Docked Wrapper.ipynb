{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58b1acb",
   "metadata": {},
   "source": [
    "# **NEW JERSEY - CITY BIKE WRAPPER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6d10408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, Dict, Any, List\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_nj_docked_onecall_utilities(\n",
    "    *,\n",
    "    # âœ… ONLY INPUTS YOU WANT\n",
    "    station_status_txt: Union[str, Path],\n",
    "    station_information_csv: Optional[Union[str, Path]] = None,\n",
    "    station_information_url: str = \"https://gbfs.lyft.com/gbfs/2.3/bkn/en/station_information.json\",\n",
    "    trip_csv: Optional[Union[str, Path]] = None,\n",
    "    trip_csv_already_has_blocks: bool = False,\n",
    "\n",
    "    # time filters (keep as inputs because they change each run)\n",
    "    station_time_start: Optional[Union[str, pd.Timestamp]] = None,  # inclusive\n",
    "    station_time_end: Optional[Union[str, pd.Timestamp]] = None,    # exclusive\n",
    "    trip_time_start: Optional[Union[str, pd.Timestamp]] = None,     # inclusive\n",
    "    trip_time_end: Optional[Union[str, pd.Timestamp]] = None,       # inclusive\n",
    "\n",
    "    # output folder (relative ok)\n",
    "    output_dir: Union[str, Path] = \"NJ_OneCall\",\n",
    "\n",
    "    # switches\n",
    "    normalize: bool = True,\n",
    "    compute_safety: bool = True,\n",
    "    compute_idle: bool = True,\n",
    "    fill_missing_blocks_with_census_api: bool = True,\n",
    "    reuse_if_exists: bool = True,\n",
    "    save_outputs: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    NJ Docked \"one-call\" pipeline.\n",
    "\n",
    "    âœ… Only external inputs:\n",
    "      - station_status_txt (json-lines)\n",
    "      - station_information_csv OR station_information_url\n",
    "      - trip_csv (optional, enables usage + idle time)\n",
    "\n",
    "    ðŸ”’ Everything else is hardcoded to NJ paths inside this function.\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================\n",
    "    # ðŸ”’ HARD-CODED NJ PATHS (change once here, never in calls)\n",
    "    # ============================================================\n",
    "    NJ_CENSUS_BLOCKS_SHP = r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\GBFS_Census_Tract\\NJ\\tl_2024_34_tabblock20.shp\"\n",
    "    NJ_CENTROID_TRACT_CSV = r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\NJ\\centroid_tract_nj.csv\"\n",
    "    NJ_CENTERLINE_SHP = r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NJ\\Tran_road.shp\"\n",
    "    NJ_BIKE_LANES_SHP = r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NJ\\bike-lanes-2020-division-of-transportation.shp\"\n",
    "\n",
    "    # ============================================================\n",
    "    # Assumed schema (matches what you used)\n",
    "    # ============================================================\n",
    "    station_id_col = \"station_id\"\n",
    "    bikes_col = \"num_bikes_available\"\n",
    "    ebikes_col = \"num_ebikes_available\"\n",
    "    docks_col = \"num_docks_available\"\n",
    "\n",
    "    si_station_id_col = \"station_id\"\n",
    "    si_lat_col = \"lat\"\n",
    "    si_lon_col = \"lon\"\n",
    "    si_capacity_col = \"capacity\"\n",
    "\n",
    "    trip_start_lat_col = \"start_lat\"\n",
    "    trip_start_lon_col = \"start_lng\"\n",
    "    trip_end_lat_col = \"end_lat\"\n",
    "    trip_end_lon_col = \"end_lng\"\n",
    "    trip_started_at_col = \"started_at\"\n",
    "    trip_ended_at_col = \"ended_at\"\n",
    "    trip_start_block_col = \"start_census_block\"\n",
    "    trip_end_block_col = \"end_census_block\"\n",
    "\n",
    "    blocks_geoid_col = \"GEOID20\"\n",
    "    tract_digits = 11\n",
    "\n",
    "    block_time_granularity = \"5min\"\n",
    "    tract_time_granularity = \"1H\"\n",
    "\n",
    "    drop_station_status_cols_if_present = [\n",
    "        \"num_scooters_available\",\n",
    "        \"num_scooters_unavailable\",\n",
    "        \"is_installed\",\n",
    "        \"is_returning\",\n",
    "        \"is_renting\",\n",
    "    ]\n",
    "\n",
    "    # Census API params (your exact style)\n",
    "    census_benchmark = \"Public_AR_Census2020\"\n",
    "    census_vintage = \"2020\"\n",
    "    trip_latlon_round_decimals = 6\n",
    "\n",
    "    # Safety params\n",
    "    safety_working_epsg = 2263\n",
    "    bike_lane_type_col = \"type\"\n",
    "    protected_bike_lane_value = \"PROTECTED BIKE LANE\"\n",
    "\n",
    "    # ============================================================\n",
    "    # Setup outputs\n",
    "    # ============================================================\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    p_station_status_raw = output_dir / \"nj_station_status_raw.csv\"\n",
    "    p_station_info_raw = output_dir / \"nj_station_information_raw.csv\"\n",
    "    p_station_info_done = output_dir / \"nj_station_information_done.csv\"\n",
    "    p_station_status_done = output_dir / \"nj_station_status_done.csv\"\n",
    "\n",
    "    p_avail_block_5min = output_dir / \"availability_block_5min_nj.csv\"\n",
    "    p_avail_hourly_tract = output_dir / \"availability_hourly_tract_nj.csv\"\n",
    "    p_avail_norm_hourly_tract = output_dir / \"availability_norm_hourly_tract_nj.csv\"\n",
    "\n",
    "    p_capacity_block = output_dir / \"capacity_block_nj.csv\"\n",
    "    p_capacity_tract = output_dir / \"capacity_tract_nj.csv\"\n",
    "    p_capacity_norm_tract = output_dir / \"capacity_norm_tract_nj.csv\"\n",
    "\n",
    "    p_usage_5min_block = output_dir / \"usage_5min_block_nj.csv\"\n",
    "    p_usage_hourly_block = output_dir / \"usage_hourly_block_nj.csv\"\n",
    "    p_usage_hourly_tract = output_dir / \"usage_hourly_tract_nj.csv\"\n",
    "    p_usage_norm_hourly_tract = output_dir / \"usage_norm_hourly_tract_nj.csv\"\n",
    "\n",
    "    p_idle_block_hourly = output_dir / \"idle_time_block_hourly_nj.csv\"\n",
    "    p_idle_tract_hourly = output_dir / \"idle_time_tract_hourly_nj.csv\"\n",
    "    p_idle_norm_tract_hourly = output_dir / \"idle_time_norm_tract_hourly_nj.csv\"\n",
    "\n",
    "    p_safety_block = output_dir / \"safety_block_bike_lane_nj.csv\"\n",
    "    p_safety_tract = output_dir / \"safety_bike_lane_tract_nj.csv\"\n",
    "    p_safety_norm_tract = output_dir / \"safety_bike_lane_norm_tract_nj.csv\"\n",
    "\n",
    "    # ============================================================\n",
    "    # Helpers\n",
    "    # ============================================================\n",
    "    def _minmax_norm(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "        out = df.copy()\n",
    "        for c in cols:\n",
    "            mn = out[c].min()\n",
    "            mx = out[c].max()\n",
    "            if pd.isna(mn) or pd.isna(mx) or mx == mn:\n",
    "                out[c + \"_norm\"] = 0.0\n",
    "            else:\n",
    "                out[c + \"_norm\"] = (out[c] - mn) / (mx - mn)\n",
    "            out[c + \"_norm\"] = out[c + \"_norm\"].round(5)\n",
    "        return out\n",
    "\n",
    "    def _load_centroids() -> pd.DataFrame:\n",
    "        ct = pd.read_csv(NJ_CENTROID_TRACT_CSV, dtype={\"census_tract\": str})\n",
    "        if \"county_name\" not in ct.columns:\n",
    "            ct[\"county_name\"] = np.nan\n",
    "        ct[\"census_tract\"] = ct[\"census_tract\"].astype(str)\n",
    "        return ct[[\"census_tract\", \"county_name\"]].drop_duplicates()\n",
    "\n",
    "    # ============================================================\n",
    "    # A) Parse station status txt -> raw df\n",
    "    # ============================================================\n",
    "    if reuse_if_exists and p_station_status_raw.exists():\n",
    "        station_status_raw = pd.read_csv(p_station_status_raw)\n",
    "        station_status_raw[\"timestamp\"] = pd.to_datetime(station_status_raw[\"timestamp\"], errors=\"coerce\")\n",
    "    else:\n",
    "        station_status_txt = Path(station_status_txt)\n",
    "        with station_status_txt.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        rows: List[dict] = []\n",
    "        for line in lines:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                blob = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            ts = list(blob.keys())[0]\n",
    "            entries = blob[ts]\n",
    "\n",
    "            for entry in entries:\n",
    "                # flatten vehicle_types_available if present\n",
    "                if isinstance(entry, dict) and \"vehicle_types_available\" in entry:\n",
    "                    vta = entry.get(\"vehicle_types_available\", [])\n",
    "                    if isinstance(vta, list):\n",
    "                        for vt in vta:\n",
    "                            try:\n",
    "                                vid = vt[\"vehicle_type_id\"]\n",
    "                                cnt = vt[\"count\"]\n",
    "                                entry[f\"vehicle_type_{vid}_count\"] = cnt\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                    entry.pop(\"vehicle_types_available\", None)\n",
    "\n",
    "                entry[\"timestamp\"] = ts\n",
    "                rows.append(entry)\n",
    "\n",
    "        station_status_raw = pd.DataFrame(rows)\n",
    "\n",
    "        # remove \" EDT\" then parse\n",
    "        station_status_raw[\"timestamp\"] = station_status_raw[\"timestamp\"].astype(str).str.replace(\" EDT\", \"\", regex=False)\n",
    "        station_status_raw[\"timestamp\"] = pd.to_datetime(station_status_raw[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "        # drop noisy columns if present\n",
    "        drop_now = [c for c in drop_station_status_cols_if_present if c in station_status_raw.columns]\n",
    "        if drop_now:\n",
    "            station_status_raw = station_status_raw.drop(columns=drop_now)\n",
    "\n",
    "        if save_outputs:\n",
    "            station_status_raw.to_csv(p_station_status_raw, index=False)\n",
    "\n",
    "    if station_status_raw[\"timestamp\"].isna().any():\n",
    "        raise ValueError(\"Station status has unparseable timestamps (NaT). Fix the raw txt timestamps first.\")\n",
    "\n",
    "    # filter station status window\n",
    "    sts = station_status_raw.copy()\n",
    "    st_start = pd.to_datetime(station_time_start) if station_time_start is not None else None\n",
    "    st_end = pd.to_datetime(station_time_end) if station_time_end is not None else None\n",
    "    if st_start is not None:\n",
    "        sts = sts[sts[\"timestamp\"] >= st_start].copy()\n",
    "    if st_end is not None:\n",
    "        sts = sts[sts[\"timestamp\"] < st_end].copy()\n",
    "    if sts.empty:\n",
    "        raise ValueError(\"Station status is empty after station_time_start/station_time_end filtering.\")\n",
    "\n",
    "    # ============================================================\n",
    "    # B) Station info (csv or GBFS)\n",
    "    # ============================================================\n",
    "    if reuse_if_exists and p_station_info_raw.exists():\n",
    "        df_si_raw = pd.read_csv(p_station_info_raw)\n",
    "    else:\n",
    "        if station_information_csv is not None:\n",
    "            df_si_raw = pd.read_csv(station_information_csv)\n",
    "        else:\n",
    "            import requests\n",
    "            r = requests.get(station_information_url, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            js = r.json()\n",
    "            df_si_raw = pd.DataFrame(js[\"data\"][\"stations\"])\n",
    "\n",
    "        if save_outputs:\n",
    "            df_si_raw.to_csv(p_station_info_raw, index=False)\n",
    "\n",
    "    for c in [si_station_id_col, si_lat_col, si_lon_col, si_capacity_col]:\n",
    "        if c not in df_si_raw.columns:\n",
    "            raise ValueError(f\"Station information missing required column '{c}'.\")\n",
    "\n",
    "    # ============================================================\n",
    "    # C) Geocode station lat/lon -> blocks/tracts (spatial join + API fallback)\n",
    "    # ============================================================\n",
    "    if reuse_if_exists and p_station_info_done.exists():\n",
    "        df_si_done = pd.read_csv(p_station_info_done, dtype={\"census_block\": str})\n",
    "        df_si_done[\"census_block\"] = df_si_done[\"census_block\"].astype(str)\n",
    "        df_si_done[\"census_tract\"] = df_si_done[\"census_block\"].astype(str).str[:tract_digits]\n",
    "    else:\n",
    "        import geopandas as gpd\n",
    "        from shapely.geometry import Point\n",
    "\n",
    "        blocks = gpd.read_file(NJ_CENSUS_BLOCKS_SHP).to_crs(epsg=4326)\n",
    "        if blocks_geoid_col not in blocks.columns:\n",
    "            raise ValueError(f\"NJ block shapefile missing '{blocks_geoid_col}'.\")\n",
    "\n",
    "        latlon_df = df_si_raw.drop_duplicates(subset=[si_lat_col, si_lon_col])[[si_lat_col, si_lon_col]].copy()\n",
    "        latlon_df = latlon_df.rename(columns={si_lat_col: \"lat\", si_lon_col: \"lon\"})\n",
    "        pts = gpd.GeoDataFrame(\n",
    "            latlon_df,\n",
    "            geometry=[Point(xy) for xy in zip(latlon_df[\"lon\"], latlon_df[\"lat\"])],\n",
    "            crs=\"EPSG:4326\",\n",
    "        )\n",
    "\n",
    "        joined = gpd.sjoin(pts, blocks[[blocks_geoid_col, \"geometry\"]], how=\"left\", predicate=\"within\")\n",
    "        joined = joined.rename(columns={blocks_geoid_col: \"census_block\"})\n",
    "        joined[\"census_block\"] = joined[\"census_block\"].astype(str)\n",
    "        joined[\"census_tract\"] = joined[\"census_block\"].astype(str).str[:-4]\n",
    "\n",
    "        latlon_blocks = joined[[\"lat\", \"lon\", \"census_block\", \"census_tract\"]].copy()\n",
    "\n",
    "        if fill_missing_blocks_with_census_api:\n",
    "            import requests\n",
    "\n",
    "            missing = latlon_blocks[latlon_blocks[\"census_block\"].isin([\"nan\", \"None\"])][[\"lat\", \"lon\"]].drop_duplicates()\n",
    "            if not missing.empty:\n",
    "                def _get_block(lat: float, lon: float) -> Optional[str]:\n",
    "                    try:\n",
    "                        url = (\n",
    "                            \"https://geocoding.geo.census.gov/geocoder/geographies/coordinates\"\n",
    "                            f\"?x={lon}&y={lat}&benchmark={census_benchmark}&vintage={census_vintage}&format=json\"\n",
    "                        )\n",
    "                        resp = requests.get(url, timeout=30)\n",
    "                        resp.raise_for_status()\n",
    "                        js = resp.json()\n",
    "                        geos = js.get(\"result\", {}).get(\"geographies\", {})\n",
    "                        blocks_list = geos.get(\"Census Blocks\", []) or geos.get(\"2020 Census Blocks\", [])\n",
    "                        if blocks_list:\n",
    "                            return blocks_list[0].get(\"GEOID\")\n",
    "                        return None\n",
    "                    except Exception:\n",
    "                        return None\n",
    "\n",
    "                try:\n",
    "                    from tqdm import tqdm\n",
    "                    tqdm.pandas()\n",
    "                    missing[\"census_block_new\"] = missing.progress_apply(lambda r: _get_block(r[\"lat\"], r[\"lon\"]), axis=1)\n",
    "                except Exception:\n",
    "                    missing[\"census_block_new\"] = missing.apply(lambda r: _get_block(r[\"lat\"], r[\"lon\"]), axis=1)\n",
    "\n",
    "                missing[\"census_tract_new\"] = missing[\"census_block_new\"].apply(\n",
    "                    lambda x: str(x)[:-4] if pd.notna(x) and str(x) not in {\"nan\", \"None\"} else None\n",
    "                )\n",
    "\n",
    "                latlon_blocks = latlon_blocks.merge(missing, on=[\"lat\", \"lon\"], how=\"left\")\n",
    "                latlon_blocks[\"census_block\"] = latlon_blocks[\"census_block\"].replace({\"nan\": np.nan, \"None\": np.nan})\n",
    "                latlon_blocks[\"census_tract\"] = latlon_blocks[\"census_tract\"].replace({\"nan\": np.nan, \"None\": np.nan})\n",
    "                latlon_blocks[\"census_block\"] = latlon_blocks[\"census_block\"].fillna(latlon_blocks[\"census_block_new\"])\n",
    "                latlon_blocks[\"census_tract\"] = latlon_blocks[\"census_tract\"].fillna(latlon_blocks[\"census_tract_new\"])\n",
    "                latlon_blocks = latlon_blocks.drop(columns=[\"census_block_new\", \"census_tract_new\"])\n",
    "\n",
    "        df_si_done = df_si_raw.merge(\n",
    "            latlon_blocks,\n",
    "            left_on=[si_lat_col, si_lon_col],\n",
    "            right_on=[\"lat\", \"lon\"],\n",
    "            how=\"left\",\n",
    "        ).drop(columns=[\"lat\", \"lon\"])\n",
    "\n",
    "        df_si_done[\"census_block\"] = df_si_done[\"census_block\"].astype(str)\n",
    "        df_si_done[\"census_tract\"] = df_si_done[\"census_block\"].astype(str).str[:-4]\n",
    "\n",
    "        if save_outputs:\n",
    "            df_si_done.to_csv(p_station_info_done, index=False)\n",
    "\n",
    "    # ============================================================\n",
    "    # D) Merge station status + census_block\n",
    "    # ============================================================\n",
    "    if reuse_if_exists and p_station_status_done.exists():\n",
    "        status_done = pd.read_csv(p_station_status_done, dtype={\"census_block\": str, \"census_tract\": str})\n",
    "        status_done[\"timestamp\"] = pd.to_datetime(status_done[\"timestamp\"], errors=\"coerce\")\n",
    "    else:\n",
    "        if station_id_col not in sts.columns:\n",
    "            raise ValueError(f\"Station status missing '{station_id_col}'.\")\n",
    "        status_done = sts.merge(\n",
    "            df_si_done[[si_station_id_col, \"census_block\"]],\n",
    "            left_on=station_id_col,\n",
    "            right_on=si_station_id_col,\n",
    "            how=\"left\",\n",
    "        ).drop(columns=[si_station_id_col])\n",
    "\n",
    "        status_done[\"census_block\"] = status_done[\"census_block\"].astype(str)\n",
    "        status_done[\"census_tract\"] = status_done[\"census_block\"].astype(str).str[:-4]\n",
    "        status_done = status_done.sort_values([\"timestamp\", \"census_block\"]).reset_index(drop=True)\n",
    "\n",
    "        if save_outputs:\n",
    "            status_done.to_csv(p_station_status_done, index=False)\n",
    "\n",
    "    # ============================================================\n",
    "    # E) Availability (block 5-min + tract hourly avg + norm)\n",
    "    # ============================================================\n",
    "    for c in [bikes_col, ebikes_col, docks_col]:\n",
    "        if c not in status_done.columns:\n",
    "            raise ValueError(f\"Status done missing availability column '{c}'.\")\n",
    "\n",
    "    status_done[\"time_slot\"] = pd.to_datetime(status_done[\"timestamp\"]).dt.floor(block_time_granularity)\n",
    "    status_done[\"total_vehicle_available\"] = status_done[bikes_col] + status_done[ebikes_col]\n",
    "\n",
    "    availability_block_5min = (\n",
    "        status_done.groupby([\"census_block\", \"time_slot\"], as_index=False)[\n",
    "            [bikes_col, ebikes_col, docks_col, \"total_vehicle_available\"]\n",
    "        ].sum()\n",
    "        .sort_values([\"census_block\", \"time_slot\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    tract_5min = (\n",
    "        status_done.groupby([\"census_tract\", \"timestamp\"], as_index=False)[\n",
    "            [bikes_col, ebikes_col, docks_col, \"total_vehicle_available\"]\n",
    "        ].sum()\n",
    "    )\n",
    "    tract_5min[\"time_slot_hour\"] = pd.to_datetime(tract_5min[\"timestamp\"]).dt.floor(tract_time_granularity)\n",
    "\n",
    "    availability_hourly_tract = (\n",
    "        tract_5min.groupby([\"census_tract\", \"time_slot_hour\"], as_index=False)[\n",
    "            [bikes_col, ebikes_col, docks_col, \"total_vehicle_available\"]\n",
    "        ]\n",
    "        .mean()\n",
    "        .round(0)\n",
    "        .astype({bikes_col: int, ebikes_col: int, docks_col: int, \"total_vehicle_available\": int})\n",
    "        .rename(columns={\"time_slot_hour\": \"time_slot\"})\n",
    "        .sort_values([\"census_tract\", \"time_slot\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    availability_norm_hourly_tract = (\n",
    "        _minmax_norm(availability_hourly_tract, [bikes_col, ebikes_col, docks_col, \"total_vehicle_available\"])\n",
    "        if normalize\n",
    "        else availability_hourly_tract.copy()\n",
    "    )\n",
    "\n",
    "    centroids = _load_centroids()\n",
    "    availability_norm_hourly_tract = availability_norm_hourly_tract.merge(centroids, on=\"census_tract\", how=\"left\")\n",
    "\n",
    "    # ============================================================\n",
    "    # F) Capacity (block + tract + norm)\n",
    "    # ============================================================\n",
    "    capacity_block = (\n",
    "        df_si_done.groupby(\"census_block\", as_index=False)\n",
    "        .agg(total_capacity=(si_capacity_col, \"sum\"), num_station=(si_station_id_col, \"count\"))\n",
    "    )\n",
    "\n",
    "    cap_tract = (\n",
    "        df_si_done.groupby(\"census_tract\", as_index=False)\n",
    "        .agg(total_capacity=(si_capacity_col, \"sum\"), num_station=(si_station_id_col, \"count\"))\n",
    "    )\n",
    "    capacity_tract = centroids.merge(cap_tract, on=\"census_tract\", how=\"left\")\n",
    "    capacity_tract[[\"total_capacity\", \"num_station\"]] = capacity_tract[[\"total_capacity\", \"num_station\"]].fillna(0)\n",
    "\n",
    "    capacity_norm_tract = _minmax_norm(capacity_tract, [\"total_capacity\", \"num_station\"]) if normalize else capacity_tract.copy()\n",
    "\n",
    "    # ============================================================\n",
    "    # G) Usage (optional) + Idle time (optional)\n",
    "    # ============================================================\n",
    "    usage_5min_block = None\n",
    "    usage_hourly_block = None\n",
    "    usage_hourly_tract = None\n",
    "    usage_norm_hourly_tract = None\n",
    "\n",
    "    idle_time_block_hourly = None\n",
    "    idle_time_tract_hourly = None\n",
    "    idle_time_norm_tract_hourly = None\n",
    "\n",
    "    if trip_csv is not None:\n",
    "        trip_df = pd.read_csv(trip_csv, low_memory=False)\n",
    "\n",
    "        trip_df[trip_started_at_col] = pd.to_datetime(trip_df[trip_started_at_col], errors=\"coerce\")\n",
    "        trip_df[trip_ended_at_col] = pd.to_datetime(trip_df[trip_ended_at_col], errors=\"coerce\")\n",
    "        trip_df = trip_df.dropna(subset=[trip_started_at_col, trip_ended_at_col]).copy()\n",
    "\n",
    "        t_start = pd.to_datetime(trip_time_start) if trip_time_start is not None else trip_df[trip_started_at_col].min()\n",
    "        t_end = pd.to_datetime(trip_time_end) if trip_time_end is not None else trip_df[trip_started_at_col].max()\n",
    "        trip_df = trip_df[(trip_df[trip_started_at_col] >= t_start) & (trip_df[trip_started_at_col] <= t_end)].copy()\n",
    "\n",
    "        trip_df[\"trip_duration_min\"] = (trip_df[trip_ended_at_col] - trip_df[trip_started_at_col]).dt.total_seconds() / 60.0\n",
    "        trip_df = trip_df[trip_df[\"trip_duration_min\"] <= 240].copy()\n",
    "\n",
    "        # geocode to census block if needed\n",
    "        if not trip_csv_already_has_blocks:\n",
    "            needed = {trip_start_lat_col, trip_start_lon_col, trip_end_lat_col, trip_end_lon_col}\n",
    "            miss = needed - set(trip_df.columns)\n",
    "            if miss:\n",
    "                raise ValueError(f\"Trip CSV missing coord columns needed for geocoding: {miss}\")\n",
    "\n",
    "            import geopandas as gpd\n",
    "            from shapely.geometry import Point\n",
    "            import requests\n",
    "\n",
    "            blocks_gdf = gpd.read_file(NJ_CENSUS_BLOCKS_SHP).to_crs(epsg=4326)\n",
    "            blocks_gdf[blocks_geoid_col] = blocks_gdf[blocks_geoid_col].astype(str)\n",
    "\n",
    "            start_coords = trip_df[[trip_start_lat_col, trip_start_lon_col]].dropna().drop_duplicates().copy()\n",
    "            end_coords = trip_df[[trip_end_lat_col, trip_end_lon_col]].dropna().drop_duplicates().copy()\n",
    "\n",
    "            start_coords[\"geometry\"] = [Point(xy) for xy in zip(start_coords[trip_start_lon_col], start_coords[trip_start_lat_col])]\n",
    "            end_coords[\"geometry\"] = [Point(xy) for xy in zip(end_coords[trip_end_lon_col], end_coords[trip_end_lat_col])]\n",
    "\n",
    "            start_gdf = gpd.GeoDataFrame(start_coords, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "            end_gdf = gpd.GeoDataFrame(end_coords, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "            start_join = gpd.sjoin(start_gdf, blocks_gdf[[blocks_geoid_col, \"geometry\"]], how=\"left\", predicate=\"intersects\")\n",
    "            end_join = gpd.sjoin(end_gdf, blocks_gdf[[blocks_geoid_col, \"geometry\"]], how=\"left\", predicate=\"intersects\")\n",
    "\n",
    "            start_map = start_join[[trip_start_lat_col, trip_start_lon_col, blocks_geoid_col]].rename(columns={blocks_geoid_col: trip_start_block_col})\n",
    "            end_map = end_join[[trip_end_lat_col, trip_end_lon_col, blocks_geoid_col]].rename(columns={blocks_geoid_col: trip_end_block_col})\n",
    "\n",
    "            trip_df = trip_df.merge(start_map, on=[trip_start_lat_col, trip_start_lon_col], how=\"left\")\n",
    "            trip_df = trip_df.merge(end_map, on=[trip_end_lat_col, trip_end_lon_col], how=\"left\")\n",
    "\n",
    "            if fill_missing_blocks_with_census_api:\n",
    "                def _get_block(lat: float, lon: float) -> Optional[str]:\n",
    "                    try:\n",
    "                        url = (\n",
    "                            \"https://geocoding.geo.census.gov/geocoder/geographies/coordinates\"\n",
    "                            f\"?x={lon}&y={lat}&benchmark={census_benchmark}&vintage={census_vintage}&format=json\"\n",
    "                        )\n",
    "                        resp = requests.get(url, timeout=10)\n",
    "                        resp.raise_for_status()\n",
    "                        js = resp.json()\n",
    "                        geos = js.get(\"result\", {}).get(\"geographies\", {})\n",
    "                        blocks_list = geos.get(\"Census Blocks\", []) or geos.get(\"2020 Census Blocks\", [])\n",
    "                        if blocks_list:\n",
    "                            return blocks_list[0].get(\"GEOID\")\n",
    "                        return None\n",
    "                    except Exception:\n",
    "                        return None\n",
    "\n",
    "                miss_start = trip_df[trip_df[trip_start_block_col].isna()][[trip_start_lat_col, trip_start_lon_col]].dropna().copy()\n",
    "                miss_end = trip_df[trip_df[trip_end_block_col].isna()][[trip_end_lat_col, trip_end_lon_col]].dropna().copy()\n",
    "                miss_start = miss_start.rename(columns={trip_start_lat_col: \"lat\", trip_start_lon_col: \"lon\"})\n",
    "                miss_end = miss_end.rename(columns={trip_end_lat_col: \"lat\", trip_end_lon_col: \"lon\"})\n",
    "                miss = pd.concat([miss_start, miss_end], ignore_index=True).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "                miss[\"lat\"] = miss[\"lat\"].round(trip_latlon_round_decimals).astype(float)\n",
    "                miss[\"lon\"] = miss[\"lon\"].round(trip_latlon_round_decimals).astype(float)\n",
    "\n",
    "                try:\n",
    "                    from tqdm import tqdm\n",
    "                    tqdm.pandas()\n",
    "                    miss[\"census_block\"] = miss.progress_apply(lambda r: _get_block(r[\"lat\"], r[\"lon\"]), axis=1)\n",
    "                except Exception:\n",
    "                    miss[\"census_block\"] = miss.apply(lambda r: _get_block(r[\"lat\"], r[\"lon\"]), axis=1)\n",
    "\n",
    "                miss = miss.dropna(subset=[\"census_block\"]).copy()\n",
    "\n",
    "                trip_df[trip_start_lat_col] = trip_df[trip_start_lat_col].round(trip_latlon_round_decimals).astype(float)\n",
    "                trip_df[trip_start_lon_col] = trip_df[trip_start_lon_col].round(trip_latlon_round_decimals).astype(float)\n",
    "                trip_df = trip_df.merge(\n",
    "                    miss.rename(columns={\"lat\": trip_start_lat_col, \"lon\": trip_start_lon_col, \"census_block\": trip_start_block_col + \"_new\"}),\n",
    "                    on=[trip_start_lat_col, trip_start_lon_col],\n",
    "                    how=\"left\",\n",
    "                )\n",
    "                trip_df[trip_start_block_col] = trip_df[trip_start_block_col].fillna(trip_df[trip_start_block_col + \"_new\"])\n",
    "                trip_df = trip_df.drop(columns=[trip_start_block_col + \"_new\"])\n",
    "\n",
    "                trip_df[trip_end_lat_col] = trip_df[trip_end_lat_col].round(trip_latlon_round_decimals).astype(float)\n",
    "                trip_df[trip_end_lon_col] = trip_df[trip_end_lon_col].round(trip_latlon_round_decimals).astype(float)\n",
    "                trip_df = trip_df.merge(\n",
    "                    miss.rename(columns={\"lat\": trip_end_lat_col, \"lon\": trip_end_lon_col, \"census_block\": trip_end_block_col + \"_new\"}),\n",
    "                    on=[trip_end_lat_col, trip_end_lon_col],\n",
    "                    how=\"left\",\n",
    "                )\n",
    "                trip_df[trip_end_block_col] = trip_df[trip_end_block_col].fillna(trip_df[trip_end_block_col + \"_new\"])\n",
    "                trip_df = trip_df.drop(columns=[trip_end_block_col + \"_new\"])\n",
    "\n",
    "        # ========== usage 5min ==========\n",
    "        trip_df[\"time_slot\"] = trip_df[trip_started_at_col].dt.floor(block_time_granularity)\n",
    "        trip_df[\"time_slot_end\"] = trip_df[trip_ended_at_col].dt.floor(block_time_granularity)\n",
    "\n",
    "        trips_starting = (\n",
    "            trip_df.groupby([trip_start_block_col, \"time_slot\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"trips_starting\")\n",
    "            .rename(columns={trip_start_block_col: \"census_block\"})\n",
    "        )\n",
    "        trips_ending = (\n",
    "            trip_df.groupby([trip_end_block_col, \"time_slot_end\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"trips_ending\")\n",
    "            .rename(columns={trip_end_block_col: \"census_block\", \"time_slot_end\": \"time_slot\"})\n",
    "        )\n",
    "\n",
    "        all_blocks = pd.unique(pd.concat([trip_df[trip_start_block_col].astype(str), trip_df[trip_end_block_col].astype(str)]))\n",
    "        all_blocks = pd.Series(all_blocks).astype(str)\n",
    "        all_blocks = all_blocks[(all_blocks != \"nan\") & (all_blocks != \"0\")].unique()\n",
    "        all_slots = pd.date_range(start=t_start.floor(block_time_granularity), end=t_end.ceil(block_time_granularity), freq=block_time_granularity)\n",
    "        grid = pd.MultiIndex.from_product([all_blocks, all_slots], names=[\"census_block\", \"time_slot\"]).to_frame(index=False)\n",
    "\n",
    "        usage_5min_block = grid.merge(trips_starting, on=[\"census_block\", \"time_slot\"], how=\"left\")\n",
    "        usage_5min_block = usage_5min_block.merge(trips_ending, on=[\"census_block\", \"time_slot\"], how=\"left\")\n",
    "        usage_5min_block[[\"trips_starting\", \"trips_ending\"]] = usage_5min_block[[\"trips_starting\", \"trips_ending\"]].fillna(0).astype(int)\n",
    "        usage_5min_block = usage_5min_block.sort_values([\"census_block\", \"time_slot\"]).reset_index(drop=True)\n",
    "\n",
    "        # ========== usage hourly block ==========\n",
    "        trip_df[\"hour_start\"] = trip_df[trip_started_at_col].dt.floor(tract_time_granularity)\n",
    "        trip_df[\"hour_end\"] = trip_df[trip_ended_at_col].dt.floor(tract_time_granularity)\n",
    "\n",
    "        start_hr = (\n",
    "            trip_df.groupby([trip_start_block_col, \"hour_start\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"trips_starting\")\n",
    "            .rename(columns={trip_start_block_col: \"census_block\", \"hour_start\": \"time_slot\"})\n",
    "        )\n",
    "        end_hr = (\n",
    "            trip_df.groupby([trip_end_block_col, \"hour_end\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"trips_ending\")\n",
    "            .rename(columns={trip_end_block_col: \"census_block\", \"hour_end\": \"time_slot\"})\n",
    "        )\n",
    "\n",
    "        all_hours = pd.date_range(start=t_start.floor(tract_time_granularity), end=t_end.ceil(tract_time_granularity), freq=tract_time_granularity)\n",
    "        grid_hr = pd.MultiIndex.from_product([all_blocks, all_hours], names=[\"census_block\", \"time_slot\"]).to_frame(index=False)\n",
    "\n",
    "        usage_hourly_block = grid_hr.merge(start_hr, on=[\"census_block\", \"time_slot\"], how=\"left\").merge(end_hr, on=[\"census_block\", \"time_slot\"], how=\"left\")\n",
    "        usage_hourly_block[[\"trips_starting\", \"trips_ending\"]] = usage_hourly_block[[\"trips_starting\", \"trips_ending\"]].fillna(0).astype(int)\n",
    "        usage_hourly_block = usage_hourly_block.sort_values([\"census_block\", \"time_slot\"]).reset_index(drop=True)\n",
    "\n",
    "        # ========== usage hourly tract ==========\n",
    "        usage_hourly_tract = usage_hourly_block.copy()\n",
    "        usage_hourly_tract[\"census_tract\"] = usage_hourly_tract[\"census_block\"].astype(str).str[:tract_digits]\n",
    "        usage_hourly_tract = (\n",
    "            usage_hourly_tract.groupby([\"census_tract\", \"time_slot\"], as_index=False)[[\"trips_starting\", \"trips_ending\"]]\n",
    "            .sum()\n",
    "            .sort_values([\"census_tract\", \"time_slot\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        usage_norm_hourly_tract = _minmax_norm(usage_hourly_tract, [\"trips_starting\", \"trips_ending\"]) if normalize else usage_hourly_tract.copy()\n",
    "\n",
    "        # ========== idle time (optional) ==========\n",
    "        if compute_idle:\n",
    "            from collections import deque\n",
    "\n",
    "            station_avail = (\n",
    "                status_done.groupby([\"census_block\", \"time_slot\"], as_index=False)[[bikes_col, ebikes_col, \"total_vehicle_available\"]]\n",
    "                .sum()\n",
    "            )\n",
    "            station_avail[\"time_slot\"] = pd.to_datetime(station_avail[\"time_slot\"]) - pd.Timedelta(minutes=5)\n",
    "\n",
    "            merged = station_avail.merge(usage_5min_block, on=[\"census_block\", \"time_slot\"], how=\"left\")\n",
    "            merged[[\"trips_starting\", \"trips_ending\"]] = merged[[\"trips_starting\", \"trips_ending\"]].fillna(0).astype(int)\n",
    "            merged[\"vehicles_moved\"] = merged[\"trips_starting\"] + merged[\"trips_ending\"]\n",
    "            merged[\"idle_vehicles\"] = (merged[\"total_vehicle_available\"] - merged[\"vehicles_moved\"]).clip(lower=0).astype(int)\n",
    "\n",
    "            merged[\"hour\"] = pd.to_datetime(merged[\"time_slot\"]).dt.floor(tract_time_granularity)\n",
    "            merged = merged.sort_values([\"census_block\", \"time_slot\"]).reset_index(drop=True)\n",
    "\n",
    "            idle_rows: List[dict] = []\n",
    "            for block in merged[\"census_block\"].unique():\n",
    "                bdf = merged[merged[\"census_block\"] == block]\n",
    "                for hour in bdf[\"hour\"].unique():\n",
    "                    hdf = bdf[bdf[\"hour\"] == hour].copy().sort_values(\"time_slot\")\n",
    "\n",
    "                    q = deque()\n",
    "                    finalized: List[float] = []\n",
    "\n",
    "                    for _, r in hdf.iterrows():\n",
    "                        t = r[\"time_slot\"]\n",
    "                        cur_idle = int(r[\"idle_vehicles\"])\n",
    "\n",
    "                        if cur_idle > len(q):\n",
    "                            for _ in range(cur_idle - len(q)):\n",
    "                                q.append(t)\n",
    "                        elif cur_idle < len(q):\n",
    "                            for _ in range(len(q) - cur_idle):\n",
    "                                stt = q.popleft()\n",
    "                                finalized.append((t - stt).total_seconds() / 60.0)\n",
    "\n",
    "                    final_t = pd.to_datetime(hour) + pd.Timedelta(hours=1)\n",
    "                    while q:\n",
    "                        stt = q.popleft()\n",
    "                        finalized.append((final_t - stt).total_seconds() / 60.0)\n",
    "\n",
    "                    avg_idle = round(float(np.mean(finalized)), 2) if finalized else 0.0\n",
    "                    idle_rows.append({\"census_block\": block, \"time_slot\": pd.to_datetime(hour), \"avg_idle_time\": avg_idle})\n",
    "\n",
    "            idle_time_block_hourly = pd.DataFrame(idle_rows).sort_values([\"census_block\", \"time_slot\"]).reset_index(drop=True)\n",
    "\n",
    "            idle_time_tract_hourly = idle_time_block_hourly.copy()\n",
    "            idle_time_tract_hourly[\"census_tract\"] = idle_time_tract_hourly[\"census_block\"].astype(str).str[:tract_digits]\n",
    "            idle_time_tract_hourly = (\n",
    "                idle_time_tract_hourly.groupby([\"census_tract\", \"time_slot\"], as_index=False)[\"avg_idle_time\"]\n",
    "                .mean()\n",
    "                .sort_values([\"census_tract\", \"time_slot\"])\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "            idle_time_norm_tract_hourly = _minmax_norm(idle_time_tract_hourly, [\"avg_idle_time\"]) if normalize else idle_time_tract_hourly.copy()\n",
    "\n",
    "    # ============================================================\n",
    "    # H) Safety (optional)\n",
    "    # ============================================================\n",
    "    safety_block = None\n",
    "    safety_tract = None\n",
    "    safety_norm_tract = None\n",
    "\n",
    "    if compute_safety:\n",
    "        import geopandas as gpd\n",
    "\n",
    "        blocks = gpd.read_file(NJ_CENSUS_BLOCKS_SHP).to_crs(epsg=int(safety_working_epsg))\n",
    "        streets = gpd.read_file(NJ_CENTERLINE_SHP).to_crs(epsg=int(safety_working_epsg))\n",
    "        lanes = gpd.read_file(NJ_BIKE_LANES_SHP).to_crs(epsg=int(safety_working_epsg))\n",
    "\n",
    "        streets_in_blocks = gpd.overlay(streets, blocks, how=\"intersection\")\n",
    "        streets_in_blocks[\"segment_length\"] = streets_in_blocks.geometry.length\n",
    "        street_len = (\n",
    "            streets_in_blocks.groupby(blocks_geoid_col, as_index=False)\n",
    "            .agg(streets_leng=(\"segment_length\", \"sum\"))\n",
    "            .rename(columns={blocks_geoid_col: \"census_block\"})\n",
    "        )\n",
    "        street_len[\"streets_leng\"] = street_len[\"streets_leng\"].round(3)\n",
    "\n",
    "        lanes_in_blocks = gpd.overlay(lanes, blocks, how=\"intersection\")\n",
    "        lanes_in_blocks[\"bike_lane_length\"] = lanes_in_blocks.geometry.length\n",
    "        lane_len = (\n",
    "            lanes_in_blocks.groupby(blocks_geoid_col, as_index=False)\n",
    "            .agg(total_bike_lane_length=(\"bike_lane_length\", \"sum\"))\n",
    "            .rename(columns={blocks_geoid_col: \"census_block\"})\n",
    "        )\n",
    "        lane_len[\"total_bike_lane_length\"] = lane_len[\"total_bike_lane_length\"].round(3)\n",
    "\n",
    "        prot_len = pd.DataFrame({\"census_block\": [], \"protected_bike_lane_length\": []})\n",
    "        if bike_lane_type_col in lanes.columns:\n",
    "            protected = lanes[lanes[bike_lane_type_col] == protected_bike_lane_value].copy()\n",
    "            if not protected.empty:\n",
    "                prot_in_blocks = gpd.overlay(protected, blocks, how=\"intersection\")\n",
    "                prot_in_blocks[\"protected_lane_length\"] = prot_in_blocks.geometry.length\n",
    "                prot_len = (\n",
    "                    prot_in_blocks.groupby(blocks_geoid_col, as_index=False)\n",
    "                    .agg(protected_bike_lane_length=(\"protected_lane_length\", \"sum\"))\n",
    "                    .rename(columns={blocks_geoid_col: \"census_block\"})\n",
    "                )\n",
    "                prot_len[\"protected_bike_lane_length\"] = prot_len[\"protected_bike_lane_length\"].round(3)\n",
    "\n",
    "        safety_block = (\n",
    "            street_len.merge(lane_len, on=\"census_block\", how=\"left\")\n",
    "            .merge(prot_len, on=\"census_block\", how=\"left\")\n",
    "        )\n",
    "        safety_block[\"total_bike_lane_length\"] = safety_block[\"total_bike_lane_length\"].fillna(0.0)\n",
    "        safety_block[\"protected_bike_lane_length\"] = safety_block[\"protected_bike_lane_length\"].fillna(0.0)\n",
    "\n",
    "        safety_block[\"bike_lane_ratio\"] = safety_block.apply(\n",
    "            lambda r: (r[\"total_bike_lane_length\"] / r[\"streets_leng\"]) if r[\"streets_leng\"] > 0 else 0.0,\n",
    "            axis=1,\n",
    "        ).round(3)\n",
    "        safety_block[\"protected_bike_lane_ratio\"] = safety_block.apply(\n",
    "            lambda r: (r[\"protected_bike_lane_length\"] / r[\"streets_leng\"]) if r[\"streets_leng\"] > 0 else 0.0,\n",
    "            axis=1,\n",
    "        ).round(3)\n",
    "\n",
    "        safety_block[\"census_block\"] = safety_block[\"census_block\"].astype(str)\n",
    "        safety_block[\"census_tract\"] = safety_block[\"census_block\"].str[:tract_digits]\n",
    "\n",
    "        tract_sums = (\n",
    "            safety_block.groupby(\"census_tract\", as_index=False)[\n",
    "                [\"streets_leng\", \"total_bike_lane_length\", \"protected_bike_lane_length\"]\n",
    "            ].sum()\n",
    "        )\n",
    "\n",
    "        safety_tract = centroids.merge(tract_sums, on=\"census_tract\", how=\"left\")\n",
    "        safety_tract[[\"streets_leng\", \"total_bike_lane_length\", \"protected_bike_lane_length\"]] = (\n",
    "            safety_tract[[\"streets_leng\", \"total_bike_lane_length\", \"protected_bike_lane_length\"]].fillna(0.0)\n",
    "        )\n",
    "        safety_tract[\"bike_lane_ratio\"] = safety_tract.apply(\n",
    "            lambda r: (r[\"total_bike_lane_length\"] / r[\"streets_leng\"]) if r[\"streets_leng\"] > 0 else 0.0,\n",
    "            axis=1,\n",
    "        )\n",
    "        safety_tract[\"protected_bike_lane_ratio\"] = safety_tract.apply(\n",
    "            lambda r: (r[\"protected_bike_lane_length\"] / r[\"streets_leng\"]) if r[\"streets_leng\"] > 0 else 0.0,\n",
    "            axis=1,\n",
    "        )\n",
    "        safety_tract = safety_tract.sort_values(\"census_tract\").reset_index(drop=True)\n",
    "        safety_norm_tract = _minmax_norm(safety_tract, [\"bike_lane_ratio\", \"protected_bike_lane_ratio\"]) if normalize else safety_tract.copy()\n",
    "\n",
    "    # ============================================================\n",
    "    # Save outputs\n",
    "    # ============================================================\n",
    "    if save_outputs:\n",
    "        availability_block_5min.to_csv(p_avail_block_5min, index=False)\n",
    "        availability_hourly_tract.to_csv(p_avail_hourly_tract, index=False)\n",
    "        availability_norm_hourly_tract.to_csv(p_avail_norm_hourly_tract, index=False)\n",
    "\n",
    "        capacity_block.to_csv(p_capacity_block, index=False)\n",
    "        capacity_tract.to_csv(p_capacity_tract, index=False)\n",
    "        capacity_norm_tract.to_csv(p_capacity_norm_tract, index=False)\n",
    "\n",
    "        if usage_5min_block is not None:\n",
    "            usage_5min_block.to_csv(p_usage_5min_block, index=False)\n",
    "        if usage_hourly_block is not None:\n",
    "            usage_hourly_block.to_csv(p_usage_hourly_block, index=False)\n",
    "        if usage_hourly_tract is not None:\n",
    "            usage_hourly_tract.to_csv(p_usage_hourly_tract, index=False)\n",
    "        if usage_norm_hourly_tract is not None:\n",
    "            usage_norm_hourly_tract.to_csv(p_usage_norm_hourly_tract, index=False)\n",
    "\n",
    "        if idle_time_block_hourly is not None:\n",
    "            idle_time_block_hourly.to_csv(p_idle_block_hourly, index=False)\n",
    "        if idle_time_tract_hourly is not None:\n",
    "            idle_time_tract_hourly.to_csv(p_idle_tract_hourly, index=False)\n",
    "        if idle_time_norm_tract_hourly is not None:\n",
    "            idle_time_norm_tract_hourly.to_csv(p_idle_norm_tract_hourly, index=False)\n",
    "\n",
    "        if safety_block is not None:\n",
    "            safety_block.to_csv(p_safety_block, index=False)\n",
    "        if safety_tract is not None:\n",
    "            safety_tract.to_csv(p_safety_tract, index=False)\n",
    "        if safety_norm_tract is not None:\n",
    "            safety_norm_tract.to_csv(p_safety_norm_tract, index=False)\n",
    "\n",
    "    return {\n",
    "        \"station_status_raw_df\": station_status_raw,\n",
    "        \"station_information_raw_df\": df_si_raw,\n",
    "        \"station_information_done_df\": df_si_done,\n",
    "        \"station_status_done_df\": status_done,\n",
    "\n",
    "        \"availability_block_5min_df\": availability_block_5min,\n",
    "        \"availability_hourly_tract_df\": availability_hourly_tract,\n",
    "        \"availability_norm_hourly_tract_df\": availability_norm_hourly_tract,\n",
    "\n",
    "        \"capacity_block_df\": capacity_block,\n",
    "        \"capacity_tract_df\": capacity_tract,\n",
    "        \"capacity_norm_tract_df\": capacity_norm_tract,\n",
    "\n",
    "        \"usage_5min_block_df\": usage_5min_block,\n",
    "        \"usage_hourly_block_df\": usage_hourly_block,\n",
    "        \"usage_hourly_tract_df\": usage_hourly_tract,\n",
    "        \"usage_norm_hourly_tract_df\": usage_norm_hourly_tract,\n",
    "\n",
    "        \"idle_time_block_hourly_df\": idle_time_block_hourly,\n",
    "        \"idle_time_tract_hourly_df\": idle_time_tract_hourly,\n",
    "        \"idle_time_norm_tract_hourly_df\": idle_time_norm_tract_hourly,\n",
    "\n",
    "        \"safety_block_df\": safety_block,\n",
    "        \"safety_tract_df\": safety_tract,\n",
    "        \"safety_norm_tract_df\": safety_norm_tract,\n",
    "\n",
    "        \"paths\": {k: str(v) for k, v in {\n",
    "            \"availability_norm_hourly_tract_csv\": p_avail_norm_hourly_tract,\n",
    "            \"station_status_done_csv\": p_station_status_done,\n",
    "            \"station_information_done_csv\": p_station_info_done,\n",
    "            \"capacity_norm_tract_csv\": p_capacity_norm_tract,\n",
    "            \"usage_norm_hourly_tract_csv\": p_usage_norm_hourly_tract,\n",
    "            \"idle_time_norm_tract_hourly_csv\": p_idle_norm_tract_hourly,\n",
    "            \"safety_bike_lane_norm_tract_csv\": p_safety_norm_tract,\n",
    "        }.items()},\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe05b1",
   "metadata": {},
   "source": [
    "## **CALLING THE FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29c98064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_41700\\2645987263.py:375: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  tract_5min[\"time_slot_hour\"] = pd.to_datetime(tract_5min[\"timestamp\"]).dt.floor(tract_time_granularity)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [01:28<00:00,  1.14it/s]\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_41700\\2645987263.py:558: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  trip_df[\"hour_start\"] = trip_df[trip_started_at_col].dt.floor(tract_time_granularity)\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_41700\\2645987263.py:559: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  trip_df[\"hour_end\"] = trip_df[trip_ended_at_col].dt.floor(tract_time_granularity)\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_41700\\2645987263.py:574: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  all_hours = pd.date_range(start=t_start.floor(tract_time_granularity), end=t_end.ceil(tract_time_granularity), freq=tract_time_granularity)\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_41700\\2645987263.py:607: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  merged[\"hour\"] = pd.to_datetime(merged[\"time_slot\"]).dt.floor(tract_time_granularity)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NJ_OneCall\\availability_norm_hourly_tract_nj.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Example call (now only 3 inputs + optional time windows)\n",
    "# ----------------------------\n",
    "out = compute_nj_docked_onecall_utilities(\n",
    "    station_status_txt=r\"D:\\Research Fellowship\\Summer Research Stuff\\Collected Data\\Week 1\\09-June\\nyc_docked_station_status_6_9.txt\",\n",
    "    station_information_csv=r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\GBFS_Census_Tract\\NJ\\NJ station information 06_09.csv\",  # or pass a local CSV if you prefer\n",
    "    trip_csv=r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Usage\\NJ\\JC-202505-citibike-tripdata.csv\",\n",
    "    trip_csv_already_has_blocks=False,\n",
    "    station_time_start=\"2025-06-09 06:00:00\",\n",
    "    station_time_end=\"2025-06-09 12:00:00\",\n",
    "    trip_time_start=\"2025-05-01 00:00:00\",\n",
    "    trip_time_end=\"2025-05-31 23:59:59\",\n",
    "    output_dir=\"NJ_OneCall\",\n",
    ")\n",
    "print(out[\"paths\"][\"availability_norm_hourly_tract_csv\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70d4742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------\n",
    "# # Example call (NJ docked)\n",
    "# # ----------------------------\n",
    "# out = compute_nj_docked_onecall_utilities(\n",
    "#     station_status_txt=r\"C:\\Research Fellowship\\Collected Data\\Week 1\\09-June\\nj_docked_station_status_6_9.txt\",\n",
    "#     census_blocks_shp=r\"C:\\Research Fellowship\\Clean_Utilities\\Safety\\NJ\\tl_2024_34_tabblock20.shp\",\n",
    "#     centroid_tract_csv=r\"C:\\Research Fellowship\\Clean_Utilities\\Capacity\\NJ\\centroid_tract_nj.csv\",\n",
    "#     output_dir=\"NJ_OneCall\",\n",
    "\n",
    "#     # use either:\n",
    "#     # station_information_csv=r\"C:\\...\\NJ station information 06_09.csv\",\n",
    "#     # OR let it fetch from GBFS:\n",
    "#     station_information_url=\"https://gbfs.lyft.com/gbfs/2.3/bkn/en/station_information.json\",\n",
    "\n",
    "#     # Optional trips (enables usage + idle time):\n",
    "#     trip_csv=r\"C:\\Research Fellowship\\Clean_Utilities\\Usage\\NJ\\JC-202505-citibike-tripdata.csv\",\n",
    "#     trip_csv_already_has_blocks=False,\n",
    "\n",
    "#     station_time_start=\"2025-06-09 06:00:00\",\n",
    "#     station_time_end=\"2025-06-09 12:00:00\",\n",
    "#     trip_time_start=\"2025-05-01 00:00:00\",\n",
    "#     trip_time_end=\"2025-05-31 23:59:59\",\n",
    "\n",
    "#     # If you ever need to drop leak-in state blocks:\n",
    "#     # drop_census_prefixes_in_station=[\"36\"],\n",
    "#     # drop_census_prefixes_in_trip=[\"36\"],\n",
    "\n",
    "#     # Optional safety:\n",
    "#     centerline_streets_shp=r\"C:\\Research Fellowship\\Clean_Utilities\\Safety\\NJ\\Tran_road.shp\",\n",
    "#     bike_lanes_shp=r\"C:\\Research Fellowship\\Clean_Utilities\\Safety\\NJ\\bike-lanes-2020-division-of-transportation.shp\",\n",
    "#     safety_working_epsg=2263,\n",
    "# )\n",
    "# print(out[\"paths\"][\"availability_norm_hourly_tract_csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4051ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------\n",
    "# # Example call (NJ docked)\n",
    "# # ----------------------------\n",
    "# out = compute_nj_docked_onecall_utilities(\n",
    "#     station_status_txt=r\"C:\\Research Fellowship\\Collected Data\\Week 1\\09-June\\nj_docked_station_status_6_9.txt\",\n",
    "#     census_blocks_shp=r\"C:\\Research Fellowship\\Clean_Utilities\\Safety\\NJ\\tl_2024_34_tabblock20.shp\",\n",
    "#     centroid_tract_csv=r\"C:\\Research Fellowship\\Clean_Utilities\\Capacity\\NJ\\centroid_tract_nj.csv\",\n",
    "#     output_dir=\"NJ_OneCall\",\n",
    "\n",
    "#     # use either:\n",
    "#     # station_information_csv=r\"C:\\...\\NJ station information 06_09.csv\",\n",
    "#     # OR let it fetch from GBFS:\n",
    "#     station_information_url=\"https://gbfs.lyft.com/gbfs/2.3/bkn/en/station_information.json\",\n",
    "\n",
    "#     # Optional trips (enables usage + idle time):\n",
    "#     trip_csv=r\"C:\\Research Fellowship\\Clean_Utilities\\Usage\\NJ\\JC-202505-citibike-tripdata.csv\",\n",
    "#     trip_csv_already_has_blocks=False,\n",
    "\n",
    "#     station_time_start=\"2025-06-09 06:00:00\",\n",
    "#     station_time_end=\"2025-06-09 12:00:00\",\n",
    "#     trip_time_start=\"2025-05-01 00:00:00\",\n",
    "#     trip_time_end=\"2025-05-31 23:59:59\",\n",
    "\n",
    "#     # If you ever need to drop leak-in state blocks:\n",
    "#     # drop_census_prefixes_in_station=[\"36\"],\n",
    "#     # drop_census_prefixes_in_trip=[\"36\"],\n",
    "\n",
    "#     # Optional safety:\n",
    "#     centerline_streets_shp=r\"C:\\Research Fellowship\\Clean_Utilities\\Safety\\NJ\\Tran_road.shp\",\n",
    "#     bike_lanes_shp=r\"C:\\Research Fellowship\\Clean_Utilities\\Safety\\NJ\\bike-lanes-2020-division-of-transportation.shp\",\n",
    "#     safety_working_epsg=2263,\n",
    "# )\n",
    "# print(out[\"paths\"][\"availability_norm_hourly_tract_csv\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f84b3d",
   "metadata": {},
   "source": [
    "# **Version  2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00eecb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, Dict, Any, Sequence, Tuple\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# CITY CONFIG (hardcoded assets for NYC + NJ + PITT)\n",
    "# ==================================================\n",
    "CITY_CONFIG: Dict[str, Dict[str, Any]] = {\n",
    "    \"NYC\": {\n",
    "        \"census_blocks_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\GBFS_Census_Tract\\NYC\\tl_2024_36_tabblock20.shp\"\n",
    "        ),\n",
    "        \"tracts_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\NYC\\tl_2024_36_tract.shp\"\n",
    "        ),\n",
    "        \"centroid_tract_csv\": Path(\n",
    "            r\"D:\\Research Fellowship\\Capacity_NYC\\centroid_tract_computed.csv\"\n",
    "        ),\n",
    "        # safety (NYC = CSV WKT)\n",
    "        \"centerline_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NYC\\CSCL_PlowNYC_20250619.csv\"\n",
    "        ),\n",
    "        \"bike_lanes_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NYC\\New_York_City_Bike_Routes_20250619.csv\"\n",
    "        ),\n",
    "        \"safety_input_kind\": \"csv_wkt\",\n",
    "        \"safety_csv_wkt_candidates\": (\"the_geom\", \"wkt\", \"geometry_wkt\", \"WKT\", \"geom\"),\n",
    "        \"safety_csv_crs\": \"EPSG:4326\",\n",
    "        \"blocks_id_col\": \"GEOID20\",\n",
    "        \"tract_id_col\": \"GEOID\",\n",
    "        \"external_tract_prefix\": \"34\",  # drop NJ tracts for NYC results\n",
    "        \"has_boro_logic\": True,\n",
    "        \"drop_staten_island_default\": True,\n",
    "        \"protected_lane_rule\": {\n",
    "            \"col_candidates\": (\"facilitycl\", \"facility\", \"class\", \"ft\", \"type\"),\n",
    "            \"match_type\": \"equals\",\n",
    "            \"match_value\": \"I\",\n",
    "        },\n",
    "    },\n",
    "    \"NJ\": {\n",
    "        \"census_blocks_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NJ\\tl_2024_34_tabblock20.shp\"\n",
    "        ),\n",
    "        \"tracts_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\NJ\\tl_2024_34_tract.shp\"\n",
    "        ),\n",
    "        \"centroid_tract_csv\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\NJ\\centroid_tract_nj.csv\"\n",
    "        ),\n",
    "        # safety (NJ = SHP)\n",
    "        \"centerline_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NJ\\Tran_road.shp\"\n",
    "        ),\n",
    "        \"bike_lanes_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NJ\\bike-lanes-2020-division-of-transportation.shp\"\n",
    "        ),\n",
    "        \"safety_input_kind\": \"shp\",\n",
    "        \"blocks_id_col\": \"GEOID20\",\n",
    "        \"tract_id_col\": \"GEOID\",\n",
    "        \"external_tract_prefix\": None,\n",
    "        \"has_boro_logic\": False,\n",
    "        \"drop_staten_island_default\": False,\n",
    "        \"protected_lane_rule\": {\n",
    "            \"col_candidates\": (\"type\", \"facility\", \"facilitycl\", \"class\", \"lane_type\"),\n",
    "            \"match_type\": \"contains\",\n",
    "            \"match_value\": \"PROTECT\",\n",
    "        },\n",
    "    },\n",
    "    \"PITT\": {\n",
    "        \"census_blocks_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Pitt\\tl_2024_42_tabblock20.shp\"\n",
    "        ),\n",
    "        \"tracts_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\Pitt\\tl_2024_42_tract.shp\"\n",
    "        ),\n",
    "        \"centroid_tract_csv\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\Pitt\\centroid_tract_pa.csv\"\n",
    "        ),\n",
    "        # safety (PITT = SHP)\n",
    "        \"centerline_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Pitt\\Pittsburgh_Street_Centerline.shp\"\n",
    "        ),\n",
    "        \"bike_lanes_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Pitt\\Bike Map\\Bike Lanes.shp\"\n",
    "        ),\n",
    "        \"safety_input_kind\": \"shp\",\n",
    "        \"blocks_id_col\": \"GEOID20\",\n",
    "        \"tract_id_col\": \"GEOID\",\n",
    "        \"external_tract_prefix\": None,\n",
    "        \"has_boro_logic\": False,\n",
    "        \"drop_staten_island_default\": False,\n",
    "        \"protected_lane_rule\": {\n",
    "            \"col_candidates\": (\"facility\", \"type\", \"class\", \"status\", \"lane_type\", \"BIKE_FACIL\", \"CATEGORY\"),\n",
    "            \"match_type\": \"contains\",\n",
    "            \"match_value\": \"PROTECT\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# UTILITIES\n",
    "# ==================================================\n",
    "def _ensure_exists(p: Path, label: str) -> None:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"{label} not found: {p}\")\n",
    "\n",
    "\n",
    "def _minmax_norm(series: pd.Series) -> pd.Series:\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    mn = s.min(skipna=True)\n",
    "    mx = s.max(skipna=True)\n",
    "    if pd.isna(mn) or pd.isna(mx) or mx <= mn:\n",
    "        return pd.Series(0.0, index=s.index)\n",
    "    return (s - mn) / (mx - mn)\n",
    "\n",
    "\n",
    "def _safe_ratio(numer: pd.Series, denom: pd.Series) -> pd.Series:\n",
    "    return numer.div(denom.where(denom > 0, other=pd.NA)).fillna(0)\n",
    "\n",
    "\n",
    "def _pick_col(df: pd.DataFrame, candidates: Sequence[str], required: bool, label: str) -> Optional[str]:\n",
    "    cols = set(df.columns)\n",
    "    for c in candidates:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    if required:\n",
    "        raise ValueError(\n",
    "            f\"Could not find required column for {label}. Tried {list(candidates)}. \"\n",
    "            f\"Found columns (sample): {list(df.columns)[:200]}\"\n",
    "        )\n",
    "    return None\n",
    "\n",
    "\n",
    "def _tract_from_block(block_series: pd.Series) -> pd.Series:\n",
    "    s = block_series.astype(str)\n",
    "    return s.where(s.str.len() < 15, s.str[:-4])\n",
    "\n",
    "\n",
    "def _standardize_station_info(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    sid = _pick_col(df, [\"station_id\", \"stationId\", \"id\", \"Station ID\", \"Station Id\"], True, \"station_id\")\n",
    "    lat = _pick_col(df, [\"lat\", \"latitude\", \"station_lat\", \"y\"], True, \"lat\")\n",
    "    lon = _pick_col(df, [\"lon\", \"lng\", \"longitude\", \"station_lon\", \"x\"], True, \"lon\")\n",
    "    cap = _pick_col(df, [\"capacity\", \"dock_count\", \"num_docks\", \"total_docks\"], True, \"capacity\")\n",
    "\n",
    "    out = df.copy().rename(columns={sid: \"station_id\", lat: \"lat\", lon: \"lon\", cap: \"capacity\"})\n",
    "    out[\"station_id\"] = out[\"station_id\"].astype(str).str.strip()\n",
    "\n",
    "    cb = _pick_col(out, [\"census_block\", \"block_geoid\", \"GEOID20\"], False, \"census_block\")\n",
    "    if cb and cb != \"census_block\":\n",
    "        out = out.rename(columns={cb: \"census_block\"})\n",
    "    ct = _pick_col(out, [\"census_tract\", \"tract_geoid\", \"GEOID\"], False, \"census_tract\")\n",
    "    if ct and ct != \"census_tract\":\n",
    "        out = out.rename(columns={ct: \"census_tract\"})\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _standardize_trips(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # times (CitiBike + PIT exports)\n",
    "    started = _pick_col(\n",
    "        df,\n",
    "        [\"started_at\", \"start_time\", \"trip_start_time\", \"starttime\", \"Start Date\", \"start_date\", \"StartDate\"],\n",
    "        True,\n",
    "        \"started_at\",\n",
    "    )\n",
    "    ended = _pick_col(\n",
    "        df,\n",
    "        [\"ended_at\", \"end_time\", \"trip_end_time\", \"stoptime\", \"End Date\", \"end_date\", \"EndDate\"],\n",
    "        True,\n",
    "        \"ended_at\",\n",
    "    )\n",
    "    out = df.copy().rename(columns={started: \"started_at\", ended: \"ended_at\"})\n",
    "\n",
    "    # rideable_type / rider type (optional)\n",
    "    rt = _pick_col(out, [\"rideable_type\", \"bike_type\", \"vehicle_type\", \"Rider Type\", \"rider_type\"], False, \"rideable_type\")\n",
    "    if rt and rt != \"rideable_type\":\n",
    "        out = out.rename(columns={rt: \"rideable_type\"})\n",
    "\n",
    "    # precomputed census blocks (optional)\n",
    "    sb = _pick_col(out, [\"start_census_block\", \"start_block\", \"start_block_geoid\"], False, \"start_census_block\")\n",
    "    eb = _pick_col(out, [\"end_census_block\", \"end_block\", \"end_block_geoid\"], False, \"end_census_block\")\n",
    "    if sb and sb != \"start_census_block\":\n",
    "        out = out.rename(columns={sb: \"start_census_block\"})\n",
    "    if eb and eb != \"end_census_block\":\n",
    "        out = out.rename(columns={eb: \"end_census_block\"})\n",
    "\n",
    "    # lat/lng\n",
    "    slat = _pick_col(out, [\"start_lat\", \"startLatitude\", \"StartLat\"], False, \"start_lat\")\n",
    "    slng = _pick_col(out, [\"start_lng\", \"start_lon\", \"startLongitude\", \"StartLng\"], False, \"start_lng\")\n",
    "    elat = _pick_col(out, [\"end_lat\", \"endLatitude\", \"EndLat\"], False, \"end_lat\")\n",
    "    elng = _pick_col(out, [\"end_lng\", \"end_lon\", \"endLongitude\", \"EndLng\"], False, \"end_lng\")\n",
    "\n",
    "    if slat and slat != \"start_lat\":\n",
    "        out = out.rename(columns={slat: \"start_lat\"})\n",
    "    if slng and slng != \"start_lng\":\n",
    "        out = out.rename(columns={slng: \"start_lng\"})\n",
    "    if elat and elat != \"end_lat\":\n",
    "        out = out.rename(columns={elat: \"end_lat\"})\n",
    "    if elng and elng != \"end_lng\":\n",
    "        out = out.rename(columns={elng: \"end_lng\"})\n",
    "\n",
    "    # station ids (optional)\n",
    "    ssid = _pick_col(out, [\"start_station_id\", \"Start Station Id\", \"Start Station ID\"], False, \"start_station_id\")\n",
    "    esid = _pick_col(out, [\"end_station_id\", \"End Station Id\", \"End Station ID\"], False, \"end_station_id\")\n",
    "    if ssid and ssid != \"start_station_id\":\n",
    "        out = out.rename(columns={ssid: \"start_station_id\"})\n",
    "    if esid and esid != \"end_station_id\":\n",
    "        out = out.rename(columns={esid: \"end_station_id\"})\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _read_lines_layer(path: Path, *, kind: str, target_crs, csv_wkt_candidates: Tuple[str, ...], csv_crs: str):\n",
    "    import geopandas as gpd\n",
    "    from shapely import wkt\n",
    "\n",
    "    if kind == \"shp\":\n",
    "        gdf = gpd.read_file(str(path))\n",
    "        if gdf.crs is None:\n",
    "            raise ValueError(f\"CRS missing for {path}. Define CRS before projecting.\")\n",
    "        return gdf.to_crs(target_crs)\n",
    "\n",
    "    # csv with WKT\n",
    "    df = pd.read_csv(path)\n",
    "    wkt_col = _pick_col(df, csv_wkt_candidates, True, f\"WKT geometry in {path.name}\")\n",
    "    df = df.copy()\n",
    "    df[\"geometry\"] = df[wkt_col].apply(wkt.loads)\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=\"geometry\", crs=csv_crs)\n",
    "    return gdf.to_crs(target_crs)\n",
    "\n",
    "\n",
    "def _overlay_length_by_block(lines_gdf, blocks_gdf, *, blocks_id_col: str, out_col: str) -> pd.DataFrame:\n",
    "    import geopandas as gpd\n",
    "\n",
    "    blocks = blocks_gdf[[blocks_id_col, \"geometry\"]].copy()\n",
    "    lines = lines_gdf[[\"geometry\"]].copy()\n",
    "\n",
    "    inter = gpd.overlay(lines, blocks, how=\"intersection\", keep_geom_type=False)\n",
    "    if inter.empty:\n",
    "        return pd.DataFrame({\"census_block\": pd.Series([], dtype=str), out_col: pd.Series([], dtype=float)})\n",
    "\n",
    "    inter[\"seg_len\"] = inter.geometry.length\n",
    "    inter = inter.rename(columns={blocks_id_col: \"census_block\"})\n",
    "    out = inter.groupby(\"census_block\", as_index=False)[\"seg_len\"].sum().rename(columns={\"seg_len\": out_col})\n",
    "    out[out_col] = out[out_col].round(3)\n",
    "    out[\"census_block\"] = out[\"census_block\"].astype(str)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _protected_filter(lines_gdf, rule: Dict[str, Any]) -> Tuple[Optional[str], Any]:\n",
    "    col = _pick_col(lines_gdf, list(rule.get(\"col_candidates\", [])), required=False, label=\"protected lane col\")\n",
    "    if col is None:\n",
    "        return None, lines_gdf\n",
    "\n",
    "    s = lines_gdf[col].astype(str)\n",
    "    match_type = rule.get(\"match_type\", \"equals\")\n",
    "    match_value = str(rule.get(\"match_value\", \"\"))\n",
    "\n",
    "    if match_type == \"equals\":\n",
    "        mask = s.str.upper() == match_value.upper()\n",
    "    elif match_type == \"contains\":\n",
    "        mask = s.str.upper().str.contains(match_value.upper(), na=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported match_type='{match_type}' in protected_lane_rule\")\n",
    "\n",
    "    return col, lines_gdf[mask].copy()\n",
    "\n",
    "\n",
    "def _geocode_points_to_blocks_spatial(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    lon_col: str,\n",
    "    lat_col: str,\n",
    "    blocks_path: Path,\n",
    "    blocks_id_col: str,\n",
    ") -> pd.DataFrame:\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Point\n",
    "\n",
    "    blocks = gpd.read_file(str(blocks_path)).to_crs(epsg=4326)\n",
    "    if blocks_id_col not in blocks.columns:\n",
    "        raise ValueError(f\"Block shapefile missing '{blocks_id_col}' for spatial geocoding.\")\n",
    "\n",
    "    pts = df[[lon_col, lat_col]].dropna().drop_duplicates().copy()\n",
    "    pts = pts.rename(columns={lon_col: \"lon\", lat_col: \"lat\"})\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        pts,\n",
    "        geometry=[Point(xy) for xy in zip(pts[\"lon\"], pts[\"lat\"])],\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "    joined = gpd.sjoin(gdf, blocks[[blocks_id_col, \"geometry\"]], how=\"left\", predicate=\"within\")\n",
    "    joined = joined.rename(columns={blocks_id_col: \"census_block\"})\n",
    "    out = joined[[\"lon\", \"lat\", \"census_block\"]].copy()\n",
    "    out[\"census_block\"] = out[\"census_block\"].astype(str)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _census_api_block(lat: float, lon: float, benchmark: str, vintage: str) -> Optional[str]:\n",
    "    import requests\n",
    "    try:\n",
    "        url = (\n",
    "            \"https://geocoding.geo.census.gov/geocoder/geographies/coordinates\"\n",
    "            f\"?x={lon}&y={lat}\"\n",
    "            f\"&benchmark={benchmark}\"\n",
    "            f\"&vintage={vintage}\"\n",
    "            \"&format=json\"\n",
    "        )\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        js = r.json()\n",
    "        blocks = js[\"result\"][\"geographies\"].get(\"2020 Census Blocks\", [])\n",
    "        if not blocks:\n",
    "            return None\n",
    "        return blocks[0].get(\"GEOID\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# MAIN FUNCTION\n",
    "# ==================================================\n",
    "def run_docked_all_utilities_single_function(\n",
    "    *,\n",
    "    city: str,\n",
    "    station_status_txt: Union[str, Path],\n",
    "    station_information_csv: Union[str, Path],\n",
    "    trip_csv: Union[str, Path],\n",
    "    output_dir: Optional[Union[str, Path]] = None,\n",
    "    save_outputs: bool = True,\n",
    "    compute_idle_time: bool = True,  # <--- IMPORTANT: PIT trips often don't have bike-type; this allows skip\n",
    "    availability_time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    availability_time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    availability_time_granularity: str = \"1H\",\n",
    "    availability_group_level: str = \"both\",\n",
    "    peak_time_slot: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    peak_metric: str = \"total_vehicle_available\",\n",
    "    drop_staten_island: Optional[bool] = None,\n",
    "    usage_time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    usage_time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    tracts_to_remove: Optional[Sequence[str]] = None,\n",
    "    idle_time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    idle_time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    remove_tz_suffix: str = \" EDT\",\n",
    "    drop_cols: tuple[str, ...] = (\n",
    "        \"num_scooters_available\",\n",
    "        \"num_scooters_unavailable\",\n",
    "        \"is_installed\",\n",
    "        \"is_returning\",\n",
    "        \"is_renting\",\n",
    "    ),\n",
    "    fill_missing_with_census_api: bool = True,\n",
    "    census_geocoder_benchmark: str = \"Public_AR_Census2020\",\n",
    "    census_geocoder_vintage: str = \"Census2020_Current\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    One-function multi-city docked pipeline (NYC/NJ/PITT):\n",
    "      Step0: station status JSONL -> df, station info -> add census block/tract\n",
    "      Availability: block/tract time series + norm (tolerant to missing ebikes)\n",
    "      Capacity: station capacities + peak vehicle/dock capacity + norm\n",
    "      Safety: overlay-based street + bike lane + protected lane ratios (CRS-safe: uses blocks CRS)\n",
    "      Trips: supports CitiBike schema + Pittsburgh export schema; builds start/end census blocks from lat/lng if missing\n",
    "      Usage: 5-min + hourly + tract hourly norm\n",
    "      Idle: optional; runs only if compute_idle_time=True AND trip data has usable bike-type field\n",
    "    \"\"\"\n",
    "\n",
    "    city_key = city.strip().upper()\n",
    "    if city_key in {\"PITTSBURGH\", \"PIT\"}:\n",
    "        city_key = \"PITT\"\n",
    "\n",
    "    if city_key not in CITY_CONFIG:\n",
    "        raise ValueError(f\"Unsupported city='{city}'. Supported: {list(CITY_CONFIG.keys())}\")\n",
    "\n",
    "    cfg = CITY_CONFIG[city_key]\n",
    "\n",
    "    station_status_txt = Path(station_status_txt)\n",
    "    station_information_csv = Path(station_information_csv)\n",
    "    trip_csv = Path(trip_csv)\n",
    "\n",
    "    census_blocks_path = Path(cfg[\"census_blocks_path\"])\n",
    "    tracts_path = Path(cfg[\"tracts_path\"])\n",
    "    centroid_tract_csv = Path(cfg[\"centroid_tract_csv\"])\n",
    "    centerline_path = Path(cfg[\"centerline_path\"])\n",
    "    bike_lanes_path = Path(cfg[\"bike_lanes_path\"])\n",
    "\n",
    "    blocks_id_col = str(cfg[\"blocks_id_col\"])\n",
    "    tract_id_col = str(cfg[\"tract_id_col\"])\n",
    "    external_tract_prefix = cfg.get(\"external_tract_prefix\", None)\n",
    "\n",
    "    if drop_staten_island is None:\n",
    "        drop_staten_island = bool(cfg.get(\"drop_staten_island_default\", False))\n",
    "\n",
    "    out_dir = Path(output_dir) if output_dir is not None else Path(f\"./{city_key}_ALL_IN_ONE_OUTPUTS\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # existence checks\n",
    "    _ensure_exists(station_status_txt, \"station_status_txt\")\n",
    "    _ensure_exists(station_information_csv, \"station_information_csv\")\n",
    "    _ensure_exists(trip_csv, \"trip_csv\")\n",
    "    _ensure_exists(census_blocks_path, \"census_blocks_path\")\n",
    "    _ensure_exists(tracts_path, \"tracts_path\")\n",
    "    _ensure_exists(centroid_tract_csv, \"centroid_tract_csv\")\n",
    "    _ensure_exists(centerline_path, \"centerline_path\")\n",
    "    _ensure_exists(bike_lanes_path, \"bike_lanes_path\")\n",
    "\n",
    "    # normalize times\n",
    "    availability_time_start = pd.to_datetime(availability_time_start) if availability_time_start is not None else None\n",
    "    availability_time_end = pd.to_datetime(availability_time_end) if availability_time_end is not None else None\n",
    "    usage_time_start = pd.to_datetime(usage_time_start) if usage_time_start is not None else None\n",
    "    usage_time_end = pd.to_datetime(usage_time_end) if usage_time_end is not None else None\n",
    "    idle_time_start = pd.to_datetime(idle_time_start) if idle_time_start is not None else None\n",
    "    idle_time_end = pd.to_datetime(idle_time_end) if idle_time_end is not None else None\n",
    "\n",
    "    availability_group_level = availability_group_level.lower().strip()\n",
    "    if availability_group_level not in {\"block\", \"tract\", \"both\"}:\n",
    "        raise ValueError(\"availability_group_level must be one of: 'block', 'tract', 'both'\")\n",
    "\n",
    "    if peak_metric not in {\"total_vehicle_available\", \"num_docks_available\"}:\n",
    "        raise ValueError(\"peak_metric must be 'total_vehicle_available' or 'num_docks_available'\")\n",
    "\n",
    "    results: Dict[str, Any] = {\n",
    "        \"city\": city_key,\n",
    "        \"system_type\": \"docked\",\n",
    "        \"output_dir\": str(out_dir),\n",
    "        \"city_assets\": {\n",
    "            \"census_blocks_path\": str(census_blocks_path),\n",
    "            \"tracts_path\": str(tracts_path),\n",
    "            \"centroid_tract_csv\": str(centroid_tract_csv),\n",
    "            \"centerline_path\": str(centerline_path),\n",
    "            \"bike_lanes_path\": str(bike_lanes_path),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # ==================================================\n",
    "    # STEP 0 â€” station_status JSONL flatten\n",
    "    # ==================================================\n",
    "    with station_status_txt.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    di_list = []\n",
    "    for item in lines:\n",
    "        if not item.strip():\n",
    "            continue\n",
    "        js = json.loads(item)\n",
    "        timestamp = list(js.keys())[0]\n",
    "        for entry in js[timestamp]:\n",
    "            for vehicle_type in entry.get(\"vehicle_types_available\", []):\n",
    "                entry[f\"vehicle_type_{vehicle_type['vehicle_type_id']}_count\"] = vehicle_type[\"count\"]\n",
    "            if \"vehicle_types_available\" in entry:\n",
    "                del entry[\"vehicle_types_available\"]\n",
    "            entry[\"timestamp\"] = timestamp\n",
    "            di_list.append(entry)\n",
    "\n",
    "    station_status_flat = pd.DataFrame(di_list)\n",
    "    results[\"station_status_flat\"] = station_status_flat\n",
    "    if save_outputs:\n",
    "        station_status_flat.to_csv(out_dir / f\"{city_key.lower()}_station_status_flat.csv\", index=False)\n",
    "\n",
    "    ss_df = station_status_flat.copy()\n",
    "    ss_df[\"timestamp\"] = ss_df[\"timestamp\"].astype(str).str.replace(remove_tz_suffix, \"\", regex=False)\n",
    "    ss_df[\"timestamp\"] = pd.to_datetime(ss_df[\"timestamp\"], errors=\"coerce\")\n",
    "    ss_df[\"hour\"] = ss_df[\"timestamp\"].dt.hour\n",
    "    ss_df[\"date\"] = ss_df[\"timestamp\"].dt.date\n",
    "    cols_to_drop = [c for c in drop_cols if c in ss_df.columns]\n",
    "    if cols_to_drop:\n",
    "        ss_df = ss_df.drop(cols_to_drop, axis=1)\n",
    "\n",
    "    if \"station_id\" not in ss_df.columns:\n",
    "        raise ValueError(\"station_status data missing station_id after flattening.\")\n",
    "    ss_df[\"station_id\"] = ss_df[\"station_id\"].astype(str).str.strip()\n",
    "\n",
    "    results[\"station_status_clean\"] = ss_df\n",
    "    if save_outputs:\n",
    "        ss_df.to_csv(out_dir / f\"{city_key.lower()}_station_status_clean.csv\", index=False)\n",
    "\n",
    "    # ==================================================\n",
    "    # STEP 0 â€” station_information -> standardized + census block/tract\n",
    "    # ==================================================\n",
    "    si_raw = pd.read_csv(station_information_csv)\n",
    "    results[\"station_info_raw\"] = si_raw\n",
    "    if save_outputs:\n",
    "        si_raw.to_csv(out_dir / f\"{city_key.lower()}_station_information_raw.csv\", index=False)\n",
    "\n",
    "    si = _standardize_station_info(si_raw)\n",
    "\n",
    "    # If census_block missing, compute via spatial join\n",
    "    if (\"census_block\" not in si.columns) or si[\"census_block\"].isna().any():\n",
    "        import geopandas as gpd\n",
    "        from shapely.geometry import Point\n",
    "\n",
    "        pts = si.drop_duplicates(subset=[\"lat\", \"lon\"])[[\"lon\", \"lat\"]].copy()\n",
    "        pts_gdf = gpd.GeoDataFrame(\n",
    "            pts,\n",
    "            geometry=[Point(xy) for xy in zip(pts[\"lon\"], pts[\"lat\"])],\n",
    "            crs=\"EPSG:4326\",\n",
    "        )\n",
    "\n",
    "        blocks_wgs84 = gpd.read_file(str(census_blocks_path)).to_crs(epsg=4326)\n",
    "        if blocks_id_col not in blocks_wgs84.columns:\n",
    "            raise ValueError(f\"Block shapefile missing '{blocks_id_col}' for station geocoding.\")\n",
    "\n",
    "        joined = gpd.sjoin(pts_gdf, blocks_wgs84[[blocks_id_col, \"geometry\"]], how=\"left\", predicate=\"within\")\n",
    "        joined = joined.rename(columns={blocks_id_col: \"census_block\"})\n",
    "        joined[\"census_block\"] = joined[\"census_block\"].astype(str)\n",
    "        joined[\"census_tract\"] = _tract_from_block(joined[\"census_block\"]).astype(str)\n",
    "\n",
    "        si = si.merge(joined[[\"lon\", \"lat\", \"census_block\", \"census_tract\"]], on=[\"lon\", \"lat\"], how=\"left\", suffixes=(\"\", \"_sj\"))\n",
    "\n",
    "        # fill from spatial join columns if needed\n",
    "        if \"census_block_sj\" in si.columns:\n",
    "            if \"census_block\" not in si.columns:\n",
    "                si[\"census_block\"] = si[\"census_block_sj\"]\n",
    "            else:\n",
    "                si[\"census_block\"] = si[\"census_block\"].fillna(si[\"census_block_sj\"])\n",
    "            si = si.drop(columns=[\"census_block_sj\"], errors=\"ignore\")\n",
    "\n",
    "        if \"census_tract_sj\" in si.columns:\n",
    "            if \"census_tract\" not in si.columns:\n",
    "                si[\"census_tract\"] = si[\"census_tract_sj\"]\n",
    "            else:\n",
    "                si[\"census_tract\"] = si[\"census_tract\"].fillna(si[\"census_tract_sj\"])\n",
    "            si = si.drop(columns=[\"census_tract_sj\"], errors=\"ignore\")\n",
    "\n",
    "        # Census API fill for missing\n",
    "        if fill_missing_with_census_api and (\"census_block\" in si.columns) and si[\"census_block\"].isna().any():\n",
    "            miss = si[si[\"census_block\"].isna()][[\"lat\", \"lon\"]].drop_duplicates().copy()\n",
    "            miss[\"census_block_new\"] = [\n",
    "                _census_api_block(lat, lon, census_geocoder_benchmark, census_geocoder_vintage)\n",
    "                for lat, lon in zip(miss[\"lat\"], miss[\"lon\"])\n",
    "            ]\n",
    "            miss[\"census_tract_new\"] = _tract_from_block(miss[\"census_block_new\"]).astype(str)\n",
    "\n",
    "            si = si.merge(miss, on=[\"lat\", \"lon\"], how=\"left\")\n",
    "            si[\"census_block\"] = si[\"census_block\"].fillna(si[\"census_block_new\"])\n",
    "            if \"census_tract\" not in si.columns:\n",
    "                si[\"census_tract\"] = pd.NA\n",
    "            si[\"census_tract\"] = si[\"census_tract\"].fillna(si[\"census_tract_new\"])\n",
    "            si = si.drop(columns=[\"census_block_new\", \"census_tract_new\"], errors=\"ignore\")\n",
    "\n",
    "    si[\"census_block\"] = si[\"census_block\"].astype(str)\n",
    "    if \"census_tract\" not in si.columns or si[\"census_tract\"].isna().any():\n",
    "        si[\"census_tract\"] = _tract_from_block(si[\"census_block\"]).astype(str)\n",
    "    else:\n",
    "        si[\"census_tract\"] = si[\"census_tract\"].astype(str)\n",
    "\n",
    "    results[\"station_info_done\"] = si\n",
    "    if save_outputs:\n",
    "        si.to_csv(out_dir / f\"{city_key.lower()}_station_information_done.csv\", index=False)\n",
    "\n",
    "    # Merge blocks into station status\n",
    "    ss_done = ss_df.merge(si[[\"station_id\", \"census_block\"]], on=\"station_id\", how=\"left\")\n",
    "    ss_done[\"census_block\"] = ss_done[\"census_block\"].astype(str)\n",
    "    ss_done[\"census_tract\"] = _tract_from_block(ss_done[\"census_block\"]).astype(str)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Availability column standardization (tolerant to missing ebikes)\n",
    "    # --------------------------------------------------\n",
    "    if \"num_bikes_available\" not in ss_done.columns:\n",
    "        raise ValueError(\n",
    "            \"station_status_done is missing 'num_bikes_available'. \"\n",
    "            f\"Available columns: {list(ss_done.columns)[:200]}\"\n",
    "        )\n",
    "    if \"num_ebikes_available\" not in ss_done.columns:\n",
    "        ss_done[\"num_ebikes_available\"] = 0\n",
    "\n",
    "    if \"num_docks_available\" not in ss_done.columns:\n",
    "        alt = None\n",
    "        for c in [\"docks_available\", \"num_docks_free\", \"num_docks_open\"]:\n",
    "            if c in ss_done.columns:\n",
    "                alt = c\n",
    "                break\n",
    "        if alt is None:\n",
    "            raise ValueError(\n",
    "                \"station_status_done is missing 'num_docks_available' and no alternative dock column was found. \"\n",
    "                f\"Available columns: {list(ss_done.columns)[:200]}\"\n",
    "            )\n",
    "        ss_done[\"num_docks_available\"] = pd.to_numeric(ss_done[alt], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    ss_done[\"num_bikes_available\"] = pd.to_numeric(ss_done[\"num_bikes_available\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    ss_done[\"num_ebikes_available\"] = pd.to_numeric(ss_done[\"num_ebikes_available\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    ss_done[\"num_docks_available\"] = pd.to_numeric(ss_done[\"num_docks_available\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    ss_done[\"total_vehicle_available\"] = ss_done[\"num_bikes_available\"] + ss_done[\"num_ebikes_available\"]\n",
    "\n",
    "    results[\"station_status_done\"] = ss_done\n",
    "    if save_outputs:\n",
    "        ss_done.to_csv(out_dir / f\"{city_key.lower()}_station_status_done.csv\", index=False)\n",
    "\n",
    "    # ==================================================\n",
    "    # AVAILABILITY\n",
    "    # ==================================================\n",
    "    required_cols = {\"timestamp\", \"num_bikes_available\", \"num_docks_available\", \"census_block\", \"census_tract\"}\n",
    "    miss = required_cols - set(ss_done.columns)\n",
    "    if miss:\n",
    "        raise ValueError(f\"station_status_done missing required columns for availability: {miss}\")\n",
    "\n",
    "    ss_av = ss_done.copy()\n",
    "    ss_av[\"timestamp\"] = pd.to_datetime(ss_av[\"timestamp\"], errors=\"coerce\")\n",
    "    if ss_av[\"timestamp\"].isna().any():\n",
    "        raise ValueError(\"Found unparseable timestamps in station_status_done.\")\n",
    "\n",
    "    data_min = ss_av[\"timestamp\"].min()\n",
    "    data_max = ss_av[\"timestamp\"].max()\n",
    "\n",
    "    if availability_time_start is not None:\n",
    "        ss_av = ss_av[ss_av[\"timestamp\"] >= availability_time_start].copy()\n",
    "    if availability_time_end is not None:\n",
    "        ss_av = ss_av[ss_av[\"timestamp\"] < availability_time_end].copy()\n",
    "    if ss_av.empty:\n",
    "        raise ValueError(f\"No availability rows after time filter. Data range: [{data_min}, {data_max}]\")\n",
    "\n",
    "    def _availability_for_level(level: str) -> Dict[str, pd.DataFrame]:\n",
    "        geo_key = \"census_block\" if level == \"block\" else \"census_tract\"\n",
    "\n",
    "        temp = ss_av.copy()\n",
    "        temp[\"time_slot\"] = temp[\"timestamp\"].dt.floor(availability_time_granularity)\n",
    "\n",
    "        geo_ts = (\n",
    "            temp.groupby([geo_key, \"timestamp\"], as_index=False)\n",
    "            .agg(\n",
    "                num_bikes_available=(\"num_bikes_available\", \"sum\"),\n",
    "                num_ebikes_available=(\"num_ebikes_available\", \"sum\"),\n",
    "                num_docks_available=(\"num_docks_available\", \"sum\"),\n",
    "                total_vehicle_available=(\"total_vehicle_available\", \"sum\"),\n",
    "            )\n",
    "        )\n",
    "        geo_ts[\"time_slot\"] = geo_ts[\"timestamp\"].dt.floor(availability_time_granularity)\n",
    "\n",
    "        raw_df = (\n",
    "            geo_ts.groupby([geo_key, \"time_slot\"], as_index=False)\n",
    "            .mean(numeric_only=True)\n",
    "            .round(0)\n",
    "            .sort_values([geo_key, \"time_slot\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        for c in [\"num_bikes_available\", \"num_ebikes_available\", \"num_docks_available\", \"total_vehicle_available\"]:\n",
    "            raw_df[c] = raw_df[c].astype(int)\n",
    "\n",
    "        if level == \"tract\" and external_tract_prefix:\n",
    "            raw_df = raw_df[~raw_df[\"census_tract\"].astype(str).str.startswith(external_tract_prefix)].copy()\n",
    "\n",
    "        norm_df = raw_df.copy()\n",
    "        for c in [\"num_bikes_available\", \"num_ebikes_available\", \"num_docks_available\", \"total_vehicle_available\"]:\n",
    "            norm_df[c + \"_norm\"] = _minmax_norm(norm_df[c]).round(5)\n",
    "\n",
    "        if save_outputs:\n",
    "            raw_df.to_csv(out_dir / f\"availability__raw__{level}.csv\", index=False)\n",
    "            norm_df.to_csv(out_dir / f\"availability__norm__{level}.csv\", index=False)\n",
    "\n",
    "        return {\"raw\": raw_df, \"norm\": norm_df}\n",
    "\n",
    "    availability_out: Dict[str, Any] = {\"meta\": {\n",
    "        \"time_start\": availability_time_start,\n",
    "        \"time_end\": availability_time_end,\n",
    "        \"data_min\": data_min,\n",
    "        \"data_max\": data_max,\n",
    "        \"time_granularity\": availability_time_granularity,\n",
    "        \"group_level\": availability_group_level,\n",
    "    }}\n",
    "\n",
    "    if availability_group_level in {\"block\", \"both\"}:\n",
    "        blk = _availability_for_level(\"block\")\n",
    "        availability_out[\"availability_block_raw\"] = blk[\"raw\"]\n",
    "        availability_out[\"availability_block_norm\"] = blk[\"norm\"]\n",
    "\n",
    "    if availability_group_level in {\"tract\", \"both\"}:\n",
    "        tr = _availability_for_level(\"tract\")\n",
    "        availability_out[\"availability_tract_raw\"] = tr[\"raw\"]\n",
    "        availability_out[\"availability_tract_norm\"] = tr[\"norm\"]\n",
    "\n",
    "    if \"availability_tract_norm\" not in availability_out:\n",
    "        tr = _availability_for_level(\"tract\")\n",
    "        availability_out[\"availability_tract_raw\"] = tr[\"raw\"]\n",
    "        availability_out[\"availability_tract_norm\"] = tr[\"norm\"]\n",
    "\n",
    "    results[\"availability\"] = availability_out\n",
    "\n",
    "    availability_df_for_capacity = availability_out[\"availability_tract_norm\"].copy()\n",
    "    availability_df_for_capacity[\"census_tract\"] = availability_df_for_capacity[\"census_tract\"].astype(str)\n",
    "    availability_df_for_capacity[\"time_slot\"] = pd.to_datetime(availability_df_for_capacity[\"time_slot\"])\n",
    "\n",
    "    # ==================================================\n",
    "    # CAPACITY (centroids computed from tract shapefile)\n",
    "    # ==================================================\n",
    "    import geopandas as gpd\n",
    "\n",
    "    tracts_gdf = gpd.read_file(str(tracts_path))\n",
    "    if tracts_gdf.crs is None:\n",
    "        raise ValueError(f\"Tracts shapefile CRS is missing: {tracts_path}\")\n",
    "    if tract_id_col not in tracts_gdf.columns:\n",
    "        raise ValueError(f\"Tract shapefile missing '{tract_id_col}' column.\")\n",
    "\n",
    "    if cfg.get(\"has_boro_logic\", False):\n",
    "        tracts_gdf[\"county_fips\"] = tracts_gdf[\"STATEFP\"].astype(str) + tracts_gdf[\"COUNTYFP\"].astype(str)\n",
    "        county_to_boro = {\n",
    "            \"36005\": \"The Bronx\",\n",
    "            \"36047\": \"Brooklyn\",\n",
    "            \"36061\": \"Manhattan\",\n",
    "            \"36081\": \"Queens\",\n",
    "            \"36085\": \"Staten Island\",\n",
    "        }\n",
    "        tracts_gdf[\"boro\"] = tracts_gdf[\"county_fips\"].map(county_to_boro)\n",
    "        if drop_staten_island:\n",
    "            tracts_gdf = tracts_gdf[tracts_gdf[\"boro\"] != \"Staten Island\"].copy()\n",
    "\n",
    "    centroids_wgs84 = gpd.GeoSeries(tracts_gdf.geometry.centroid, crs=tracts_gdf.crs).to_crs(epsg=4326)\n",
    "    tract_centroids_df = pd.DataFrame({\n",
    "        \"census_tract\": tracts_gdf[tract_id_col].astype(str).values,\n",
    "        \"centroid_lon\": centroids_wgs84.x.values,\n",
    "        \"centroid_lat\": centroids_wgs84.y.values,\n",
    "    })\n",
    "    if \"boro\" in tracts_gdf.columns:\n",
    "        tract_centroids_df[\"boro\"] = tracts_gdf[\"boro\"].values\n",
    "\n",
    "    results[\"tract_centroids_df\"] = tract_centroids_df\n",
    "    if save_outputs:\n",
    "        tract_centroids_df.to_csv(out_dir / \"centroid_tract_computed.csv\", index=False)\n",
    "\n",
    "    station_info = si.copy()\n",
    "    station_info[\"census_block\"] = station_info[\"census_block\"].astype(str)\n",
    "    station_info[\"census_tract\"] = _tract_from_block(station_info[\"census_block\"]).astype(str)\n",
    "\n",
    "    capacity_block = (\n",
    "        station_info.groupby(\"census_block\", as_index=False)\n",
    "        .agg(total_capacity=(\"capacity\", \"sum\"), num_station=(\"station_id\", \"count\"))\n",
    "    )\n",
    "    results[\"capacity_block\"] = capacity_block\n",
    "    if save_outputs:\n",
    "        capacity_block.to_csv(out_dir / \"capacity_block.csv\", index=False)\n",
    "\n",
    "    tract_capacity = (\n",
    "        station_info.groupby(\"census_tract\", as_index=False)\n",
    "        .agg(total_capacity=(\"capacity\", \"sum\"), num_station=(\"station_id\", \"count\"))\n",
    "    )\n",
    "\n",
    "    capacity_tract = tract_centroids_df[[\"census_tract\"]].drop_duplicates().merge(\n",
    "        tract_capacity, on=\"census_tract\", how=\"left\"\n",
    "    )\n",
    "    capacity_tract[[\"total_capacity\", \"num_station\"]] = capacity_tract[[\"total_capacity\", \"num_station\"]].fillna(0)\n",
    "\n",
    "    if external_tract_prefix:\n",
    "        capacity_tract = capacity_tract[~capacity_tract[\"census_tract\"].astype(str).str.startswith(external_tract_prefix)].copy()\n",
    "\n",
    "    capacity_tract = capacity_tract.sort_values(\"census_tract\").reset_index(drop=True)\n",
    "    capacity_tract[\"total_capacity_norm\"] = _minmax_norm(capacity_tract[\"total_capacity\"]).round(5)\n",
    "    capacity_tract[\"num_station_norm\"] = _minmax_norm(capacity_tract[\"num_station\"]).round(5)\n",
    "\n",
    "    results[\"capacity_tract_norm\"] = capacity_tract\n",
    "    if save_outputs:\n",
    "        capacity_tract.to_csv(out_dir / \"capacity_tract_norm.csv\", index=False)\n",
    "\n",
    "    avail = availability_df_for_capacity.copy()\n",
    "    if external_tract_prefix:\n",
    "        avail = avail[~avail[\"census_tract\"].astype(str).str.startswith(external_tract_prefix)].copy()\n",
    "\n",
    "    if peak_time_slot is None:\n",
    "        peak_series = avail.groupby(\"time_slot\")[peak_metric].sum()\n",
    "        if peak_series.empty:\n",
    "            raise ValueError(\"No availability data after filters; cannot find peak time_slot.\")\n",
    "        peak_time_slot = peak_series.idxmax()\n",
    "    peak_time_slot = pd.to_datetime(peak_time_slot)\n",
    "\n",
    "    peak_df = avail[avail[\"time_slot\"] == peak_time_slot].copy()\n",
    "\n",
    "    vehicle_capacity = (\n",
    "        peak_df.groupby(\"census_tract\", as_index=False)[\"total_vehicle_available\"]\n",
    "        .sum()\n",
    "        .rename(columns={\"total_vehicle_available\": \"vehicle_capacity\"})\n",
    "    )\n",
    "    dock_capacity = (\n",
    "        peak_df.groupby(\"census_tract\", as_index=False)[\"num_docks_available\"]\n",
    "        .sum()\n",
    "        .rename(columns={\"num_docks_available\": \"dock_capacity\"})\n",
    "    )\n",
    "\n",
    "    capacity_df = (\n",
    "        capacity_tract.merge(vehicle_capacity, on=\"census_tract\", how=\"left\")\n",
    "        .merge(dock_capacity, on=\"census_tract\", how=\"left\")\n",
    "    )\n",
    "    capacity_df[\"vehicle_capacity\"] = capacity_df[\"vehicle_capacity\"].fillna(0)\n",
    "    capacity_df[\"dock_capacity\"] = capacity_df[\"dock_capacity\"].fillna(0)\n",
    "\n",
    "    capacity_df[\"vehicle_capacity_norm\"] = _minmax_norm(capacity_df[\"vehicle_capacity\"]).round(5)\n",
    "    capacity_df[\"dock_capacity_norm\"] = _minmax_norm(capacity_df[\"dock_capacity\"]).round(5)\n",
    "\n",
    "    capacity_df[\"occupancy_rate\"] = capacity_df.apply(\n",
    "        lambda r: (r[\"vehicle_capacity\"] / r[\"total_capacity\"]) if r[\"total_capacity\"] else 0.0, axis=1\n",
    "    )\n",
    "    capacity_df[\"return_pressure\"] = capacity_df.apply(\n",
    "        lambda r: 1.0 - (r[\"dock_capacity\"] / r[\"total_capacity\"]) if r[\"total_capacity\"] else 0.0, axis=1\n",
    "    )\n",
    "\n",
    "    results[\"capacity_df\"] = capacity_df\n",
    "    results[\"peak_time_slot\"] = peak_time_slot\n",
    "    results[\"peak_metric\"] = peak_metric\n",
    "    if save_outputs:\n",
    "        capacity_df.to_csv(out_dir / \"capacity_tract_with_vehicle_and_docks_norm.csv\", index=False)\n",
    "\n",
    "    # ==================================================\n",
    "    # SAFETY (CRS-safe: uses blocks CRS)\n",
    "    # ==================================================\n",
    "    blocks_gdf = gpd.read_file(str(census_blocks_path))\n",
    "    if blocks_gdf.crs is None:\n",
    "        raise ValueError(f\"Blocks shapefile CRS is missing: {census_blocks_path}\")\n",
    "    if blocks_id_col not in blocks_gdf.columns:\n",
    "        raise ValueError(f\"Blocks shapefile missing '{blocks_id_col}' column.\")\n",
    "\n",
    "    target_crs = blocks_gdf.crs\n",
    "    blocks_gdf = blocks_gdf.to_crs(target_crs)\n",
    "\n",
    "    safety_kind = str(cfg.get(\"safety_input_kind\", \"shp\"))\n",
    "    streets_gdf = _read_lines_layer(\n",
    "        centerline_path,\n",
    "        kind=safety_kind,\n",
    "        target_crs=target_crs,\n",
    "        csv_wkt_candidates=tuple(cfg.get(\"safety_csv_wkt_candidates\", (\"the_geom\", \"wkt\"))),\n",
    "        csv_crs=str(cfg.get(\"safety_csv_crs\", \"EPSG:4326\")),\n",
    "    )\n",
    "    bike_gdf = _read_lines_layer(\n",
    "        bike_lanes_path,\n",
    "        kind=safety_kind,\n",
    "        target_crs=target_crs,\n",
    "        csv_wkt_candidates=tuple(cfg.get(\"safety_csv_wkt_candidates\", (\"the_geom\", \"wkt\"))),\n",
    "        csv_crs=str(cfg.get(\"safety_csv_crs\", \"EPSG:4326\")),\n",
    "    )\n",
    "\n",
    "    street_len = _overlay_length_by_block(streets_gdf, blocks_gdf, blocks_id_col=blocks_id_col, out_col=\"streets_leng\")\n",
    "    bike_len = _overlay_length_by_block(bike_gdf, blocks_gdf, blocks_id_col=blocks_id_col, out_col=\"total_bike_lane_length\")\n",
    "\n",
    "    protected_rule = cfg.get(\"protected_lane_rule\", {})\n",
    "    facility_col, protected_gdf = _protected_filter(bike_gdf, protected_rule)\n",
    "\n",
    "    if facility_col is None:\n",
    "        prot_len = pd.DataFrame({\"census_block\": street_len[\"census_block\"].astype(str).unique()})\n",
    "        prot_len[\"protected_bike_lane_length\"] = 0.0\n",
    "        results[\"safety_note\"] = (\n",
    "            f\"[{city_key}] No protected-lane column found. Protected length set to 0. \"\n",
    "            f\"Tried: {protected_rule.get('col_candidates')}\"\n",
    "        )\n",
    "    else:\n",
    "        prot_len = _overlay_length_by_block(protected_gdf, blocks_gdf, blocks_id_col=blocks_id_col, out_col=\"protected_bike_lane_length\")\n",
    "        results[\"safety_protected_lane_col_used\"] = facility_col\n",
    "        results[\"safety_protected_lane_value_used\"] = protected_rule.get(\"match_value\")\n",
    "\n",
    "    safety_block = (\n",
    "        street_len.merge(bike_len, on=\"census_block\", how=\"left\")\n",
    "        .merge(prot_len, on=\"census_block\", how=\"left\")\n",
    "    )\n",
    "    safety_block[\"total_bike_lane_length\"] = safety_block[\"total_bike_lane_length\"].fillna(0)\n",
    "    safety_block[\"protected_bike_lane_length\"] = safety_block[\"protected_bike_lane_length\"].fillna(0)\n",
    "    safety_block[\"bike_lane_ratio\"] = _safe_ratio(safety_block[\"total_bike_lane_length\"], safety_block[\"streets_leng\"]).round(3)\n",
    "    safety_block[\"protected_bike_lane_ratio\"] = _safe_ratio(safety_block[\"protected_bike_lane_length\"], safety_block[\"streets_leng\"]).round(3)\n",
    "    safety_block[\"census_tract\"] = _tract_from_block(safety_block[\"census_block\"]).astype(str)\n",
    "\n",
    "    safety_tract = (\n",
    "        safety_block.groupby(\"census_tract\", as_index=False)[\n",
    "            [\"streets_leng\", \"total_bike_lane_length\", \"protected_bike_lane_length\"]\n",
    "        ].sum()\n",
    "    )\n",
    "\n",
    "    centroids_csv_df = pd.read_csv(centroid_tract_csv)\n",
    "    centroids_csv_df[\"census_tract\"] = centroids_csv_df[\"census_tract\"].astype(str)\n",
    "    region_cols = [\"census_tract\"]\n",
    "    for c in [\"boro\", \"county_name\", \"county\", \"state\"]:\n",
    "        if c in centroids_csv_df.columns:\n",
    "            region_cols.append(c)\n",
    "\n",
    "    safety_tract = centroids_csv_df[region_cols].merge(safety_tract, on=\"census_tract\", how=\"left\")\n",
    "    for c in [\"streets_leng\", \"total_bike_lane_length\", \"protected_bike_lane_length\"]:\n",
    "        safety_tract[c] = safety_tract[c].fillna(0)\n",
    "\n",
    "    safety_tract[\"bike_lane_ratio\"] = _safe_ratio(safety_tract[\"total_bike_lane_length\"], safety_tract[\"streets_leng\"])\n",
    "    safety_tract[\"protected_bike_lane_ratio\"] = _safe_ratio(safety_tract[\"protected_bike_lane_length\"], safety_tract[\"streets_leng\"])\n",
    "    safety_tract = safety_tract.sort_values(\"census_tract\").reset_index(drop=True)\n",
    "\n",
    "    service_tracts = set(capacity_df[\"census_tract\"].astype(str).unique().tolist())\n",
    "    safety_service = safety_tract[safety_tract[\"census_tract\"].astype(str).isin(service_tracts)].reset_index(drop=True)\n",
    "\n",
    "    safety_norm = safety_service.copy()\n",
    "    safety_norm[\"bike_lane_ratio_norm\"] = _minmax_norm(safety_norm[\"bike_lane_ratio\"]).round(5)\n",
    "    safety_norm[\"protected_bike_lane_ratio_norm\"] = _minmax_norm(safety_norm[\"protected_bike_lane_ratio\"]).round(5)\n",
    "\n",
    "    results[\"safety_bike_lane_block\"] = safety_block\n",
    "    results[\"safety_bike_lane_tract\"] = safety_tract\n",
    "    results[\"safety_bike_lane_service_area\"] = safety_service\n",
    "    results[\"safety_bike_lane_norm_tract\"] = safety_norm\n",
    "\n",
    "    if save_outputs:\n",
    "        safety_block.to_csv(out_dir / \"safety_bike_lane_block.csv\", index=False)\n",
    "        safety_tract.to_csv(out_dir / \"safety_bike_lane_tract.csv\", index=False)\n",
    "        safety_service.to_csv(out_dir / \"safety_bike_lane_service_area.csv\", index=False)\n",
    "        safety_norm.to_csv(out_dir / \"safety_bike_lane_norm_tract.csv\", index=False)\n",
    "\n",
    "    # ==================================================\n",
    "    # TRIPS â€” load + standardize + compute start/end census blocks if missing\n",
    "    # ==================================================\n",
    "    trip_raw = pd.read_csv(trip_csv)\n",
    "    trip_df = _standardize_trips(trip_raw)\n",
    "\n",
    "    # parse dates\n",
    "    trip_df[\"started_at\"] = pd.to_datetime(trip_df[\"started_at\"], errors=\"coerce\")\n",
    "    trip_df[\"ended_at\"] = pd.to_datetime(trip_df[\"ended_at\"], errors=\"coerce\")\n",
    "\n",
    "    # compute start/end census blocks if missing\n",
    "    if (\"start_census_block\" not in trip_df.columns) or (\"end_census_block\" not in trip_df.columns):\n",
    "        if not {\"start_lat\", \"start_lng\", \"end_lat\", \"end_lng\"}.issubset(trip_df.columns):\n",
    "            raise ValueError(\n",
    "                \"Trip CSV does not have start/end census blocks AND does not have start/end lat/lng to compute them. \"\n",
    "                f\"Found: {list(trip_df.columns)[:200]}\"\n",
    "            )\n",
    "\n",
    "        start_map = _geocode_points_to_blocks_spatial(\n",
    "            trip_df, lon_col=\"start_lng\", lat_col=\"start_lat\",\n",
    "            blocks_path=census_blocks_path, blocks_id_col=blocks_id_col\n",
    "        ).rename(columns={\"census_block\": \"start_census_block\"})\n",
    "\n",
    "        end_map = _geocode_points_to_blocks_spatial(\n",
    "            trip_df, lon_col=\"end_lng\", lat_col=\"end_lat\",\n",
    "            blocks_path=census_blocks_path, blocks_id_col=blocks_id_col\n",
    "        ).rename(columns={\"census_block\": \"end_census_block\"})\n",
    "\n",
    "        trip_df = trip_df.merge(\n",
    "            start_map, left_on=[\"start_lng\", \"start_lat\"], right_on=[\"lon\", \"lat\"], how=\"left\"\n",
    "        ).drop(columns=[\"lon\", \"lat\"], errors=\"ignore\")\n",
    "\n",
    "        trip_df = trip_df.merge(\n",
    "            end_map, left_on=[\"end_lng\", \"end_lat\"], right_on=[\"lon\", \"lat\"], how=\"left\"\n",
    "        ).drop(columns=[\"lon\", \"lat\"], errors=\"ignore\")\n",
    "\n",
    "    required_trip_cols = {\"started_at\", \"ended_at\", \"start_census_block\", \"end_census_block\"}\n",
    "    miss_trip = required_trip_cols - set(trip_df.columns)\n",
    "    if miss_trip:\n",
    "        raise ValueError(\n",
    "            f\"Trip CSV is missing required columns after standardization: {miss_trip}. \"\n",
    "            f\"Available columns: {list(trip_df.columns)[:200]}\"\n",
    "        )\n",
    "\n",
    "    trip_df[\"start_census_block\"] = trip_df[\"start_census_block\"].astype(str)\n",
    "    trip_df[\"end_census_block\"] = trip_df[\"end_census_block\"].astype(str)\n",
    "\n",
    "    # default usage/idle windows\n",
    "    if usage_time_start is None:\n",
    "        usage_time_start = availability_time_start\n",
    "    if usage_time_end is None:\n",
    "        usage_time_end = availability_time_end\n",
    "    if idle_time_start is None:\n",
    "        idle_time_start = availability_time_start\n",
    "    if idle_time_end is None:\n",
    "        idle_time_end = availability_time_end\n",
    "\n",
    "    if usage_time_start is None:\n",
    "        usage_time_start = trip_df[\"started_at\"].min()\n",
    "    if usage_time_end is None:\n",
    "        usage_time_end = trip_df[\"started_at\"].max()\n",
    "    if idle_time_start is None:\n",
    "        idle_time_start = trip_df[\"started_at\"].min()\n",
    "    if idle_time_end is None:\n",
    "        idle_time_end = trip_df[\"started_at\"].max()\n",
    "\n",
    "    usage_time_start = pd.to_datetime(usage_time_start)\n",
    "    usage_time_end = pd.to_datetime(usage_time_end)\n",
    "    idle_time_start = pd.to_datetime(idle_time_start)\n",
    "    idle_time_end = pd.to_datetime(idle_time_end)\n",
    "\n",
    "    # ==================================================\n",
    "    # USAGE (5-min, hourly, tract-hourly norm)\n",
    "    # ==================================================\n",
    "    trips = trip_df.copy()\n",
    "    trips = trips[(trips[\"started_at\"] >= usage_time_start) & (trips[\"started_at\"] <= usage_time_end)].copy()\n",
    "    trips[\"trip_duration\"] = (trips[\"ended_at\"] - trips[\"started_at\"]).dt.total_seconds() / 60.0\n",
    "    trips = trips[trips[\"trip_duration\"] <= 240].copy()\n",
    "\n",
    "    trips[\"time_slot_start\"] = trips[\"started_at\"].dt.floor(\"5min\")\n",
    "    trips[\"time_slot_end\"] = trips[\"ended_at\"].dt.floor(\"5min\")\n",
    "\n",
    "    trips_starting_5 = trips.groupby([\"start_census_block\", \"time_slot_start\"]).size().reset_index(name=\"trips_starting\")\n",
    "    trips_ending_5 = trips.groupby([\"end_census_block\", \"time_slot_end\"]).size().reset_index(name=\"trips_ending\")\n",
    "\n",
    "    trips_starting_5 = trips_starting_5.rename(columns={\"start_census_block\": \"census_block\", \"time_slot_start\": \"time_slot\"})\n",
    "    trips_ending_5 = trips_ending_5.rename(columns={\"end_census_block\": \"census_block\", \"time_slot_end\": \"time_slot\"})\n",
    "\n",
    "    all_blocks = pd.unique(pd.concat([trips[\"start_census_block\"], trips[\"end_census_block\"]]).astype(str))\n",
    "    all_blocks = [b for b in all_blocks if b not in (\"nan\", \"0\")]\n",
    "\n",
    "    all_time_slots_5 = pd.date_range(start=usage_time_start.floor(\"5min\"), end=usage_time_end.ceil(\"5min\"), freq=\"5min\")\n",
    "    grid_5 = pd.MultiIndex.from_product([all_blocks, all_time_slots_5], names=[\"census_block\", \"time_slot\"]).to_frame(index=False)\n",
    "\n",
    "    usage_5min_block = (\n",
    "        grid_5.merge(trips_starting_5, on=[\"census_block\", \"time_slot\"], how=\"left\")\n",
    "             .merge(trips_ending_5, on=[\"census_block\", \"time_slot\"], how=\"left\")\n",
    "    )\n",
    "    usage_5min_block[[\"trips_starting\", \"trips_ending\"]] = usage_5min_block[[\"trips_starting\", \"trips_ending\"]].fillna(0).astype(int)\n",
    "    usage_5min_block = usage_5min_block.sort_values([\"census_block\", \"time_slot\"]).reset_index(drop=True)\n",
    "\n",
    "    if save_outputs:\n",
    "        usage_5min_block.to_csv(out_dir / \"usage_5min_block.csv\", index=False)\n",
    "\n",
    "    trips[\"hour_start\"] = trips[\"started_at\"].dt.floor(\"H\")\n",
    "    trips[\"hour_end\"] = trips[\"ended_at\"].dt.floor(\"H\")\n",
    "\n",
    "    trips_starting_h = trips.groupby([\"start_census_block\", \"hour_start\"]).size().reset_index(name=\"trips_starting\")\n",
    "    trips_ending_h = trips.groupby([\"end_census_block\", \"hour_end\"]).size().reset_index(name=\"trips_ending\")\n",
    "\n",
    "    trips_starting_h = trips_starting_h.rename(columns={\"start_census_block\": \"census_block\", \"hour_start\": \"hour\"})\n",
    "    trips_ending_h = trips_ending_h.rename(columns={\"end_census_block\": \"census_block\", \"hour_end\": \"hour\"})\n",
    "\n",
    "    all_hours = pd.date_range(start=usage_time_start.floor(\"H\"), end=usage_time_end.ceil(\"H\"), freq=\"H\")\n",
    "    grid_h = pd.MultiIndex.from_product([all_blocks, all_hours], names=[\"census_block\", \"hour\"]).to_frame(index=False)\n",
    "\n",
    "    usage_hourly_block = (\n",
    "        grid_h.merge(trips_starting_h, on=[\"census_block\", \"hour\"], how=\"left\")\n",
    "             .merge(trips_ending_h, on=[\"census_block\", \"hour\"], how=\"left\")\n",
    "    )\n",
    "    usage_hourly_block[[\"trips_starting\", \"trips_ending\"]] = usage_hourly_block[[\"trips_starting\", \"trips_ending\"]].fillna(0).astype(int)\n",
    "    usage_hourly_block = usage_hourly_block.sort_values([\"census_block\", \"hour\"]).reset_index(drop=True)\n",
    "\n",
    "    if save_outputs:\n",
    "        usage_hourly_block.to_csv(out_dir / \"usage_hourly_block.csv\", index=False)\n",
    "\n",
    "    usage_hourly = usage_hourly_block.copy()\n",
    "    usage_hourly[\"census_block\"] = usage_hourly[\"census_block\"].astype(str)\n",
    "    usage_hourly[\"census_tract\"] = _tract_from_block(usage_hourly[\"census_block\"]).astype(str)\n",
    "\n",
    "    if external_tract_prefix:\n",
    "        usage_hourly = usage_hourly[~usage_hourly[\"census_tract\"].astype(str).str.startswith(external_tract_prefix)].copy()\n",
    "\n",
    "    usage_hourly[\"time_slot\"] = pd.to_datetime(usage_hourly[\"hour\"])\n",
    "    usage_hourly = usage_hourly.drop(columns=[\"hour\"])\n",
    "\n",
    "    usage_hourly_tract = (\n",
    "        usage_hourly.groupby([\"census_tract\", \"time_slot\"], as_index=False)\n",
    "        .agg(trips_starting=(\"trips_starting\", \"sum\"), trips_ending=(\"trips_ending\", \"sum\"))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    usage_hourly_tract_raw = usage_hourly_tract.copy()\n",
    "\n",
    "    if tracts_to_remove:\n",
    "        usage_hourly_tract = usage_hourly_tract[~usage_hourly_tract[\"census_tract\"].isin(tracts_to_remove)].reset_index(drop=True)\n",
    "\n",
    "    usage_hourly_tract[\"trips_starting_norm\"] = _minmax_norm(usage_hourly_tract[\"trips_starting\"]).round(5)\n",
    "    usage_hourly_tract[\"trips_ending_norm\"] = _minmax_norm(usage_hourly_tract[\"trips_ending\"]).round(5)\n",
    "\n",
    "    if save_outputs:\n",
    "        usage_hourly_tract_raw.to_csv(out_dir / \"usage_hourly_tract_raw.csv\", index=False)\n",
    "        usage_hourly_tract.to_csv(out_dir / \"usage_norm_hourly_tract.csv\", index=False)\n",
    "\n",
    "    results[\"usage_5min_block\"] = usage_5min_block\n",
    "    results[\"usage_hourly_block\"] = usage_hourly_block\n",
    "    results[\"usage_hourly_tract_raw\"] = usage_hourly_tract_raw\n",
    "    results[\"usage_norm_hourly_tract\"] = usage_hourly_tract\n",
    "\n",
    "    # ==================================================\n",
    "    # IDLE TIME (optional; skip if no usable bike-type)\n",
    "    # ==================================================\n",
    "    if not compute_idle_time:\n",
    "        results[\"idle_note\"] = \"Idle time skipped (compute_idle_time=False).\"\n",
    "        return results\n",
    "\n",
    "    if \"rideable_type\" not in trip_df.columns:\n",
    "        results[\"idle_note\"] = \"Idle time skipped: trip file has no rideable_type.\"\n",
    "        return results\n",
    "\n",
    "    # CitiBike-style idle expects classic_bike/electric_bike; PIT 'Rider Type' isn't that.\n",
    "    # If values don't look like bike types, skip (prevents false results).\n",
    "    vals = pd.Series(trip_df[\"rideable_type\"].dropna().astype(str).unique())\n",
    "    looks_like_bike_type = vals.str.contains(\"bike|electric|classic|ebike\", case=False, na=False).any()\n",
    "    if not looks_like_bike_type:\n",
    "        results[\"idle_note\"] = f\"Idle time skipped: rideable_type values don't look like bike types. Sample: {vals.head(10).tolist()}\"\n",
    "        return results\n",
    "\n",
    "    from collections import deque\n",
    "\n",
    "    ss_idle = ss_done.copy()\n",
    "    ss_idle[\"timestamp\"] = pd.to_datetime(ss_idle[\"timestamp\"])\n",
    "    ss_idle[\"census_block\"] = ss_idle[\"census_block\"].astype(str)\n",
    "    ss_idle[\"census_tract\"] = ss_idle[\"census_tract\"].astype(str)\n",
    "    ss_idle[\"total_available\"] = ss_idle[\"total_vehicle_available\"]\n",
    "    ss_idle[\"time_5min\"] = ss_idle[\"timestamp\"].dt.floor(\"5min\")\n",
    "\n",
    "    trips_idle = trip_df.copy()\n",
    "    trips_idle[\"started_at\"] = pd.to_datetime(trips_idle[\"started_at\"])\n",
    "    trips_idle[\"ended_at\"] = pd.to_datetime(trips_idle[\"ended_at\"])\n",
    "    trips_idle[\"start_census_block\"] = trips_idle[\"start_census_block\"].astype(str)\n",
    "    trips_idle[\"end_census_block\"] = trips_idle[\"end_census_block\"].astype(str)\n",
    "\n",
    "    trips_idle = trips_idle[(trips_idle[\"started_at\"] >= idle_time_start) & (trips_idle[\"started_at\"] <= idle_time_end)].copy()\n",
    "    trips_idle[\"time_slot_start\"] = trips_idle[\"started_at\"].dt.floor(\"5min\")\n",
    "    trips_idle[\"time_slot_end\"] = trips_idle[\"ended_at\"].dt.floor(\"5min\")\n",
    "\n",
    "    ts_start = (\n",
    "        trips_idle.groupby([\"start_census_block\", \"time_slot_start\", \"rideable_type\"])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "        .reset_index()\n",
    "    )\n",
    "    ts_start = ts_start.rename(columns={\"start_census_block\": \"census_block\", \"time_slot_start\": \"time_slot\"})\n",
    "\n",
    "    ts_end = (\n",
    "        trips_idle.groupby([\"end_census_block\", \"time_slot_end\", \"rideable_type\"])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "        .reset_index()\n",
    "    )\n",
    "    ts_end = ts_end.rename(columns={\"end_census_block\": \"census_block\", \"time_slot_end\": \"time_slot\"})\n",
    "\n",
    "    # Force classic/electric columns if present; otherwise treat all as generic (still usable)\n",
    "    for col in [\"classic_bike\", \"electric_bike\"]:\n",
    "        if col not in ts_start.columns:\n",
    "            ts_start[col] = 0\n",
    "        if col not in ts_end.columns:\n",
    "            ts_end[col] = 0\n",
    "\n",
    "    ts_start = ts_start.rename(columns={\"classic_bike\": \"trips_starting_bike\", \"electric_bike\": \"trips_starting_ebike\"})\n",
    "    ts_end = ts_end.rename(columns={\"classic_bike\": \"trips_ending_bike\", \"electric_bike\": \"trips_ending_ebike\"})\n",
    "\n",
    "    all_blocks_idle = pd.unique(pd.concat([trips_idle[\"start_census_block\"], trips_idle[\"end_census_block\"]]))\n",
    "    all_blocks_idle = pd.Series(all_blocks_idle).dropna().astype(str).unique()\n",
    "    all_time_slots_idle = pd.date_range(start=idle_time_start.floor(\"5min\"), end=idle_time_end.ceil(\"5min\"), freq=\"5min\")\n",
    "\n",
    "    grid_idle = pd.MultiIndex.from_product([all_blocks_idle, all_time_slots_idle], names=[\"census_block\", \"time_slot\"]).to_frame(index=False)\n",
    "\n",
    "    result_complete = (\n",
    "        grid_idle.merge(ts_start[[\"census_block\", \"time_slot\", \"trips_starting_bike\", \"trips_starting_ebike\"]],\n",
    "                        on=[\"census_block\", \"time_slot\"], how=\"left\")\n",
    "                .merge(ts_end[[\"census_block\", \"time_slot\", \"trips_ending_bike\", \"trips_ending_ebike\"]],\n",
    "                       on=[\"census_block\", \"time_slot\"], how=\"left\")\n",
    "    )\n",
    "    for c in [\"trips_starting_bike\", \"trips_starting_ebike\", \"trips_ending_bike\", \"trips_ending_ebike\"]:\n",
    "        result_complete[c] = result_complete[c].fillna(0).astype(int)\n",
    "\n",
    "    result_complete[\"trips_starting\"] = result_complete[\"trips_starting_bike\"] + result_complete[\"trips_starting_ebike\"]\n",
    "    result_complete[\"trips_ending\"] = result_complete[\"trips_ending_bike\"] + result_complete[\"trips_ending_ebike\"]\n",
    "    result_complete = result_complete.sort_values([\"census_block\", \"time_slot\"]).reset_index(drop=True)\n",
    "\n",
    "    station_availability_df = (\n",
    "        ss_idle.groupby([\"census_block\", \"time_5min\"], as_index=False)[[\"total_available\"]].sum()\n",
    "    ).rename(columns={\"time_5min\": \"time_slot\"})\n",
    "    station_availability_df[\"time_slot\"] = station_availability_df[\"time_slot\"] - pd.Timedelta(minutes=5)\n",
    "\n",
    "    merged_idle = station_availability_df.merge(result_complete, on=[\"census_block\", \"time_slot\"], how=\"left\")\n",
    "    merged_idle[[\"trips_starting\", \"trips_ending\"]] = merged_idle[[\"trips_starting\", \"trips_ending\"]].fillna(0).astype(int)\n",
    "    merged_idle[\"vehicles_moved\"] = merged_idle[\"trips_starting\"] + merged_idle[\"trips_ending\"]\n",
    "    merged_idle[\"idle_vehicles\"] = (merged_idle[\"total_available\"] - merged_idle[\"vehicles_moved\"]).clip(lower=0)\n",
    "\n",
    "    merged_idle[\"time_slot\"] = pd.to_datetime(merged_idle[\"time_slot\"])\n",
    "    merged_idle = merged_idle.sort_values([\"census_block\", \"time_slot\"])\n",
    "    merged_idle[\"hour\"] = merged_idle[\"time_slot\"].dt.floor(\"H\")\n",
    "\n",
    "    idle_results = []\n",
    "    for block, block_df in merged_idle.groupby(\"census_block\"):\n",
    "        for hour, hour_df in block_df.groupby(\"hour\"):\n",
    "            hour_df = hour_df.sort_values(\"time_slot\")\n",
    "            virtual_vehicles = deque()\n",
    "            durations = []\n",
    "\n",
    "            for _, row in hour_df.iterrows():\n",
    "                current_time = row[\"time_slot\"]\n",
    "                current_idle = int(row[\"idle_vehicles\"])\n",
    "\n",
    "                if current_idle > len(virtual_vehicles):\n",
    "                    for _ in range(current_idle - len(virtual_vehicles)):\n",
    "                        virtual_vehicles.append(current_time)\n",
    "                elif current_idle < len(virtual_vehicles):\n",
    "                    for _ in range(len(virtual_vehicles) - current_idle):\n",
    "                        start_t = virtual_vehicles.popleft()\n",
    "                        durations.append((current_time - start_t).total_seconds() / 60.0)\n",
    "\n",
    "            final_time = hour + pd.Timedelta(hours=1)\n",
    "            while virtual_vehicles:\n",
    "                start_t = virtual_vehicles.popleft()\n",
    "                durations.append((final_time - start_t).total_seconds() / 60.0)\n",
    "\n",
    "            avg_idle_time = round(sum(durations) / len(durations), 2) if durations else 0.0\n",
    "            idle_results.append({\"census_block\": block, \"time_slot\": hour, \"avg_idle_time\": avg_idle_time})\n",
    "\n",
    "    idle_time_block = pd.DataFrame(idle_results)\n",
    "    idle_time_block[\"census_block\"] = idle_time_block[\"census_block\"].astype(str)\n",
    "    idle_time_block[\"census_tract\"] = _tract_from_block(idle_time_block[\"census_block\"]).astype(str)\n",
    "\n",
    "    idle_time_tract = idle_time_block.groupby([\"census_tract\", \"time_slot\"], as_index=False)[\"avg_idle_time\"].mean()\n",
    "    if external_tract_prefix:\n",
    "        idle_time_tract = idle_time_tract[~idle_time_tract[\"census_tract\"].astype(str).str.startswith(external_tract_prefix)].reset_index(drop=True)\n",
    "\n",
    "    idle_time_tract[\"avg_idle_time_norm\"] = _minmax_norm(idle_time_tract[\"avg_idle_time\"]).round(5)\n",
    "\n",
    "    results[\"idle_merged_5min\"] = merged_idle\n",
    "    results[\"idle_time_block\"] = idle_time_block\n",
    "    results[\"idle_time_tract_raw\"] = idle_time_tract.drop(columns=[\"avg_idle_time_norm\"])\n",
    "    results[\"idle_time_norm_tract\"] = idle_time_tract\n",
    "\n",
    "    if save_outputs:\n",
    "        merged_idle.to_csv(out_dir / \"idle_merged_5min.csv\", index=False)\n",
    "        idle_time_block.to_csv(out_dir / \"idle_time_block.csv\", index=False)\n",
    "        results[\"idle_time_tract_raw\"].to_csv(out_dir / \"idle_time_tract_raw.csv\", index=False)\n",
    "        idle_time_tract.to_csv(out_dir / \"idle_time_norm_tract.csv\", index=False)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5184b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, Dict, Any, Sequence, Tuple\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# CITY CONFIG (hardcoded assets for NYC + NJ + PITT + SF)\n",
    "# ==================================================\n",
    "CITY_CONFIG: Dict[str, Dict[str, Any]] = {\n",
    "    \"NYC\": {\n",
    "        \"census_blocks_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\GBFS_Census_Tract\\NYC\\tl_2024_36_tabblock20.shp\"\n",
    "        ),\n",
    "        \"tracts_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\NYC\\tl_2024_36_tract.shp\"\n",
    "        ),\n",
    "        \"centroid_tract_csv\": Path(\n",
    "            r\"D:\\Research Fellowship\\Capacity_NYC\\centroid_tract_computed.csv\"\n",
    "        ),\n",
    "        # safety (NYC = CSV WKT)\n",
    "        \"centerline_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NYC\\CSCL_PlowNYC_20250619.csv\"\n",
    "        ),\n",
    "        \"bike_lanes_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NYC\\New_York_City_Bike_Routes_20250619.csv\"\n",
    "        ),\n",
    "        \"safety_input_kind\": \"csv_wkt\",\n",
    "        \"safety_csv_wkt_candidates\": (\"geometry\", \"the_geom\", \"wkt\", \"geometry_wkt\", \"WKT\", \"geom\", \"shape\", \"line\"),\n",
    "        \"safety_csv_crs\": \"EPSG:4326\",\n",
    "        \"blocks_id_col\": \"GEOID20\",\n",
    "        \"tract_id_col\": \"GEOID\",\n",
    "        \"external_tract_prefix\": \"34\",  # drop NJ tracts for NYC results\n",
    "        \"has_boro_logic\": True,\n",
    "        \"drop_staten_island_default\": True,\n",
    "        \"protected_lane_rule\": {\n",
    "            \"col_candidates\": (\"facilitycl\", \"facility\", \"class\", \"ft\", \"type\"),\n",
    "            \"match_type\": \"equals\",\n",
    "            \"match_value\": \"I\",\n",
    "        },\n",
    "    },\n",
    "    \"NJ\": {\n",
    "        \"census_blocks_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NJ\\tl_2024_34_tabblock20.shp\"\n",
    "        ),\n",
    "        \"tracts_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\NJ\\tl_2024_34_tract.shp\"\n",
    "        ),\n",
    "        \"centroid_tract_csv\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\NJ\\centroid_tract_nj.csv\"\n",
    "        ),\n",
    "        # safety (NJ = SHP)\n",
    "        \"centerline_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NJ\\Tran_road.shp\"\n",
    "        ),\n",
    "        \"bike_lanes_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NJ\\bike-lanes-2020-division-of-transportation.shp\"\n",
    "        ),\n",
    "        \"safety_input_kind\": \"shp\",\n",
    "        \"blocks_id_col\": \"GEOID20\",\n",
    "        \"tract_id_col\": \"GEOID\",\n",
    "        \"external_tract_prefix\": None,\n",
    "        \"has_boro_logic\": False,\n",
    "        \"drop_staten_island_default\": False,\n",
    "        \"protected_lane_rule\": {\n",
    "            \"col_candidates\": (\"type\", \"facility\", \"facilitycl\", \"class\", \"lane_type\"),\n",
    "            \"match_type\": \"contains\",\n",
    "            \"match_value\": \"PROTECT\",\n",
    "        },\n",
    "    },\n",
    "    \"PITT\": {\n",
    "        \"census_blocks_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Pitt\\tl_2024_42_tabblock20.shp\"\n",
    "        ),\n",
    "        \"tracts_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\Pitt\\tl_2024_42_tract.shp\"\n",
    "        ),\n",
    "        \"centroid_tract_csv\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\Pitt\\centroid_tract_pa.csv\"\n",
    "        ),\n",
    "        # safety (PITT = SHP)\n",
    "        \"centerline_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Pitt\\Pittsburgh_Street_Centerline.shp\"\n",
    "        ),\n",
    "        \"bike_lanes_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Pitt\\Bike Map\\Bike Lanes.shp\"\n",
    "        ),\n",
    "        \"safety_input_kind\": \"shp\",\n",
    "        \"blocks_id_col\": \"GEOID20\",\n",
    "        \"tract_id_col\": \"GEOID\",\n",
    "        \"external_tract_prefix\": None,\n",
    "        \"has_boro_logic\": False,\n",
    "        \"drop_staten_island_default\": False,\n",
    "        \"protected_lane_rule\": {\n",
    "            \"col_candidates\": (\"facility\", \"type\", \"class\", \"status\", \"lane_type\", \"BIKE_FACIL\", \"CATEGORY\"),\n",
    "            \"match_type\": \"contains\",\n",
    "            \"match_value\": \"PROTECT\",\n",
    "        },\n",
    "    },\n",
    "    \"SF\": {\n",
    "        \"census_blocks_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Usage\\San_Fran_Baywheels\\tl_2024_06_tabblock20.shp\"\n",
    "        ),\n",
    "        \"tracts_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\San_Fran_Baywheels\\tl_2024_06_tract.shp\"\n",
    "        ),\n",
    "        \"centroid_tract_csv\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\San_Fran_Baywheels\\centroid_tract_ca.csv\"\n",
    "        ),\n",
    "        # safety (SF = CSV WKT)\n",
    "        \"centerline_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\San_Fran_Baywheels\\Streets___Active_and_Retired_20250626 (1).csv\"\n",
    "        ),\n",
    "        \"bike_lanes_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\San_Fran_Baywheels\\Bikelane.csv\"\n",
    "        ),\n",
    "        \"safety_input_kind\": \"csv_wkt\",\n",
    "        \"safety_csv_wkt_candidates\": (\"geometry\", \"shape\", \"line\", \"the_geom\", \"wkt\", \"geometry_wkt\", \"WKT\", \"geom\"),\n",
    "        \"safety_csv_crs\": \"EPSG:4326\",\n",
    "        \"blocks_id_col\": \"GEOID20\",\n",
    "        \"tract_id_col\": \"GEOID\",\n",
    "        \"external_tract_prefix\": None,\n",
    "        \"has_boro_logic\": False,\n",
    "        \"drop_staten_island_default\": False,\n",
    "        # protected heuristic: non-empty BARRIER OR keywords in FACILITY_T\n",
    "        \"protected_lane_rule\": {\n",
    "            \"col_candidates\": (\"BARRIER\", \"FACILITY_T\", \"BUFFERED\", \"RAISED\", \"SYMBOLOGY\"),\n",
    "            \"match_type\": \"not_empty_or_contains\",\n",
    "            \"match_value\": r\"PROTECT|SEPARAT|CYCLETRACK|BARRIER|RAISED|BUFFER\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# Helpers\n",
    "# ==================================================\n",
    "def _ensure_exists(p: Path, label: str) -> None:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"{label} not found: {p}\")\n",
    "\n",
    "\n",
    "def _minmax_norm(series: pd.Series) -> pd.Series:\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    mn = s.min(skipna=True)\n",
    "    mx = s.max(skipna=True)\n",
    "    if pd.isna(mn) or pd.isna(mx) or mx <= mn:\n",
    "        return pd.Series(0.0, index=s.index)\n",
    "    return (s - mn) / (mx - mn)\n",
    "\n",
    "\n",
    "def _safe_ratio(numer: pd.Series, denom: pd.Series) -> pd.Series:\n",
    "    return numer.div(denom.where(denom > 0, other=pd.NA)).fillna(0)\n",
    "\n",
    "\n",
    "def _pick_col(df: pd.DataFrame, candidates: Sequence[str], required: bool, label: str) -> Optional[str]:\n",
    "    cols = set(df.columns)\n",
    "    for c in candidates:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    if required:\n",
    "        raise ValueError(\n",
    "            f\"Could not find required column for {label}. Tried {list(candidates)}. \"\n",
    "            f\"Found columns (sample): {list(df.columns)[:200]}\"\n",
    "        )\n",
    "    return None\n",
    "\n",
    "\n",
    "def _tract_from_block(block_series: pd.Series) -> pd.Series:\n",
    "    s = block_series.astype(str)\n",
    "    # If it looks like a 15-digit block GEOID -> tract is first 11 OR strip last 4\n",
    "    # We use strip-last-4 which matches your previous logic.\n",
    "    return s.where(s.str.len() < 15, s.str[:-4])\n",
    "\n",
    "\n",
    "def _standardize_station_info(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    sid = _pick_col(df, [\"station_id\", \"stationId\", \"id\", \"Station ID\", \"Station Id\"], True, \"station_id\")\n",
    "    lat = _pick_col(df, [\"lat\", \"latitude\", \"station_lat\", \"y\"], True, \"lat\")\n",
    "    lon = _pick_col(df, [\"lon\", \"lng\", \"longitude\", \"station_lon\", \"x\"], True, \"lon\")\n",
    "    cap = _pick_col(df, [\"capacity\", \"dock_count\", \"num_docks\", \"total_docks\"], True, \"capacity\")\n",
    "\n",
    "    out = df.copy().rename(columns={sid: \"station_id\", lat: \"lat\", lon: \"lon\", cap: \"capacity\"})\n",
    "    out[\"station_id\"] = out[\"station_id\"].astype(str).str.strip()\n",
    "\n",
    "    cb = _pick_col(out, [\"census_block\", \"block_geoid\", \"GEOID20\"], False, \"census_block\")\n",
    "    if cb and cb != \"census_block\":\n",
    "        out = out.rename(columns={cb: \"census_block\"})\n",
    "\n",
    "    ct = _pick_col(out, [\"census_tract\", \"tract_geoid\", \"GEOID\"], False, \"census_tract\")\n",
    "    if ct and ct != \"census_tract\":\n",
    "        out = out.rename(columns={ct: \"census_tract\"})\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _standardize_trips(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    started = _pick_col(\n",
    "        df,\n",
    "        [\"started_at\", \"start_time\", \"trip_start_time\", \"starttime\", \"Start Date\", \"start_date\", \"StartDate\"],\n",
    "        True,\n",
    "        \"started_at\",\n",
    "    )\n",
    "    ended = _pick_col(\n",
    "        df,\n",
    "        [\"ended_at\", \"end_time\", \"trip_end_time\", \"stoptime\", \"End Date\", \"end_date\", \"EndDate\"],\n",
    "        True,\n",
    "        \"ended_at\",\n",
    "    )\n",
    "    out = df.copy().rename(columns={started: \"started_at\", ended: \"ended_at\"})\n",
    "\n",
    "    rt = _pick_col(out, [\"rideable_type\", \"bike_type\", \"vehicle_type\", \"Rider Type\", \"rider_type\"], False, \"rideable_type\")\n",
    "    if rt and rt != \"rideable_type\":\n",
    "        out = out.rename(columns={rt: \"rideable_type\"})\n",
    "\n",
    "    # precomputed blocks if present\n",
    "    sb = _pick_col(out, [\"start_census_block\", \"start_block\", \"start_block_geoid\"], False, \"start_census_block\")\n",
    "    eb = _pick_col(out, [\"end_census_block\", \"end_block\", \"end_block_geoid\"], False, \"end_census_block\")\n",
    "    if sb and sb != \"start_census_block\":\n",
    "        out = out.rename(columns={sb: \"start_census_block\"})\n",
    "    if eb and eb != \"end_census_block\":\n",
    "        out = out.rename(columns={eb: \"end_census_block\"})\n",
    "\n",
    "    # lat/lon fallbacks\n",
    "    slat = _pick_col(out, [\"start_lat\", \"startLatitude\", \"StartLat\"], False, \"start_lat\")\n",
    "    slng = _pick_col(out, [\"start_lng\", \"start_lon\", \"startLongitude\", \"StartLng\"], False, \"start_lng\")\n",
    "    elat = _pick_col(out, [\"end_lat\", \"endLatitude\", \"EndLat\"], False, \"end_lat\")\n",
    "    elng = _pick_col(out, [\"end_lng\", \"end_lon\", \"endLongitude\", \"EndLng\"], False, \"end_lng\")\n",
    "    if slat and slat != \"start_lat\":\n",
    "        out = out.rename(columns={slat: \"start_lat\"})\n",
    "    if slng and slng != \"start_lng\":\n",
    "        out = out.rename(columns={slng: \"start_lng\"})\n",
    "    if elat and elat != \"end_lat\":\n",
    "        out = out.rename(columns={elat: \"end_lat\"})\n",
    "    if elng and elng != \"end_lng\":\n",
    "        out = out.rename(columns={elng: \"end_lng\"})\n",
    "\n",
    "    # station ids (optional)\n",
    "    ssid = _pick_col(out, [\"start_station_id\", \"Start Station Id\", \"Start Station ID\"], False, \"start_station_id\")\n",
    "    esid = _pick_col(out, [\"end_station_id\", \"End Station Id\", \"End Station ID\"], False, \"end_station_id\")\n",
    "    if ssid and ssid != \"start_station_id\":\n",
    "        out = out.rename(columns={ssid: \"start_station_id\"})\n",
    "    if esid and esid != \"end_station_id\":\n",
    "        out = out.rename(columns={esid: \"end_station_id\"})\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _read_lines_layer(\n",
    "    path: Path,\n",
    "    *,\n",
    "    kind: str,\n",
    "    target_crs,\n",
    "    csv_wkt_candidates: Tuple[str, ...],\n",
    "    csv_crs: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Robust for SF/NYC CSV-WKT:\n",
    "      - auto-picks the WKT column by sampling values\n",
    "      - safe parses WKT (drops non-string/NaN)\n",
    "    \"\"\"\n",
    "    import geopandas as gpd\n",
    "    from shapely import wkt\n",
    "\n",
    "    if kind == \"shp\":\n",
    "        gdf = gpd.read_file(str(path))\n",
    "        if gdf.crs is None:\n",
    "            raise ValueError(f\"CRS missing for {path}. Define CRS before projecting.\")\n",
    "        return gdf.to_crs(target_crs)\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    def _looks_like_wkt(v: Any) -> bool:\n",
    "        if not isinstance(v, str):\n",
    "            return False\n",
    "        s = v.strip().upper()\n",
    "        return s.startswith((\"LINESTRING\", \"MULTILINESTRING\", \"GEOMETRYCOLLECTION\"))\n",
    "\n",
    "    wkt_col = None\n",
    "    for c in csv_wkt_candidates:\n",
    "        if c in df.columns:\n",
    "            sample = df[c].dropna().astype(str).head(80)\n",
    "            if not sample.empty and sample.map(_looks_like_wkt).mean() >= 0.2:\n",
    "                wkt_col = c\n",
    "                break\n",
    "\n",
    "    if wkt_col is None:\n",
    "        raise ValueError(\n",
    "            f\"Could not find a usable WKT column in {path.name}. \"\n",
    "            f\"Tried candidates: {list(csv_wkt_candidates)}. \"\n",
    "            f\"Found columns: {list(df.columns)[:120]}\"\n",
    "        )\n",
    "\n",
    "    def _safe_wkt_load(x: Any):\n",
    "        if not isinstance(x, str):\n",
    "            return None\n",
    "        x = x.strip()\n",
    "        if not x:\n",
    "            return None\n",
    "        try:\n",
    "            return wkt.loads(x)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"geometry\"] = df[wkt_col].apply(_safe_wkt_load)\n",
    "    df = df[df[\"geometry\"].notna()].copy()\n",
    "    if df.empty:\n",
    "        raise ValueError(\n",
    "            f\"After parsing WKT, no valid geometries remained for {path.name}. \"\n",
    "            f\"Chosen WKT column: '{wkt_col}'.\"\n",
    "        )\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=\"geometry\", crs=csv_crs)\n",
    "    return gdf.to_crs(target_crs)\n",
    "\n",
    "\n",
    "def _overlay_length_by_block(lines_gdf, blocks_gdf, *, blocks_id_col: str, out_col: str) -> pd.DataFrame:\n",
    "    import geopandas as gpd\n",
    "\n",
    "    blocks = blocks_gdf[[blocks_id_col, \"geometry\"]].copy()\n",
    "    lines = lines_gdf[[\"geometry\"]].copy()\n",
    "\n",
    "    inter = gpd.overlay(lines, blocks, how=\"intersection\", keep_geom_type=False)\n",
    "    if inter.empty:\n",
    "        return pd.DataFrame({\"census_block\": pd.Series([], dtype=str), out_col: pd.Series([], dtype=float)})\n",
    "\n",
    "    inter[\"seg_len\"] = inter.geometry.length\n",
    "    inter = inter.rename(columns={blocks_id_col: \"census_block\"})\n",
    "    out = inter.groupby(\"census_block\", as_index=False)[\"seg_len\"].sum().rename(columns={\"seg_len\": out_col})\n",
    "    out[out_col] = out[out_col].round(3)\n",
    "    out[\"census_block\"] = out[\"census_block\"].astype(str)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _protected_filter(lines_gdf, rule: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Supported match_type:\n",
    "      - equals\n",
    "      - contains\n",
    "      - not_empty\n",
    "      - not_empty_or_contains\n",
    "    \"\"\"\n",
    "    col = _pick_col(lines_gdf, list(rule.get(\"col_candidates\", [])), required=False, label=\"protected lane col\")\n",
    "    if col is None:\n",
    "        return None, lines_gdf\n",
    "\n",
    "    s = lines_gdf[col].astype(str)\n",
    "    match_type = str(rule.get(\"match_type\", \"equals\")).lower()\n",
    "    match_value = str(rule.get(\"match_value\", \"\"))\n",
    "\n",
    "    if match_type == \"equals\":\n",
    "        mask = s.str.upper() == match_value.upper()\n",
    "    elif match_type == \"contains\":\n",
    "        mask = s.str.upper().str.contains(match_value.upper(), na=False)\n",
    "    elif match_type == \"not_empty\":\n",
    "        s2 = lines_gdf[col]\n",
    "        mask = s2.notna() & (s.astype(str).str.strip() != \"\") & (~s.astype(str).str.lower().isin([\"0\", \"false\", \"none\", \"nan\"]))\n",
    "    elif match_type == \"not_empty_or_contains\":\n",
    "        s2 = lines_gdf[col]\n",
    "        non_empty = s2.notna() & (s.astype(str).str.strip() != \"\") & (~s.astype(str).str.lower().isin([\"0\", \"false\", \"none\", \"nan\"]))\n",
    "        contains = s.astype(str).str.contains(match_value, case=False, na=False, regex=True)\n",
    "        mask = non_empty | contains\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported match_type='{match_type}' in protected_lane_rule\")\n",
    "\n",
    "    return col, lines_gdf[mask].copy()\n",
    "\n",
    "\n",
    "def _geocode_points_to_blocks_spatial(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    lon_col: str,\n",
    "    lat_col: str,\n",
    "    blocks_path: Path,\n",
    "    blocks_id_col: str,\n",
    ") -> pd.DataFrame:\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Point\n",
    "\n",
    "    blocks = gpd.read_file(str(blocks_path)).to_crs(epsg=4326)\n",
    "    if blocks_id_col not in blocks.columns:\n",
    "        raise ValueError(f\"Block shapefile missing '{blocks_id_col}' for spatial geocoding.\")\n",
    "\n",
    "    pts = df[[lon_col, lat_col]].dropna().drop_duplicates().copy()\n",
    "    pts = pts.rename(columns={lon_col: \"lon\", lat_col: \"lat\"})\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        pts,\n",
    "        geometry=[Point(xy) for xy in zip(pts[\"lon\"], pts[\"lat\"])],\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "    joined = gpd.sjoin(gdf, blocks[[blocks_id_col, \"geometry\"]], how=\"left\", predicate=\"within\")\n",
    "    joined = joined.rename(columns={blocks_id_col: \"census_block\"})\n",
    "    out = joined[[\"lon\", \"lat\", \"census_block\"]].copy()\n",
    "    out[\"census_block\"] = out[\"census_block\"].astype(str)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _census_api_block(lat: float, lon: float, benchmark: str, vintage: str) -> Optional[str]:\n",
    "    import requests\n",
    "    try:\n",
    "        url = (\n",
    "            \"https://geocoding.geo.census.gov/geocoder/geographies/coordinates\"\n",
    "            f\"?x={lon}&y={lat}\"\n",
    "            f\"&benchmark={benchmark}\"\n",
    "            f\"&vintage={vintage}\"\n",
    "            \"&format=json\"\n",
    "        )\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        js = r.json()\n",
    "        blocks = js[\"result\"][\"geographies\"].get(\"2020 Census Blocks\", [])\n",
    "        if not blocks:\n",
    "            return None\n",
    "        return blocks[0].get(\"GEOID\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _map_rideable_type_to_two_columns(trips: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create columns:\n",
    "      - trips_starting_bike, trips_starting_ebike\n",
    "      - trips_ending_bike, trips_ending_ebike\n",
    "    using rideable_type values like classic_bike/electric_bike (or similar strings).\n",
    "    \"\"\"\n",
    "    df = trips.copy()\n",
    "    if \"rideable_type\" not in df.columns:\n",
    "        return df\n",
    "\n",
    "    t = df[\"rideable_type\"].astype(str).str.lower()\n",
    "    df[\"_is_ebike\"] = t.str.contains(\"electric|ebike|e-bike|assist\", na=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# MAIN FUNCTION\n",
    "# ==================================================\n",
    "def run_docked_all_utilities_single_function(\n",
    "    *,\n",
    "    city: str,\n",
    "    station_status_txt: Union[str, Path],\n",
    "    station_information_csv: Union[str, Path],\n",
    "    trip_csv: Union[str, Path],\n",
    "    output_dir: Optional[Union[str, Path]] = None,\n",
    "    save_outputs: bool = True,\n",
    "    compute_idle_time: bool = True,\n",
    "    availability_time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    availability_time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    availability_time_granularity: str = \"1H\",\n",
    "    availability_group_level: str = \"both\",\n",
    "    peak_time_slot: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    peak_metric: str = \"total_vehicle_available\",\n",
    "    drop_staten_island: Optional[bool] = None,\n",
    "    usage_time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    usage_time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    tracts_to_remove: Optional[Sequence[str]] = None,\n",
    "    idle_time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    idle_time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    remove_tz_suffix: str = \" EDT\",\n",
    "    drop_cols: tuple[str, ...] = (\n",
    "        \"num_scooters_available\",\n",
    "        \"num_scooters_unavailable\",\n",
    "        \"is_installed\",\n",
    "        \"is_returning\",\n",
    "        \"is_renting\",\n",
    "    ),\n",
    "    fill_missing_with_census_api: bool = True,\n",
    "    census_geocoder_benchmark: str = \"Public_AR_Census2020\",\n",
    "    census_geocoder_vintage: str = \"Census2020_Current\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    One-function multi-city docked pipeline (NYC/NJ/PITT/SF), including idle simulation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize city keys\n",
    "    city_key = city.strip().upper()\n",
    "    if city_key in {\"PITTSBURGH\", \"PIT\"}:\n",
    "        city_key = \"PITT\"\n",
    "    if city_key in {\"SANFRAN\", \"SAN_FRAN\", \"SAN FRANCISCO\", \"BAYWHEELS\", \"BAY WHEELS\"}:\n",
    "        city_key = \"SF\"\n",
    "\n",
    "    if city_key not in CITY_CONFIG:\n",
    "        raise ValueError(f\"Unsupported city='{city}'. Supported: {list(CITY_CONFIG.keys())}\")\n",
    "\n",
    "    cfg = CITY_CONFIG[city_key]\n",
    "\n",
    "    station_status_txt = Path(station_status_txt)\n",
    "    station_information_csv = Path(station_information_csv)\n",
    "    trip_csv = Path(trip_csv)\n",
    "\n",
    "    census_blocks_path = Path(cfg[\"census_blocks_path\"])\n",
    "    tracts_path = Path(cfg[\"tracts_path\"])\n",
    "    centroid_tract_csv = Path(cfg[\"centroid_tract_csv\"])\n",
    "    centerline_path = Path(cfg[\"centerline_path\"])\n",
    "    bike_lanes_path = Path(cfg[\"bike_lanes_path\"])\n",
    "\n",
    "    blocks_id_col = str(cfg[\"blocks_id_col\"])\n",
    "    tract_id_col = str(cfg[\"tract_id_col\"])\n",
    "    external_tract_prefix = cfg.get(\"external_tract_prefix\", None)\n",
    "\n",
    "    if drop_staten_island is None:\n",
    "        drop_staten_island = bool(cfg.get(\"drop_staten_island_default\", False))\n",
    "\n",
    "    out_dir = Path(output_dir) if output_dir is not None else Path(f\"./{city_key}_ALL_IN_ONE_OUTPUTS\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # existence checks\n",
    "    _ensure_exists(station_status_txt, \"station_status_txt\")\n",
    "    _ensure_exists(station_information_csv, \"station_information_csv\")\n",
    "    _ensure_exists(trip_csv, \"trip_csv\")\n",
    "    _ensure_exists(census_blocks_path, \"census_blocks_path\")\n",
    "    _ensure_exists(tracts_path, \"tracts_path\")\n",
    "    _ensure_exists(centroid_tract_csv, \"centroid_tract_csv\")\n",
    "    _ensure_exists(centerline_path, \"centerline_path\")\n",
    "    _ensure_exists(bike_lanes_path, \"bike_lanes_path\")\n",
    "\n",
    "    # normalize times\n",
    "    availability_time_start = pd.to_datetime(availability_time_start) if availability_time_start is not None else None\n",
    "    availability_time_end = pd.to_datetime(availability_time_end) if availability_time_end is not None else None\n",
    "    usage_time_start = pd.to_datetime(usage_time_start) if usage_time_start is not None else None\n",
    "    usage_time_end = pd.to_datetime(usage_time_end) if usage_time_end is not None else None\n",
    "    idle_time_start = pd.to_datetime(idle_time_start) if idle_time_start is not None else None\n",
    "    idle_time_end = pd.to_datetime(idle_time_end) if idle_time_end is not None else None\n",
    "\n",
    "    availability_group_level = availability_group_level.lower().strip()\n",
    "    if availability_group_level not in {\"block\", \"tract\", \"both\"}:\n",
    "        raise ValueError(\"availability_group_level must be one of: 'block', 'tract', 'both'\")\n",
    "\n",
    "    if peak_metric not in {\"total_vehicle_available\", \"num_docks_available\"}:\n",
    "        raise ValueError(\"peak_metric must be 'total_vehicle_available' or 'num_docks_available'\")\n",
    "\n",
    "    results: Dict[str, Any] = {\n",
    "        \"city\": city_key,\n",
    "        \"system_type\": \"docked\",\n",
    "        \"output_dir\": str(out_dir),\n",
    "        \"city_assets\": {\n",
    "            \"census_blocks_path\": str(census_blocks_path),\n",
    "            \"tracts_path\": str(tracts_path),\n",
    "            \"centroid_tract_csv\": str(centroid_tract_csv),\n",
    "            \"centerline_path\": str(centerline_path),\n",
    "            \"bike_lanes_path\": str(bike_lanes_path),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # ==================================================\n",
    "    # STEP 0 â€” station_status JSONL flatten\n",
    "    # ==================================================\n",
    "    with station_status_txt.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    di_list = []\n",
    "    for item in lines:\n",
    "        if not item.strip():\n",
    "            continue\n",
    "        js = json.loads(item)\n",
    "        timestamp = list(js.keys())[0]\n",
    "        for entry in js[timestamp]:\n",
    "            for vehicle_type in entry.get(\"vehicle_types_available\", []):\n",
    "                entry[f\"vehicle_type_{vehicle_type['vehicle_type_id']}_count\"] = vehicle_type[\"count\"]\n",
    "            if \"vehicle_types_available\" in entry:\n",
    "                del entry[\"vehicle_types_available\"]\n",
    "            entry[\"timestamp\"] = timestamp\n",
    "            di_list.append(entry)\n",
    "\n",
    "    station_status_flat = pd.DataFrame(di_list)\n",
    "    results[\"station_status_flat\"] = station_status_flat\n",
    "    if save_outputs:\n",
    "        station_status_flat.to_csv(out_dir / f\"{city_key.lower()}_station_status_flat.csv\", index=False)\n",
    "\n",
    "    ss_df = station_status_flat.copy()\n",
    "    ss_df[\"timestamp\"] = ss_df[\"timestamp\"].astype(str).str.replace(remove_tz_suffix, \"\", regex=False)\n",
    "    ss_df[\"timestamp\"] = pd.to_datetime(ss_df[\"timestamp\"], errors=\"coerce\")\n",
    "    ss_df[\"hour\"] = ss_df[\"timestamp\"].dt.hour\n",
    "    ss_df[\"date\"] = ss_df[\"timestamp\"].dt.date\n",
    "    cols_to_drop = [c for c in drop_cols if c in ss_df.columns]\n",
    "    if cols_to_drop:\n",
    "        ss_df = ss_df.drop(cols_to_drop, axis=1)\n",
    "\n",
    "    if \"station_id\" not in ss_df.columns:\n",
    "        raise ValueError(\"station_status data missing station_id after flattening.\")\n",
    "    ss_df[\"station_id\"] = ss_df[\"station_id\"].astype(str).str.strip()\n",
    "\n",
    "    results[\"station_status_clean\"] = ss_df\n",
    "    if save_outputs:\n",
    "        ss_df.to_csv(out_dir / f\"{city_key.lower()}_station_status_clean.csv\", index=False)\n",
    "\n",
    "    # ==================================================\n",
    "    # STEP 0 â€” station_information -> standardized + census block/tract\n",
    "    # ==================================================\n",
    "    si_raw = pd.read_csv(station_information_csv)\n",
    "    results[\"station_info_raw\"] = si_raw\n",
    "    if save_outputs:\n",
    "        si_raw.to_csv(out_dir / f\"{city_key.lower()}_station_information_raw.csv\", index=False)\n",
    "\n",
    "    si = _standardize_station_info(si_raw)\n",
    "\n",
    "    # If census_block missing, compute via spatial join\n",
    "    if (\"census_block\" not in si.columns) or si[\"census_block\"].isna().any():\n",
    "        import geopandas as gpd\n",
    "        from shapely.geometry import Point\n",
    "\n",
    "        pts = si.drop_duplicates(subset=[\"lat\", \"lon\"])[[\"lon\", \"lat\"]].copy()\n",
    "        pts_gdf = gpd.GeoDataFrame(\n",
    "            pts,\n",
    "            geometry=[Point(xy) for xy in zip(pts[\"lon\"], pts[\"lat\"])],\n",
    "            crs=\"EPSG:4326\",\n",
    "        )\n",
    "\n",
    "        blocks_wgs84 = gpd.read_file(str(census_blocks_path)).to_crs(epsg=4326)\n",
    "        if blocks_id_col not in blocks_wgs84.columns:\n",
    "            raise ValueError(f\"Block shapefile missing '{blocks_id_col}' for station geocoding.\")\n",
    "\n",
    "        joined = gpd.sjoin(pts_gdf, blocks_wgs84[[blocks_id_col, \"geometry\"]], how=\"left\", predicate=\"within\")\n",
    "        joined = joined.rename(columns={blocks_id_col: \"census_block\"})\n",
    "        joined[\"census_block\"] = joined[\"census_block\"].astype(str)\n",
    "        joined[\"census_tract\"] = _tract_from_block(joined[\"census_block\"]).astype(str)\n",
    "\n",
    "        si = si.merge(joined[[\"lon\", \"lat\", \"census_block\", \"census_tract\"]], on=[\"lon\", \"lat\"], how=\"left\", suffixes=(\"\", \"_sj\"))\n",
    "\n",
    "        if \"census_block_sj\" in si.columns:\n",
    "            si[\"census_block\"] = si.get(\"census_block\").fillna(si[\"census_block_sj\"]) if \"census_block\" in si.columns else si[\"census_block_sj\"]\n",
    "            si = si.drop(columns=[\"census_block_sj\"], errors=\"ignore\")\n",
    "\n",
    "        if \"census_tract_sj\" in si.columns:\n",
    "            if \"census_tract\" not in si.columns:\n",
    "                si[\"census_tract\"] = pd.NA\n",
    "            si[\"census_tract\"] = si[\"census_tract\"].fillna(si[\"census_tract_sj\"])\n",
    "            si = si.drop(columns=[\"census_tract_sj\"], errors=\"ignore\")\n",
    "\n",
    "        # Census API fill for remaining missing (optional)\n",
    "        if fill_missing_with_census_api and (\"census_block\" in si.columns) and si[\"census_block\"].isna().any():\n",
    "            miss = si[si[\"census_block\"].isna()][[\"lat\", \"lon\"]].drop_duplicates().copy()\n",
    "            miss[\"census_block_new\"] = [\n",
    "                _census_api_block(lat, lon, census_geocoder_benchmark, census_geocoder_vintage)\n",
    "                for lat, lon in zip(miss[\"lat\"], miss[\"lon\"])\n",
    "            ]\n",
    "            miss[\"census_tract_new\"] = _tract_from_block(miss[\"census_block_new\"]).astype(str)\n",
    "\n",
    "            si = si.merge(miss, on=[\"lat\", \"lon\"], how=\"left\")\n",
    "            si[\"census_block\"] = si[\"census_block\"].fillna(si[\"census_block_new\"])\n",
    "            if \"census_tract\" not in si.columns:\n",
    "                si[\"census_tract\"] = pd.NA\n",
    "            si[\"census_tract\"] = si[\"census_tract\"].fillna(si[\"census_tract_new\"])\n",
    "            si = si.drop(columns=[\"census_block_new\", \"census_tract_new\"], errors=\"ignore\")\n",
    "\n",
    "    si[\"census_block\"] = si[\"census_block\"].astype(str)\n",
    "    if \"census_tract\" not in si.columns or si[\"census_tract\"].isna().any():\n",
    "        si[\"census_tract\"] = _tract_from_block(si[\"census_block\"]).astype(str)\n",
    "    else:\n",
    "        si[\"census_tract\"] = si[\"census_tract\"].astype(str)\n",
    "\n",
    "    results[\"station_info_done\"] = si\n",
    "    if save_outputs:\n",
    "        si.to_csv(out_dir / f\"{city_key.lower()}_station_information_done.csv\", index=False)\n",
    "\n",
    "    # Merge blocks into station status\n",
    "    ss_done = ss_df.merge(si[[\"station_id\", \"census_block\"]], on=\"station_id\", how=\"left\")\n",
    "    ss_done[\"census_block\"] = ss_done[\"census_block\"].astype(str)\n",
    "    ss_done[\"census_tract\"] = _tract_from_block(ss_done[\"census_block\"]).astype(str)\n",
    "\n",
    "    # Availability column standardization (tolerant to missing ebikes)\n",
    "    if \"num_bikes_available\" not in ss_done.columns:\n",
    "        raise ValueError(\n",
    "            \"station_status_done is missing 'num_bikes_available'. \"\n",
    "            f\"Available columns: {list(ss_done.columns)[:200]}\"\n",
    "        )\n",
    "\n",
    "    if \"num_ebikes_available\" not in ss_done.columns:\n",
    "        ss_done[\"num_ebikes_available\"] = 0\n",
    "\n",
    "    if \"num_docks_available\" not in ss_done.columns:\n",
    "        alt = None\n",
    "        for c in [\"docks_available\", \"num_docks_free\", \"num_docks_open\"]:\n",
    "            if c in ss_done.columns:\n",
    "                alt = c\n",
    "                break\n",
    "        if alt is None:\n",
    "            raise ValueError(\n",
    "                \"station_status_done is missing 'num_docks_available' and no alternative dock column was found. \"\n",
    "                f\"Available columns: {list(ss_done.columns)[:200]}\"\n",
    "            )\n",
    "        ss_done[\"num_docks_available\"] = pd.to_numeric(ss_done[alt], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    ss_done[\"num_bikes_available\"] = pd.to_numeric(ss_done[\"num_bikes_available\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    ss_done[\"num_ebikes_available\"] = pd.to_numeric(ss_done[\"num_ebikes_available\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    ss_done[\"num_docks_available\"] = pd.to_numeric(ss_done[\"num_docks_available\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    ss_done[\"total_vehicle_available\"] = ss_done[\"num_bikes_available\"] + ss_done[\"num_ebikes_available\"]\n",
    "\n",
    "    results[\"station_status_done\"] = ss_done\n",
    "    if save_outputs:\n",
    "        ss_done.to_csv(out_dir / f\"{city_key.lower()}_station_status_done.csv\", index=False)\n",
    "\n",
    "    # ==================================================\n",
    "    # AVAILABILITY\n",
    "    # ==================================================\n",
    "    ss_av = ss_done.copy()\n",
    "    ss_av[\"timestamp\"] = pd.to_datetime(ss_av[\"timestamp\"], errors=\"coerce\")\n",
    "    if ss_av[\"timestamp\"].isna().any():\n",
    "        raise ValueError(\"Found unparseable timestamps in station_status_done.\")\n",
    "\n",
    "    data_min = ss_av[\"timestamp\"].min()\n",
    "    data_max = ss_av[\"timestamp\"].max()\n",
    "\n",
    "    if availability_time_start is not None:\n",
    "        ss_av = ss_av[ss_av[\"timestamp\"] >= availability_time_start].copy()\n",
    "    if availability_time_end is not None:\n",
    "        ss_av = ss_av[ss_av[\"timestamp\"] < availability_time_end].copy()\n",
    "\n",
    "    if ss_av.empty:\n",
    "        raise ValueError(f\"No availability rows after time filter. Data range: [{data_min}, {data_max}]\")\n",
    "\n",
    "    def _availability_for_level(level: str) -> Dict[str, pd.DataFrame]:\n",
    "        geo_key = \"census_block\" if level == \"block\" else \"census_tract\"\n",
    "\n",
    "        temp = ss_av.copy()\n",
    "        temp[\"time_slot\"] = temp[\"timestamp\"].dt.floor(availability_time_granularity)\n",
    "\n",
    "        geo_ts = (\n",
    "            temp.groupby([geo_key, \"timestamp\"], as_index=False)\n",
    "            .agg(\n",
    "                num_bikes_available=(\"num_bikes_available\", \"sum\"),\n",
    "                num_ebikes_available=(\"num_ebikes_available\", \"sum\"),\n",
    "                num_docks_available=(\"num_docks_available\", \"sum\"),\n",
    "                total_vehicle_available=(\"total_vehicle_available\", \"sum\"),\n",
    "            )\n",
    "        )\n",
    "        geo_ts[\"time_slot\"] = geo_ts[\"timestamp\"].dt.floor(availability_time_granularity)\n",
    "\n",
    "        raw_df = (\n",
    "            geo_ts.groupby([geo_key, \"time_slot\"], as_index=False)\n",
    "            .mean(numeric_only=True)\n",
    "            .round(0)\n",
    "            .sort_values([geo_key, \"time_slot\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        for c in [\"num_bikes_available\", \"num_ebikes_available\", \"num_docks_available\", \"total_vehicle_available\"]:\n",
    "            raw_df[c] = raw_df[c].astype(int)\n",
    "\n",
    "        if level == \"tract\" and external_tract_prefix:\n",
    "            raw_df = raw_df[~raw_df[\"census_tract\"].astype(str).str.startswith(external_tract_prefix)].copy()\n",
    "\n",
    "        norm_df = raw_df.copy()\n",
    "        for c in [\"num_bikes_available\", \"num_ebikes_available\", \"num_docks_available\", \"total_vehicle_available\"]:\n",
    "            norm_df[c + \"_norm\"] = _minmax_norm(norm_df[c]).round(5)\n",
    "\n",
    "        if save_outputs:\n",
    "            raw_df.to_csv(out_dir / f\"availability__raw__{level}.csv\", index=False)\n",
    "            norm_df.to_csv(out_dir / f\"availability__norm__{level}.csv\", index=False)\n",
    "\n",
    "        return {\"raw\": raw_df, \"norm\": norm_df}\n",
    "\n",
    "    availability_out: Dict[str, Any] = {\"meta\": {\n",
    "        \"time_start\": availability_time_start,\n",
    "        \"time_end\": availability_time_end,\n",
    "        \"data_min\": data_min,\n",
    "        \"data_max\": data_max,\n",
    "        \"time_granularity\": availability_time_granularity,\n",
    "        \"group_level\": availability_group_level,\n",
    "    }}\n",
    "\n",
    "    if availability_group_level in {\"block\", \"both\"}:\n",
    "        blk = _availability_for_level(\"block\")\n",
    "        availability_out[\"availability_block_raw\"] = blk[\"raw\"]\n",
    "        availability_out[\"availability_block_norm\"] = blk[\"norm\"]\n",
    "\n",
    "    if availability_group_level in {\"tract\", \"both\"}:\n",
    "        tr = _availability_for_level(\"tract\")\n",
    "        availability_out[\"availability_tract_raw\"] = tr[\"raw\"]\n",
    "        availability_out[\"availability_tract_norm\"] = tr[\"norm\"]\n",
    "\n",
    "    if \"availability_tract_norm\" not in availability_out:\n",
    "        tr = _availability_for_level(\"tract\")\n",
    "        availability_out[\"availability_tract_raw\"] = tr[\"raw\"]\n",
    "        availability_out[\"availability_tract_norm\"] = tr[\"norm\"]\n",
    "\n",
    "    results[\"availability\"] = availability_out\n",
    "\n",
    "    availability_df_for_capacity = availability_out[\"availability_tract_norm\"].copy()\n",
    "    availability_df_for_capacity[\"census_tract\"] = availability_df_for_capacity[\"census_tract\"].astype(str)\n",
    "    availability_df_for_capacity[\"time_slot\"] = pd.to_datetime(availability_df_for_capacity[\"time_slot\"])\n",
    "\n",
    "    # ==================================================\n",
    "    # CAPACITY\n",
    "    # ==================================================\n",
    "    import geopandas as gpd\n",
    "\n",
    "    tracts_gdf = gpd.read_file(str(tracts_path))\n",
    "    if tracts_gdf.crs is None:\n",
    "        raise ValueError(f\"Tracts shapefile CRS is missing: {tracts_path}\")\n",
    "    if tract_id_col not in tracts_gdf.columns:\n",
    "        raise ValueError(f\"Tract shapefile missing '{tract_id_col}' column.\")\n",
    "\n",
    "    if cfg.get(\"has_boro_logic\", False):\n",
    "        tracts_gdf[\"county_fips\"] = tracts_gdf[\"STATEFP\"].astype(str) + tracts_gdf[\"COUNTYFP\"].astype(str)\n",
    "        county_to_boro = {\n",
    "            \"36005\": \"The Bronx\",\n",
    "            \"36047\": \"Brooklyn\",\n",
    "            \"36061\": \"Manhattan\",\n",
    "            \"36081\": \"Queens\",\n",
    "            \"36085\": \"Staten Island\",\n",
    "        }\n",
    "        tracts_gdf[\"boro\"] = tracts_gdf[\"county_fips\"].map(county_to_boro)\n",
    "        if drop_staten_island:\n",
    "            tracts_gdf = tracts_gdf[tracts_gdf[\"boro\"] != \"Staten Island\"].copy()\n",
    "\n",
    "    centroids_wgs84 = gpd.GeoSeries(tracts_gdf.geometry.centroid, crs=tracts_gdf.crs).to_crs(epsg=4326)\n",
    "    tract_centroids_df = pd.DataFrame({\n",
    "        \"census_tract\": tracts_gdf[tract_id_col].astype(str).values,\n",
    "        \"centroid_lon\": centroids_wgs84.x.values,\n",
    "        \"centroid_lat\": centroids_wgs84.y.values,\n",
    "    })\n",
    "    if \"boro\" in tracts_gdf.columns:\n",
    "        tract_centroids_df[\"boro\"] = tracts_gdf[\"boro\"].values\n",
    "\n",
    "    results[\"tract_centroids_df\"] = tract_centroids_df\n",
    "    if save_outputs:\n",
    "        tract_centroids_df.to_csv(out_dir / \"centroid_tract_computed.csv\", index=False)\n",
    "\n",
    "    station_info = si.copy()\n",
    "    station_info[\"census_block\"] = station_info[\"census_block\"].astype(str)\n",
    "    station_info[\"census_tract\"] = _tract_from_block(station_info[\"census_block\"]).astype(str)\n",
    "\n",
    "    capacity_block = (\n",
    "        station_info.groupby(\"census_block\", as_index=False)\n",
    "        .agg(total_capacity=(\"capacity\", \"sum\"), num_station=(\"station_id\", \"count\"))\n",
    "    )\n",
    "    results[\"capacity_block\"] = capacity_block\n",
    "    if save_outputs:\n",
    "        capacity_block.to_csv(out_dir / \"capacity_block.csv\", index=False)\n",
    "\n",
    "    tract_capacity = (\n",
    "        station_info.groupby(\"census_tract\", as_index=False)\n",
    "        .agg(total_capacity=(\"capacity\", \"sum\"), num_station=(\"station_id\", \"count\"))\n",
    "    )\n",
    "\n",
    "    capacity_tract = tract_centroids_df[[\"census_tract\"]].drop_duplicates().merge(\n",
    "        tract_capacity, on=\"census_tract\", how=\"left\"\n",
    "    )\n",
    "    capacity_tract[[\"total_capacity\", \"num_station\"]] = capacity_tract[[\"total_capacity\", \"num_station\"]].fillna(0)\n",
    "\n",
    "    if external_tract_prefix:\n",
    "        capacity_tract = capacity_tract[~capacity_tract[\"census_tract\"].astype(str).str.startswith(external_tract_prefix)].copy()\n",
    "\n",
    "    capacity_tract = capacity_tract.sort_values(\"census_tract\").reset_index(drop=True)\n",
    "    capacity_tract[\"total_capacity_norm\"] = _minmax_norm(capacity_tract[\"total_capacity\"]).round(5)\n",
    "    capacity_tract[\"num_station_norm\"] = _minmax_norm(capacity_tract[\"num_station\"]).round(5)\n",
    "\n",
    "    results[\"capacity_tract_norm\"] = capacity_tract\n",
    "    if save_outputs:\n",
    "        capacity_tract.to_csv(out_dir / \"capacity_tract_norm.csv\", index=False)\n",
    "\n",
    "    avail = availability_df_for_capacity.copy()\n",
    "    if external_tract_prefix:\n",
    "        avail = avail[~avail[\"census_tract\"].astype(str).str.startswith(external_tract_prefix)].copy()\n",
    "\n",
    "    if peak_time_slot is None:\n",
    "        peak_series = avail.groupby(\"time_slot\")[peak_metric].sum()\n",
    "        if peak_series.empty:\n",
    "            raise ValueError(\"No availability data after filters; cannot find peak time_slot.\")\n",
    "        peak_time_slot = peak_series.idxmax()\n",
    "    peak_time_slot = pd.to_datetime(peak_time_slot)\n",
    "\n",
    "    peak_df = avail[avail[\"time_slot\"] == peak_time_slot].copy()\n",
    "\n",
    "    vehicle_capacity = (\n",
    "        peak_df.groupby(\"census_tract\", as_index=False)[\"total_vehicle_available\"]\n",
    "        .sum()\n",
    "        .rename(columns={\"total_vehicle_available\": \"vehicle_capacity\"})\n",
    "    )\n",
    "    dock_capacity = (\n",
    "        peak_df.groupby(\"census_tract\", as_index=False)[\"num_docks_available\"]\n",
    "        .sum()\n",
    "        .rename(columns={\"num_docks_available\": \"dock_capacity\"})\n",
    "    )\n",
    "\n",
    "    capacity_df = (\n",
    "        capacity_tract.merge(vehicle_capacity, on=\"census_tract\", how=\"left\")\n",
    "        .merge(dock_capacity, on=\"census_tract\", how=\"left\")\n",
    "    )\n",
    "    capacity_df[\"vehicle_capacity\"] = capacity_df[\"vehicle_capacity\"].fillna(0)\n",
    "    capacity_df[\"dock_capacity\"] = capacity_df[\"dock_capacity\"].fillna(0)\n",
    "\n",
    "    capacity_df[\"vehicle_capacity_norm\"] = _minmax_norm(capacity_df[\"vehicle_capacity\"]).round(5)\n",
    "    capacity_df[\"dock_capacity_norm\"] = _minmax_norm(capacity_df[\"dock_capacity\"]).round(5)\n",
    "\n",
    "    capacity_df[\"occupancy_rate\"] = capacity_df.apply(\n",
    "        lambda r: (r[\"vehicle_capacity\"] / r[\"total_capacity\"]) if r[\"total_capacity\"] else 0.0, axis=1\n",
    "    )\n",
    "    capacity_df[\"return_pressure\"] = capacity_df.apply(\n",
    "        lambda r: 1.0 - (r[\"dock_capacity\"] / r[\"total_capacity\"]) if r[\"total_capacity\"] else 0.0, axis=1\n",
    "    )\n",
    "\n",
    "    results[\"capacity_df\"] = capacity_df\n",
    "    results[\"peak_time_slot\"] = peak_time_slot\n",
    "    results[\"peak_metric\"] = peak_metric\n",
    "    if save_outputs:\n",
    "        capacity_df.to_csv(out_dir / \"capacity_tract_with_vehicle_and_docks_norm.csv\", index=False)\n",
    "\n",
    "    # ==================================================\n",
    "    # SAFETY (robust WKT; CRS-safe using blocks CRS)\n",
    "    # ==================================================\n",
    "    blocks_gdf = gpd.read_file(str(census_blocks_path))\n",
    "    if blocks_gdf.crs is None:\n",
    "        raise ValueError(f\"Blocks shapefile CRS is missing: {census_blocks_path}\")\n",
    "    if blocks_id_col not in blocks_gdf.columns:\n",
    "        raise ValueError(f\"Blocks shapefile missing '{blocks_id_col}' column.\")\n",
    "\n",
    "    target_crs = blocks_gdf.crs\n",
    "    safety_kind = str(cfg.get(\"safety_input_kind\", \"shp\"))\n",
    "\n",
    "    streets_gdf = _read_lines_layer(\n",
    "        centerline_path,\n",
    "        kind=safety_kind,\n",
    "        target_crs=target_crs,\n",
    "        csv_wkt_candidates=tuple(cfg.get(\"safety_csv_wkt_candidates\", (\"geometry\", \"the_geom\", \"wkt\", \"shape\", \"line\"))),\n",
    "        csv_crs=str(cfg.get(\"safety_csv_crs\", \"EPSG:4326\")),\n",
    "    )\n",
    "    bike_gdf = _read_lines_layer(\n",
    "        bike_lanes_path,\n",
    "        kind=safety_kind,\n",
    "        target_crs=target_crs,\n",
    "        csv_wkt_candidates=tuple(cfg.get(\"safety_csv_wkt_candidates\", (\"geometry\", \"the_geom\", \"wkt\", \"shape\", \"line\"))),\n",
    "        csv_crs=str(cfg.get(\"safety_csv_crs\", \"EPSG:4326\")),\n",
    "    )\n",
    "\n",
    "    street_len = _overlay_length_by_block(streets_gdf, blocks_gdf, blocks_id_col=blocks_id_col, out_col=\"streets_leng\")\n",
    "    bike_len = _overlay_length_by_block(bike_gdf, blocks_gdf, blocks_id_col=blocks_id_col, out_col=\"total_bike_lane_length\")\n",
    "\n",
    "    protected_rule = cfg.get(\"protected_lane_rule\", {})\n",
    "    facility_col, protected_gdf = _protected_filter(bike_gdf, protected_rule)\n",
    "\n",
    "    if facility_col is None:\n",
    "        prot_len = pd.DataFrame({\"census_block\": street_len[\"census_block\"].astype(str).unique()})\n",
    "        prot_len[\"protected_bike_lane_length\"] = 0.0\n",
    "        results[\"safety_note\"] = (\n",
    "            f\"[{city_key}] No protected-lane column found. Protected length set to 0. \"\n",
    "            f\"Tried: {protected_rule.get('col_candidates')}\"\n",
    "        )\n",
    "    else:\n",
    "        prot_len = _overlay_length_by_block(\n",
    "            protected_gdf, blocks_gdf, blocks_id_col=blocks_id_col, out_col=\"protected_bike_lane_length\"\n",
    "        )\n",
    "        results[\"safety_protected_lane_col_used\"] = facility_col\n",
    "        results[\"safety_protected_lane_rule\"] = protected_rule\n",
    "\n",
    "    safety_block = (\n",
    "        street_len.merge(bike_len, on=\"census_block\", how=\"left\")\n",
    "        .merge(prot_len, on=\"census_block\", how=\"left\")\n",
    "    )\n",
    "    safety_block[\"total_bike_lane_length\"] = safety_block[\"total_bike_lane_length\"].fillna(0)\n",
    "    safety_block[\"protected_bike_lane_length\"] = safety_block[\"protected_bike_lane_length\"].fillna(0)\n",
    "    safety_block[\"bike_lane_ratio\"] = _safe_ratio(safety_block[\"total_bike_lane_length\"], safety_block[\"streets_leng\"]).round(3)\n",
    "    safety_block[\"protected_bike_lane_ratio\"] = _safe_ratio(safety_block[\"protected_bike_lane_length\"], safety_block[\"streets_leng\"]).round(3)\n",
    "    safety_block[\"census_tract\"] = _tract_from_block(safety_block[\"census_block\"]).astype(str)\n",
    "\n",
    "    # tract safety using centroid file (for region cols)\n",
    "    centroids_csv_df = pd.read_csv(centroid_tract_csv)\n",
    "    centroids_csv_df[\"census_tract\"] = centroids_csv_df[\"census_tract\"].astype(str)\n",
    "    region_cols = [\"census_tract\"]\n",
    "    for c in [\"boro\", \"county_name\", \"county\", \"state\"]:\n",
    "        if c in centroids_csv_df.columns:\n",
    "            region_cols.append(c)\n",
    "\n",
    "    safety_tract = (\n",
    "        safety_block.groupby(\"census_tract\", as_index=False)[\n",
    "            [\"streets_leng\", \"total_bike_lane_length\", \"protected_bike_lane_length\"]\n",
    "        ].sum()\n",
    "    )\n",
    "    safety_tract = centroids_csv_df[region_cols].merge(safety_tract, on=\"census_tract\", how=\"left\")\n",
    "    for c in [\"streets_leng\", \"total_bike_lane_length\", \"protected_bike_lane_length\"]:\n",
    "        safety_tract[c] = safety_tract[c].fillna(0)\n",
    "\n",
    "    safety_tract[\"bike_lane_ratio\"] = _safe_ratio(safety_tract[\"total_bike_lane_length\"], safety_tract[\"streets_leng\"])\n",
    "    safety_tract[\"protected_bike_lane_ratio\"] = _safe_ratio(safety_tract[\"protected_bike_lane_length\"], safety_tract[\"streets_leng\"])\n",
    "    safety_tract = safety_tract.sort_values(\"census_tract\").reset_index(drop=True)\n",
    "\n",
    "    service_tracts = set(capacity_df[\"census_tract\"].astype(str).unique().tolist())\n",
    "    safety_service = safety_tract[safety_tract[\"census_tract\"].astype(str).isin(service_tracts)].reset_index(drop=True)\n",
    "\n",
    "    safety_norm = safety_service.copy()\n",
    "    safety_norm[\"bike_lane_ratio_norm\"] = _minmax_norm(safety_norm[\"bike_lane_ratio\"]).round(5)\n",
    "    safety_norm[\"protected_bike_lane_ratio_norm\"] = _minmax_norm(safety_norm[\"protected_bike_lane_ratio\"]).round(5)\n",
    "\n",
    "    results[\"safety_bike_lane_block\"] = safety_block\n",
    "    results[\"safety_bike_lane_tract\"] = safety_tract\n",
    "    results[\"safety_bike_lane_service_area\"] = safety_service\n",
    "    results[\"safety_bike_lane_norm_tract\"] = safety_norm\n",
    "\n",
    "    if save_outputs:\n",
    "        safety_block.to_csv(out_dir / \"safety_bike_lane_block.csv\", index=False)\n",
    "        safety_tract.to_csv(out_dir / \"safety_bike_lane_tract.csv\", index=False)\n",
    "        safety_service.to_csv(out_dir / \"safety_bike_lane_service_area.csv\", index=False)\n",
    "        safety_norm.to_csv(out_dir / \"safety_bike_lane_norm_tract.csv\", index=False)\n",
    "\n",
    "    # ==================================================\n",
    "    # TRIPS â€” load + standardize + compute start/end census blocks if missing\n",
    "    # ==================================================\n",
    "    trip_raw = pd.read_csv(trip_csv)\n",
    "    trip_df = _standardize_trips(trip_raw)\n",
    "\n",
    "    trip_df[\"started_at\"] = pd.to_datetime(trip_df[\"started_at\"], errors=\"coerce\")\n",
    "    trip_df[\"ended_at\"] = pd.to_datetime(trip_df[\"ended_at\"], errors=\"coerce\")\n",
    "\n",
    "    # Compute blocks from lat/lng if not present\n",
    "    if (\"start_census_block\" not in trip_df.columns) or (\"end_census_block\" not in trip_df.columns):\n",
    "        if not {\"start_lat\", \"start_lng\", \"end_lat\", \"end_lng\"}.issubset(trip_df.columns):\n",
    "            raise ValueError(\n",
    "                \"Trip CSV does not have start/end census blocks AND does not have start/end lat/lng to compute them. \"\n",
    "                f\"Found: {list(trip_df.columns)[:200]}\"\n",
    "            )\n",
    "\n",
    "        start_map = _geocode_points_to_blocks_spatial(\n",
    "            trip_df, lon_col=\"start_lng\", lat_col=\"start_lat\",\n",
    "            blocks_path=census_blocks_path, blocks_id_col=blocks_id_col\n",
    "        ).rename(columns={\"census_block\": \"start_census_block\"})\n",
    "\n",
    "        end_map = _geocode_points_to_blocks_spatial(\n",
    "            trip_df, lon_col=\"end_lng\", lat_col=\"end_lat\",\n",
    "            blocks_path=census_blocks_path, blocks_id_col=blocks_id_col\n",
    "        ).rename(columns={\"census_block\": \"end_census_block\"})\n",
    "\n",
    "        trip_df = trip_df.merge(\n",
    "            start_map, left_on=[\"start_lng\", \"start_lat\"], right_on=[\"lon\", \"lat\"], how=\"left\"\n",
    "        ).drop(columns=[\"lon\", \"lat\"], errors=\"ignore\")\n",
    "\n",
    "        trip_df = trip_df.merge(\n",
    "            end_map, left_on=[\"end_lng\", \"end_lat\"], right_on=[\"lon\", \"lat\"], how=\"left\"\n",
    "        ).drop(columns=[\"lon\", \"lat\"], errors=\"ignore\")\n",
    "\n",
    "    required_trip_cols = {\"started_at\", \"ended_at\", \"start_census_block\", \"end_census_block\"}\n",
    "    miss_trip = required_trip_cols - set(trip_df.columns)\n",
    "    if miss_trip:\n",
    "        raise ValueError(\n",
    "            f\"Trip CSV is missing required columns after standardization: {miss_trip}. \"\n",
    "            f\"Available columns: {list(trip_df.columns)[:200]}\"\n",
    "        )\n",
    "\n",
    "    trip_df[\"start_census_block\"] = trip_df[\"start_census_block\"].astype(str)\n",
    "    trip_df[\"end_census_block\"] = trip_df[\"end_census_block\"].astype(str)\n",
    "\n",
    "    # default usage/idle windows\n",
    "    if usage_time_start is None:\n",
    "        usage_time_start = availability_time_start\n",
    "    if usage_time_end is None:\n",
    "        usage_time_end = availability_time_end\n",
    "    if idle_time_start is None:\n",
    "        idle_time_start = availability_time_start\n",
    "    if idle_time_end is None:\n",
    "        idle_time_end = availability_time_end\n",
    "\n",
    "    if usage_time_start is None:\n",
    "        usage_time_start = trip_df[\"started_at\"].min()\n",
    "    if usage_time_end is None:\n",
    "        usage_time_end = trip_df[\"started_at\"].max()\n",
    "    if idle_time_start is None:\n",
    "        idle_time_start = trip_df[\"started_at\"].min()\n",
    "    if idle_time_end is None:\n",
    "        idle_time_end = trip_df[\"started_at\"].max()\n",
    "\n",
    "    usage_time_start = pd.to_datetime(usage_time_start)\n",
    "    usage_time_end = pd.to_datetime(usage_time_end)\n",
    "    idle_time_start = pd.to_datetime(idle_time_start)\n",
    "    idle_time_end = pd.to_datetime(idle_time_end)\n",
    "\n",
    "    # ==================================================\n",
    "    # USAGE (5-min, hourly, tract-hourly norm)\n",
    "    # ==================================================\n",
    "    trips = trip_df.copy()\n",
    "    trips = trips[(trips[\"started_at\"] >= usage_time_start) & (trips[\"started_at\"] <= usage_time_end)].copy()\n",
    "    trips[\"trip_duration\"] = (trips[\"ended_at\"] - trips[\"started_at\"]).dt.total_seconds() / 60.0\n",
    "    trips = trips[trips[\"trip_duration\"] <= 240].copy()\n",
    "\n",
    "    # 5-min\n",
    "    trips[\"time_slot_start\"] = trips[\"started_at\"].dt.floor(\"5min\")\n",
    "    trips[\"time_slot_end\"] = trips[\"ended_at\"].dt.floor(\"5min\")\n",
    "\n",
    "    trips_starting_5 = trips.groupby([\"start_census_block\", \"time_slot_start\"]).size().reset_index(name=\"trips_starting\")\n",
    "    trips_ending_5 = trips.groupby([\"end_census_block\", \"time_slot_end\"]).size().reset_index(name=\"trips_ending\")\n",
    "\n",
    "    trips_starting_5 = trips_starting_5.rename(columns={\"start_census_block\": \"census_block\", \"time_slot_start\": \"time_slot\"})\n",
    "    trips_ending_5 = trips_ending_5.rename(columns={\"end_census_block\": \"census_block\", \"time_slot_end\": \"time_slot\"})\n",
    "\n",
    "    all_blocks = pd.unique(pd.concat([trips[\"start_census_block\"], trips[\"end_census_block\"]]).astype(str))\n",
    "    all_blocks = [b for b in all_blocks if b not in (\"nan\", \"0\")]\n",
    "\n",
    "    all_time_slots_5 = pd.date_range(start=usage_time_start.floor(\"5min\"), end=usage_time_end.ceil(\"5min\"), freq=\"5min\")\n",
    "    grid_5 = pd.MultiIndex.from_product([all_blocks, all_time_slots_5], names=[\"census_block\", \"time_slot\"]).to_frame(index=False)\n",
    "\n",
    "    usage_5min_block = (\n",
    "        grid_5.merge(trips_starting_5, on=[\"census_block\", \"time_slot\"], how=\"left\")\n",
    "             .merge(trips_ending_5, on=[\"census_block\", \"time_slot\"], how=\"left\")\n",
    "    )\n",
    "    usage_5min_block[[\"trips_starting\", \"trips_ending\"]] = usage_5min_block[[\"trips_starting\", \"trips_ending\"]].fillna(0).astype(int)\n",
    "    usage_5min_block = usage_5min_block.sort_values([\"census_block\", \"time_slot\"]).reset_index(drop=True)\n",
    "\n",
    "    if save_outputs:\n",
    "        usage_5min_block.to_csv(out_dir / \"usage_5min_block.csv\", index=False)\n",
    "\n",
    "    # hourly block\n",
    "    trips[\"hour_start\"] = trips[\"started_at\"].dt.floor(\"H\")\n",
    "    trips[\"hour_end\"] = trips[\"ended_at\"].dt.floor(\"H\")\n",
    "\n",
    "    trips_starting_h = trips.groupby([\"start_census_block\", \"hour_start\"]).size().reset_index(name=\"trips_starting\")\n",
    "    trips_ending_h = trips.groupby([\"end_census_block\", \"hour_end\"]).size().reset_index(name=\"trips_ending\")\n",
    "\n",
    "    trips_starting_h = trips_starting_h.rename(columns={\"start_census_block\": \"census_block\", \"hour_start\": \"hour\"})\n",
    "    trips_ending_h = trips_ending_h.rename(columns={\"end_census_block\": \"census_block\", \"hour_end\": \"hour\"})\n",
    "\n",
    "    all_hours = pd.date_range(start=usage_time_start.floor(\"H\"), end=usage_time_end.ceil(\"H\"), freq=\"H\")\n",
    "    grid_h = pd.MultiIndex.from_product([all_blocks, all_hours], names=[\"census_block\", \"hour\"]).to_frame(index=False)\n",
    "\n",
    "    usage_hourly_block = (\n",
    "        grid_h.merge(trips_starting_h, on=[\"census_block\", \"hour\"], how=\"left\")\n",
    "             .merge(trips_ending_h, on=[\"census_block\", \"hour\"], how=\"left\")\n",
    "    )\n",
    "    usage_hourly_block[[\"trips_starting\", \"trips_ending\"]] = usage_hourly_block[[\"trips_starting\", \"trips_ending\"]].fillna(0).astype(int)\n",
    "    usage_hourly_block = usage_hourly_block.sort_values([\"census_block\", \"hour\"]).reset_index(drop=True)\n",
    "\n",
    "    if save_outputs:\n",
    "        usage_hourly_block.to_csv(out_dir / \"usage_hourly_block.csv\", index=False)\n",
    "\n",
    "    # hourly tract\n",
    "    usage_hourly = usage_hourly_block.copy()\n",
    "    usage_hourly[\"census_block\"] = usage_hourly[\"census_block\"].astype(str)\n",
    "    usage_hourly[\"census_tract\"] = _tract_from_block(usage_hourly[\"census_block\"]).astype(str)\n",
    "    if external_tract_prefix:\n",
    "        usage_hourly = usage_hourly[~usage_hourly[\"census_tract\"].astype(str).str.startswith(external_tract_prefix)].copy()\n",
    "\n",
    "    usage_hourly[\"time_slot\"] = pd.to_datetime(usage_hourly[\"hour\"])\n",
    "    usage_hourly = usage_hourly.drop(columns=[\"hour\"])\n",
    "\n",
    "    usage_hourly_tract = (\n",
    "        usage_hourly.groupby([\"census_tract\", \"time_slot\"], as_index=False)\n",
    "        .agg(trips_starting=(\"trips_starting\", \"sum\"), trips_ending=(\"trips_ending\", \"sum\"))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    usage_hourly_tract_raw = usage_hourly_tract.copy()\n",
    "\n",
    "    if tracts_to_remove:\n",
    "        usage_hourly_tract = usage_hourly_tract[~usage_hourly_tract[\"census_tract\"].isin(tracts_to_remove)].reset_index(drop=True)\n",
    "\n",
    "    usage_hourly_tract[\"trips_starting_norm\"] = _minmax_norm(usage_hourly_tract[\"trips_starting\"]).round(5)\n",
    "    usage_hourly_tract[\"trips_ending_norm\"] = _minmax_norm(usage_hourly_tract[\"trips_ending\"]).round(5)\n",
    "\n",
    "    if save_outputs:\n",
    "        usage_hourly_tract_raw.to_csv(out_dir / \"usage_hourly_tract_raw.csv\", index=False)\n",
    "        usage_hourly_tract.to_csv(out_dir / \"usage_norm_hourly_tract.csv\", index=False)\n",
    "\n",
    "    results[\"usage_5min_block\"] = usage_5min_block\n",
    "    results[\"usage_hourly_block\"] = usage_hourly_block\n",
    "    results[\"usage_hourly_tract_raw\"] = usage_hourly_tract_raw\n",
    "    results[\"usage_norm_hourly_tract\"] = usage_hourly_tract\n",
    "\n",
    "    # ==================================================\n",
    "    # IDLE TIME (full simulation, same logic as your earlier notebook)\n",
    "    # ==================================================\n",
    "    if not compute_idle_time:\n",
    "        results[\"idle_note\"] = \"Idle time skipped (compute_idle_time=False).\"\n",
    "        return results\n",
    "\n",
    "    # Need rideable_type to split bike vs ebike; if missing -> still compute total moved (no split)\n",
    "    has_rideable = \"rideable_type\" in trip_df.columns\n",
    "\n",
    "    # GBFS input for idle (has vehicles)\n",
    "    ss_idle = ss_done.copy()\n",
    "    ss_idle[\"timestamp\"] = pd.to_datetime(ss_idle[\"timestamp\"])\n",
    "    ss_idle[\"census_block\"] = ss_idle[\"census_block\"].astype(str)\n",
    "    ss_idle[\"census_tract\"] = ss_idle[\"census_tract\"].astype(str)\n",
    "    ss_idle[\"total_available\"] = ss_idle[\"num_bikes_available\"] + ss_idle[\"num_ebikes_available\"]\n",
    "    ss_idle[\"time_5min\"] = ss_idle[\"timestamp\"].dt.floor(\"5min\")\n",
    "\n",
    "    trips_idle = trip_df.copy()\n",
    "    trips_idle[\"start_census_block\"] = trips_idle[\"start_census_block\"].astype(str)\n",
    "    trips_idle[\"end_census_block\"] = trips_idle[\"end_census_block\"].astype(str)\n",
    "    trips_idle[\"started_at\"] = pd.to_datetime(trips_idle[\"started_at\"])\n",
    "    trips_idle[\"ended_at\"] = pd.to_datetime(trips_idle[\"ended_at\"])\n",
    "\n",
    "    # drop external if requested (NYC drops NJ)\n",
    "    if external_tract_prefix:\n",
    "        start_tract = _tract_from_block(trips_idle[\"start_census_block\"]).astype(str)\n",
    "        end_tract = _tract_from_block(trips_idle[\"end_census_block\"]).astype(str)\n",
    "        trips_idle = trips_idle[\n",
    "            (~start_tract.str.startswith(external_tract_prefix)) & (~end_tract.str.startswith(external_tract_prefix))\n",
    "        ].copy()\n",
    "\n",
    "    # time filter\n",
    "    trips_idle = trips_idle[(trips_idle[\"started_at\"] >= idle_time_start) & (trips_idle[\"started_at\"] <= idle_time_end)].copy()\n",
    "\n",
    "    # 5-min buckets\n",
    "    trips_idle[\"time_slot_start\"] = trips_idle[\"started_at\"].dt.floor(\"5min\")\n",
    "    trips_idle[\"time_slot_end\"] = trips_idle[\"ended_at\"].dt.floor(\"5min\")\n",
    "\n",
    "    # If rideable_type exists, compute starting/ending by type (bike vs ebike)\n",
    "    if has_rideable:\n",
    "        trips_idle = _map_rideable_type_to_two_columns(trips_idle)\n",
    "\n",
    "        # starting: bike/ebike counts\n",
    "        ts_start = (\n",
    "            trips_idle.groupby([\"start_census_block\", \"time_slot_start\", \"_is_ebike\"])\n",
    "            .size()\n",
    "            .unstack(fill_value=0)\n",
    "            .reset_index()\n",
    "        )\n",
    "        ts_start = ts_start.rename(columns={\"start_census_block\": \"census_block\", \"time_slot_start\": \"time_slot\"})\n",
    "        if True not in ts_start.columns:\n",
    "            ts_start[True] = 0\n",
    "        if False not in ts_start.columns:\n",
    "            ts_start[False] = 0\n",
    "        ts_start = ts_start.rename(columns={False: \"trips_starting_bike\", True: \"trips_starting_ebike\"})\n",
    "\n",
    "        # ending: bike/ebike counts\n",
    "        ts_end = (\n",
    "            trips_idle.groupby([\"end_census_block\", \"time_slot_end\", \"_is_ebike\"])\n",
    "            .size()\n",
    "            .unstack(fill_value=0)\n",
    "            .reset_index()\n",
    "        )\n",
    "        ts_end = ts_end.rename(columns={\"end_census_block\": \"census_block\", \"time_slot_end\": \"time_slot\"})\n",
    "        if True not in ts_end.columns:\n",
    "            ts_end[True] = 0\n",
    "        if False not in ts_end.columns:\n",
    "            ts_end[False] = 0\n",
    "        ts_end = ts_end.rename(columns={False: \"trips_ending_bike\", True: \"trips_ending_ebike\"})\n",
    "\n",
    "    else:\n",
    "        # No rideable_type: only total starting/ending\n",
    "        ts_start = trips_idle.groupby([\"start_census_block\", \"time_slot_start\"]).size().reset_index(name=\"trips_starting\")\n",
    "        ts_end = trips_idle.groupby([\"end_census_block\", \"time_slot_end\"]).size().reset_index(name=\"trips_ending\")\n",
    "        ts_start = ts_start.rename(columns={\"start_census_block\": \"census_block\", \"time_slot_start\": \"time_slot\"})\n",
    "        ts_end = ts_end.rename(columns={\"end_census_block\": \"census_block\", \"time_slot_end\": \"time_slot\"})\n",
    "\n",
    "    # full grid\n",
    "    all_blocks_idle = pd.unique(pd.concat([trips_idle[\"start_census_block\"], trips_idle[\"end_census_block\"]]))\n",
    "    all_blocks_idle = pd.Series(all_blocks_idle).dropna().astype(str).unique()\n",
    "\n",
    "    all_time_slots_idle = pd.date_range(\n",
    "        start=idle_time_start.floor(\"5min\"),\n",
    "        end=idle_time_end.ceil(\"5min\"),\n",
    "        freq=\"5min\",\n",
    "    )\n",
    "    grid_idle = pd.MultiIndex.from_product(\n",
    "        [all_blocks_idle, all_time_slots_idle],\n",
    "        names=[\"census_block\", \"time_slot\"],\n",
    "    ).to_frame(index=False)\n",
    "\n",
    "    if has_rideable:\n",
    "        result_complete = grid_idle.merge(ts_start, on=[\"census_block\", \"time_slot\"], how=\"left\").merge(\n",
    "            ts_end, on=[\"census_block\", \"time_slot\"], how=\"left\"\n",
    "        )\n",
    "        cols = [\"trips_starting_bike\", \"trips_starting_ebike\", \"trips_ending_bike\", \"trips_ending_ebike\"]\n",
    "        for c in cols:\n",
    "            if c not in result_complete.columns:\n",
    "                result_complete[c] = 0\n",
    "            result_complete[c] = result_complete[c].fillna(0).astype(int)\n",
    "\n",
    "        result_complete[\"trips_starting\"] = result_complete[\"trips_starting_bike\"] + result_complete[\"trips_starting_ebike\"]\n",
    "        result_complete[\"trips_ending\"] = result_complete[\"trips_ending_bike\"] + result_complete[\"trips_ending_ebike\"]\n",
    "\n",
    "    else:\n",
    "        result_complete = grid_idle.merge(ts_start, on=[\"census_block\", \"time_slot\"], how=\"left\").merge(\n",
    "            ts_end, on=[\"census_block\", \"time_slot\"], how=\"left\"\n",
    "        )\n",
    "        for c in [\"trips_starting\", \"trips_ending\"]:\n",
    "            if c not in result_complete.columns:\n",
    "                result_complete[c] = 0\n",
    "            result_complete[c] = result_complete[c].fillna(0).astype(int)\n",
    "\n",
    "    result_complete = result_complete.sort_values([\"census_block\", \"time_slot\"]).reset_index(drop=True)\n",
    "\n",
    "    # station availability by block 5-min\n",
    "    station_availability_df = (\n",
    "        ss_idle.groupby([\"census_block\", \"time_5min\"], as_index=False)[\n",
    "            [\"num_bikes_available\", \"num_ebikes_available\", \"total_available\"]\n",
    "        ].sum()\n",
    "    )\n",
    "    station_availability_df = station_availability_df.rename(columns={\"time_5min\": \"time_slot\"})\n",
    "    station_availability_df[\"time_slot\"] = station_availability_df[\"time_slot\"] - pd.Timedelta(minutes=5)\n",
    "\n",
    "    merged_idle = station_availability_df.merge(result_complete, on=[\"census_block\", \"time_slot\"], how=\"left\")\n",
    "    if has_rideable:\n",
    "        cols = [\"trips_starting_bike\", \"trips_starting_ebike\", \"trips_ending_bike\", \"trips_ending_ebike\", \"trips_starting\", \"trips_ending\"]\n",
    "        for c in cols:\n",
    "            if c not in merged_idle.columns:\n",
    "                merged_idle[c] = 0\n",
    "            merged_idle[c] = merged_idle[c].fillna(0).astype(int)\n",
    "\n",
    "        merged_idle[\"vehicles_moved_bike\"] = merged_idle[\"trips_starting_bike\"] + merged_idle[\"trips_ending_bike\"]\n",
    "        merged_idle[\"vehicles_moved_ebike\"] = merged_idle[\"trips_starting_ebike\"] + merged_idle[\"trips_ending_ebike\"]\n",
    "        merged_idle[\"vehicles_moved\"] = merged_idle[\"trips_starting\"] + merged_idle[\"trips_ending\"]\n",
    "    else:\n",
    "        for c in [\"trips_starting\", \"trips_ending\"]:\n",
    "            if c not in merged_idle.columns:\n",
    "                merged_idle[c] = 0\n",
    "            merged_idle[c] = merged_idle[c].fillna(0).astype(int)\n",
    "        merged_idle[\"vehicles_moved\"] = merged_idle[\"trips_starting\"] + merged_idle[\"trips_ending\"]\n",
    "\n",
    "    merged_idle[\"idle_vehicles\"] = (merged_idle[\"total_available\"] - merged_idle[\"vehicles_moved\"]).clip(lower=0)\n",
    "    merged_idle[\"time_slot\"] = pd.to_datetime(merged_idle[\"time_slot\"])\n",
    "    merged_idle = merged_idle.sort_values([\"census_block\", \"time_slot\"])\n",
    "    merged_idle[\"hour\"] = merged_idle[\"time_slot\"].dt.floor(\"H\")\n",
    "\n",
    "    # hourly idle simulation per block\n",
    "    from collections import deque\n",
    "\n",
    "    idle_results = []\n",
    "    for block, block_df in merged_idle.groupby(\"census_block\"):\n",
    "        for hour, hour_df in block_df.groupby(\"hour\"):\n",
    "            hour_df = hour_df.sort_values(\"time_slot\")\n",
    "            virtual_vehicles = deque()\n",
    "            finalized_durations = []\n",
    "\n",
    "            for _, row in hour_df.iterrows():\n",
    "                current_time = row[\"time_slot\"]\n",
    "                current_idle = int(row[\"idle_vehicles\"])\n",
    "\n",
    "                if current_idle > len(virtual_vehicles):\n",
    "                    for _ in range(current_idle - len(virtual_vehicles)):\n",
    "                        virtual_vehicles.append(current_time)\n",
    "                elif current_idle < len(virtual_vehicles):\n",
    "                    for _ in range(len(virtual_vehicles) - current_idle):\n",
    "                        start_t = virtual_vehicles.popleft()\n",
    "                        duration = (current_time - start_t).total_seconds() / 60.0\n",
    "                        finalized_durations.append(duration)\n",
    "\n",
    "            final_time = hour + pd.Timedelta(hours=1)\n",
    "            while virtual_vehicles:\n",
    "                start_t = virtual_vehicles.popleft()\n",
    "                duration = (final_time - start_t).total_seconds() / 60.0\n",
    "                finalized_durations.append(duration)\n",
    "\n",
    "            avg_idle_time = round(sum(finalized_durations) / len(finalized_durations), 2) if finalized_durations else 0.0\n",
    "            idle_results.append({\"census_block\": block, \"hour\": hour, \"avg_idle_time\": avg_idle_time})\n",
    "\n",
    "    idle_time_block = pd.DataFrame(idle_results)\n",
    "\n",
    "    # tract aggregation + normalize\n",
    "    idle_time_block[\"census_block\"] = idle_time_block[\"census_block\"].astype(str)\n",
    "    idle_time_block[\"census_tract\"] = _tract_from_block(idle_time_block[\"census_block\"]).astype(str)\n",
    "\n",
    "    idle_time_tract = idle_time_block.groupby([\"census_tract\", \"hour\"], as_index=False)[\"avg_idle_time\"].mean()\n",
    "\n",
    "    if external_tract_prefix:\n",
    "        idle_time_tract = idle_time_tract[\n",
    "            ~idle_time_tract[\"census_tract\"].astype(str).str.startswith(external_tract_prefix)\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "    idle_time_tract[\"avg_idle_time_norm\"] = _minmax_norm(idle_time_tract[\"avg_idle_time\"]).round(5)\n",
    "    idle_time_tract = idle_time_tract.rename(columns={\"hour\": \"time_slot\"})\n",
    "\n",
    "    results[\"idle_merged_5min\"] = merged_idle\n",
    "    results[\"idle_time_block\"] = idle_time_block\n",
    "    results[\"idle_time_tract_raw\"] = idle_time_tract.drop(columns=[\"avg_idle_time_norm\"])\n",
    "    results[\"idle_time_norm_tract\"] = idle_time_tract\n",
    "\n",
    "    if save_outputs:\n",
    "        merged_idle.to_csv(out_dir / \"idle_merged_5min.csv\", index=False)\n",
    "        idle_time_block.to_csv(out_dir / \"idle_time_block.csv\", index=False)\n",
    "        results[\"idle_time_tract_raw\"].to_csv(out_dir / \"idle_time_tract_raw.csv\", index=False)\n",
    "        idle_time_tract.to_csv(out_dir / \"idle_time_norm_tract.csv\", index=False)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45fe2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_10600\\2635220460.py:720: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  temp[\"time_slot\"] = temp[\"timestamp\"].dt.floor(availability_time_granularity)\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_10600\\2635220460.py:731: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  geo_ts[\"time_slot\"] = geo_ts[\"timestamp\"].dt.floor(availability_time_granularity)\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_10600\\2635220460.py:720: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  temp[\"time_slot\"] = temp[\"timestamp\"].dt.floor(availability_time_granularity)\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_10600\\2635220460.py:731: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  geo_ts[\"time_slot\"] = geo_ts[\"timestamp\"].dt.floor(availability_time_granularity)\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_10600\\2635220460.py:811: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroids_wgs84 = gpd.GeoSeries(tracts_gdf.geometry.centroid, crs=tracts_gdf.crs).to_crs(epsg=4326)\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_10600\\2635220460.py:329: UserWarning: Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  inter[\"seg_len\"] = inter.geometry.length\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_10600\\2635220460.py:329: UserWarning: Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  inter[\"seg_len\"] = inter.geometry.length\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_10600\\2635220460.py:329: UserWarning: Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  inter[\"seg_len\"] = inter.geometry.length\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_10600\\2635220460.py:1105: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  trips[\"hour_start\"] = trips[\"started_at\"].dt.floor(\"H\")\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_10600\\2635220460.py:1106: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  trips[\"hour_end\"] = trips[\"ended_at\"].dt.floor(\"H\")\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_10600\\2635220460.py:1114: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  all_hours = pd.date_range(start=usage_time_start.floor(\"H\"), end=usage_time_end.ceil(\"H\"), freq=\"H\")\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_10600\\2635220460.py:1305: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  merged_idle[\"hour\"] = merged_idle[\"time_slot\"].dt.floor(\"H\")\n"
     ]
    }
   ],
   "source": [
    "out = run_docked_all_utilities_single_function(\n",
    "    city=\"SF\", # city_key=\"NJ\" / \"NYC\" / \"PITT\" / \"SF\"\n",
    "    station_status_txt=r\"D:\\Research Fellowship\\Summer Research Stuff\\Collected Data\\Week 1\\09-June\\san_fran_baywheels_docked_station_status_6_9.txt\",\n",
    "    station_information_csv=r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\GBFS_Census_Tract\\San_Fran_Baywheels\\San_fran_Baywheels station information 06_09.csv\",\n",
    "    trip_csv=r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Usage\\San_Fran_Baywheels\\202505-baywheels-tripdata.csv\",\n",
    "    output_dir=\"San_Fran_Baywheels_outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf4fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, Dict, Any, Sequence, Tuple\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# CITY CONFIG (hardcoded assets for NYC + NJ + PITT + SF)\n",
    "# ==================================================\n",
    "CITY_CONFIG: Dict[str, Dict[str, Any]] = {\n",
    "    \"NYC\": {\n",
    "        \"census_blocks_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\GBFS_Census_Tract\\NYC\\tl_2024_36_tabblock20.shp\"\n",
    "        ),\n",
    "        \"tracts_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\NYC\\tl_2024_36_tract.shp\"\n",
    "        ),\n",
    "        \"centroid_tract_csv\": Path(\n",
    "            r\"D:\\Research Fellowship\\Capacity_NYC\\centroid_tract_computed.csv\"\n",
    "        ),\n",
    "        # safety (NYC = CSV WKT)\n",
    "        \"centerline_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NYC\\CSCL_PlowNYC_20250619.csv\"\n",
    "        ),\n",
    "        \"bike_lanes_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NYC\\New_York_City_Bike_Routes_20250619.csv\"\n",
    "        ),\n",
    "        \"safety_input_kind\": \"csv_wkt\",\n",
    "        \"safety_csv_wkt_candidates\": (\"geometry\", \"the_geom\", \"wkt\", \"geometry_wkt\", \"WKT\", \"geom\", \"shape\", \"line\"),\n",
    "        \"safety_csv_crs\": \"EPSG:4326\",\n",
    "        \"blocks_id_col\": \"GEOID20\",\n",
    "        \"tract_id_col\": \"GEOID\",\n",
    "        \"external_tract_prefix\": \"34\",  # drop NJ tracts for NYC results\n",
    "        \"has_boro_logic\": True,\n",
    "        \"drop_staten_island_default\": True,\n",
    "        \"protected_lane_rule\": {\n",
    "            \"col_candidates\": (\"facilitycl\", \"facility\", \"class\", \"ft\", \"type\"),\n",
    "            \"match_type\": \"equals\",\n",
    "            \"match_value\": \"I\",\n",
    "        },\n",
    "    },\n",
    "    \"NJ\": {\n",
    "        \"census_blocks_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NJ\\tl_2024_34_tabblock20.shp\"\n",
    "        ),\n",
    "        \"tracts_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\NJ\\tl_2024_34_tract.shp\"\n",
    "        ),\n",
    "        \"centroid_tract_csv\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\NJ\\centroid_tract_nj.csv\"\n",
    "        ),\n",
    "        # safety (NJ = SHP)\n",
    "        \"centerline_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NJ\\Tran_road.shp\"\n",
    "        ),\n",
    "        \"bike_lanes_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NJ\\bike-lanes-2020-division-of-transportation.shp\"\n",
    "        ),\n",
    "        \"safety_input_kind\": \"shp\",\n",
    "        \"blocks_id_col\": \"GEOID20\",\n",
    "        \"tract_id_col\": \"GEOID\",\n",
    "        \"external_tract_prefix\": None,\n",
    "        \"has_boro_logic\": False,\n",
    "        \"drop_staten_island_default\": False,\n",
    "        \"protected_lane_rule\": {\n",
    "            \"col_candidates\": (\"type\", \"facility\", \"facilitycl\", \"class\", \"lane_type\"),\n",
    "            \"match_type\": \"contains\",\n",
    "            \"match_value\": \"PROTECT\",\n",
    "        },\n",
    "    },\n",
    "    \"PITT\": {\n",
    "        \"census_blocks_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Pitt\\tl_2024_42_tabblock20.shp\"\n",
    "        ),\n",
    "        \"tracts_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\Pitt\\tl_2024_42_tract.shp\"\n",
    "        ),\n",
    "        \"centroid_tract_csv\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\Pitt\\centroid_tract_pa.csv\"\n",
    "        ),\n",
    "        # safety (PITT = SHP)\n",
    "        \"centerline_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Pitt\\Pittsburgh_Street_Centerline.shp\"\n",
    "        ),\n",
    "        \"bike_lanes_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Pitt\\Bike Map\\Bike Lanes.shp\"\n",
    "        ),\n",
    "        \"safety_input_kind\": \"shp\",\n",
    "        \"blocks_id_col\": \"GEOID20\",\n",
    "        \"tract_id_col\": \"GEOID\",\n",
    "        \"external_tract_prefix\": None,\n",
    "        \"has_boro_logic\": False,\n",
    "        \"drop_staten_island_default\": False,\n",
    "        \"protected_lane_rule\": {\n",
    "            \"col_candidates\": (\"facility\", \"type\", \"class\", \"status\", \"lane_type\", \"BIKE_FACIL\", \"CATEGORY\"),\n",
    "            \"match_type\": \"contains\",\n",
    "            \"match_value\": \"PROTECT\",\n",
    "        },\n",
    "    },\n",
    "    \"SF\": {\n",
    "        \"census_blocks_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Usage\\San_Fran_Baywheels\\tl_2024_06_tabblock20.shp\"\n",
    "        ),\n",
    "        \"tracts_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\San_Fran_Baywheels\\tl_2024_06_tract.shp\"\n",
    "        ),\n",
    "        \"centroid_tract_csv\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\San_Fran_Baywheels\\centroid_tract_ca.csv\"\n",
    "        ),\n",
    "        # safety (SF = CSV WKT)\n",
    "        \"centerline_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\San_Fran_Baywheels\\Streets___Active_and_Retired_20250626 (1).csv\"\n",
    "        ),\n",
    "        \"bike_lanes_path\": Path(\n",
    "            r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\San_Fran_Baywheels\\Bikelane.csv\"\n",
    "        ),\n",
    "        \"safety_input_kind\": \"csv_wkt\",\n",
    "        \"safety_csv_wkt_candidates\": (\"geometry\", \"shape\", \"line\", \"the_geom\", \"wkt\", \"geometry_wkt\", \"WKT\", \"geom\"),\n",
    "        \"safety_csv_crs\": \"EPSG:4326\",\n",
    "        \"blocks_id_col\": \"GEOID20\",\n",
    "        \"tract_id_col\": \"GEOID\",\n",
    "        \"external_tract_prefix\": None,\n",
    "        \"has_boro_logic\": False,\n",
    "        \"drop_staten_island_default\": False,\n",
    "        # protected heuristic: non-empty BARRIER OR keywords in FACILITY_T\n",
    "        \"protected_lane_rule\": {\n",
    "            \"col_candidates\": (\"BARRIER\", \"FACILITY_T\", \"BUFFERED\", \"RAISED\", \"SYMBOLOGY\"),\n",
    "            \"match_type\": \"not_empty_or_contains\",\n",
    "            \"match_value\": r\"PROTECT|SEPARAT|CYCLETRACK|BARRIER|RAISED|BUFFER\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# Helpers\n",
    "# ==================================================\n",
    "def _ensure_exists(p: Path, label: str) -> None:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"{label} not found: {p}\")\n",
    "\n",
    "\n",
    "def _minmax_norm(series: pd.Series) -> pd.Series:\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    mn = s.min(skipna=True)\n",
    "    mx = s.max(skipna=True)\n",
    "    if pd.isna(mn) or pd.isna(mx) or mx <= mn:\n",
    "        return pd.Series(0.0, index=s.index)\n",
    "    return (s - mn) / (mx - mn)\n",
    "\n",
    "\n",
    "def _safe_ratio(numer: pd.Series, denom: pd.Series) -> pd.Series:\n",
    "    return numer.div(denom.where(denom > 0, other=pd.NA)).fillna(0)\n",
    "\n",
    "\n",
    "def _pick_col(df: pd.DataFrame, candidates: Sequence[str], required: bool, label: str) -> Optional[str]:\n",
    "    cols = set(df.columns)\n",
    "    for c in candidates:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    if required:\n",
    "        raise ValueError(\n",
    "            f\"Could not find required column for {label}. Tried {list(candidates)}. \"\n",
    "            f\"Found columns (sample): {list(df.columns)[:200]}\"\n",
    "        )\n",
    "    return None\n",
    "\n",
    "\n",
    "def _tract_from_block(block_series: pd.Series) -> pd.Series:\n",
    "    s = block_series.astype(str)\n",
    "    # If it looks like a 15-digit block GEOID -> tract is first 11 OR strip last 4\n",
    "    return s.where(s.str.len() < 15, s.str[:-4])\n",
    "\n",
    "\n",
    "def _standardize_station_info(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    sid = _pick_col(df, [\"station_id\", \"stationId\", \"id\", \"Station ID\", \"Station Id\"], True, \"station_id\")\n",
    "    lat = _pick_col(df, [\"lat\", \"latitude\", \"station_lat\", \"y\"], True, \"lat\")\n",
    "    lon = _pick_col(df, [\"lon\", \"lng\", \"longitude\", \"station_lon\", \"x\"], True, \"lon\")\n",
    "    cap = _pick_col(df, [\"capacity\", \"dock_count\", \"num_docks\", \"total_docks\"], True, \"capacity\")\n",
    "\n",
    "    out = df.copy().rename(columns={sid: \"station_id\", lat: \"lat\", lon: \"lon\", cap: \"capacity\"})\n",
    "    out[\"station_id\"] = out[\"station_id\"].astype(str).str.strip()\n",
    "\n",
    "    cb = _pick_col(out, [\"census_block\", \"block_geoid\", \"GEOID20\"], False, \"census_block\")\n",
    "    if cb and cb != \"census_block\":\n",
    "        out = out.rename(columns={cb: \"census_block\"})\n",
    "\n",
    "    ct = _pick_col(out, [\"census_tract\", \"tract_geoid\", \"GEOID\"], False, \"census_tract\")\n",
    "    if ct and ct != \"census_tract\":\n",
    "        out = out.rename(columns={ct: \"census_tract\"})\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _standardize_trips(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    started = _pick_col(\n",
    "        df,\n",
    "        [\"started_at\", \"start_time\", \"trip_start_time\", \"starttime\", \"Start Date\", \"start_date\", \"StartDate\"],\n",
    "        True,\n",
    "        \"started_at\",\n",
    "    )\n",
    "    ended = _pick_col(\n",
    "        df,\n",
    "        [\"ended_at\", \"end_time\", \"trip_end_time\", \"stoptime\", \"End Date\", \"end_date\", \"EndDate\"],\n",
    "        True,\n",
    "        \"ended_at\",\n",
    "    )\n",
    "    out = df.copy().rename(columns={started: \"started_at\", ended: \"ended_at\"})\n",
    "\n",
    "    rt = _pick_col(out, [\"rideable_type\", \"bike_type\", \"vehicle_type\", \"Rider Type\", \"rider_type\"], False, \"rideable_type\")\n",
    "    if rt and rt != \"rideable_type\":\n",
    "        out = out.rename(columns={rt: \"rideable_type\"})\n",
    "\n",
    "    # precomputed blocks if present\n",
    "    sb = _pick_col(out, [\"start_census_block\", \"start_block\", \"start_block_geoid\"], False, \"start_census_block\")\n",
    "    eb = _pick_col(out, [\"end_census_block\", \"end_block\", \"end_block_geoid\"], False, \"end_census_block\")\n",
    "    if sb and sb != \"start_census_block\":\n",
    "        out = out.rename(columns={sb: \"start_census_block\"})\n",
    "    if eb and eb != \"end_census_block\":\n",
    "        out = out.rename(columns={eb: \"end_census_block\"})\n",
    "\n",
    "    # lat/lon fallbacks\n",
    "    slat = _pick_col(out, [\"start_lat\", \"startLatitude\", \"StartLat\"], False, \"start_lat\")\n",
    "    slng = _pick_col(out, [\"start_lng\", \"start_lon\", \"startLongitude\", \"StartLng\"], False, \"start_lng\")\n",
    "    elat = _pick_col(out, [\"end_lat\", \"endLatitude\", \"EndLat\"], False, \"end_lat\")\n",
    "    elng = _pick_col(out, [\"end_lng\", \"end_lon\", \"endLongitude\", \"EndLng\"], False, \"end_lng\")\n",
    "    if slat and slat != \"start_lat\":\n",
    "        out = out.rename(columns={slat: \"start_lat\"})\n",
    "    if slng and slng != \"start_lng\":\n",
    "        out = out.rename(columns={slng: \"start_lng\"})\n",
    "    if elat and elat != \"end_lat\":\n",
    "        out = out.rename(columns={elat: \"end_lat\"})\n",
    "    if elng and elng != \"end_lng\":\n",
    "        out = out.rename(columns={elng: \"end_lng\"})\n",
    "\n",
    "    # station ids (optional)\n",
    "    ssid = _pick_col(out, [\"start_station_id\", \"Start Station Id\", \"Start Station ID\"], False, \"start_station_id\")\n",
    "    esid = _pick_col(out, [\"end_station_id\", \"End Station Id\", \"End Station ID\"], False, \"end_station_id\")\n",
    "    if ssid and ssid != \"start_station_id\":\n",
    "        out = out.rename(columns={ssid: \"start_station_id\"})\n",
    "    if esid and esid != \"end_station_id\":\n",
    "        out = out.rename(columns={esid: \"end_station_id\"})\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _read_lines_layer(\n",
    "    path: Path,\n",
    "    *,\n",
    "    kind: str,\n",
    "    target_crs,\n",
    "    csv_wkt_candidates: Tuple[str, ...],\n",
    "    csv_crs: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Robust for SF/NYC CSV-WKT:\n",
    "      - auto-picks the WKT column by sampling values\n",
    "      - safe parses WKT (drops non-string/NaN)\n",
    "    \"\"\"\n",
    "    import geopandas as gpd\n",
    "    from shapely import wkt\n",
    "\n",
    "    if kind == \"shp\":\n",
    "        gdf = gpd.read_file(str(path))\n",
    "        if gdf.crs is None:\n",
    "            raise ValueError(f\"CRS missing for {path}. Define CRS before projecting.\")\n",
    "        return gdf.to_crs(target_crs)\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    def _looks_like_wkt(v: Any) -> bool:\n",
    "        if not isinstance(v, str):\n",
    "            return False\n",
    "        s = v.strip().upper()\n",
    "        return s.startswith((\"LINESTRING\", \"MULTILINESTRING\", \"GEOMETRYCOLLECTION\"))\n",
    "\n",
    "    wkt_col = None\n",
    "    for c in csv_wkt_candidates:\n",
    "        if c in df.columns:\n",
    "            sample = df[c].dropna().astype(str).head(80)\n",
    "            if not sample.empty and sample.map(_looks_like_wkt).mean() >= 0.2:\n",
    "                wkt_col = c\n",
    "                break\n",
    "\n",
    "    if wkt_col is None:\n",
    "        raise ValueError(\n",
    "            f\"Could not find a usable WKT column in {path.name}. \"\n",
    "            f\"Tried candidates: {list(csv_wkt_candidates)}. \"\n",
    "            f\"Found columns: {list(df.columns)[:120]}\"\n",
    "        )\n",
    "\n",
    "    def _safe_wkt_load(x: Any):\n",
    "        if not isinstance(x, str):\n",
    "            return None\n",
    "        x = x.strip()\n",
    "        if not x:\n",
    "            return None\n",
    "        try:\n",
    "            return wkt.loads(x)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"geometry\"] = df[wkt_col].apply(_safe_wkt_load)\n",
    "    df = df[df[\"geometry\"].notna()].copy()\n",
    "    if df.empty:\n",
    "        raise ValueError(\n",
    "            f\"After parsing WKT, no valid geometries remained for {path.name}. \"\n",
    "            f\"Chosen WKT column: '{wkt_col}'.\"\n",
    "        )\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=\"geometry\", crs=csv_crs)\n",
    "    return gdf.to_crs(target_crs)\n",
    "\n",
    "\n",
    "def _overlay_length_by_block(lines_gdf, blocks_gdf, *, blocks_id_col: str, out_col: str) -> pd.DataFrame:\n",
    "    import geopandas as gpd\n",
    "\n",
    "    blocks = blocks_gdf[[blocks_id_col, \"geometry\"]].copy()\n",
    "    lines = lines_gdf[[\"geometry\"]].copy()\n",
    "\n",
    "    inter = gpd.overlay(lines, blocks, how=\"intersection\", keep_geom_type=False)\n",
    "    if inter.empty:\n",
    "        return pd.DataFrame({\"census_block\": pd.Series([], dtype=str), out_col: pd.Series([], dtype=float)})\n",
    "\n",
    "    inter[\"seg_len\"] = inter.geometry.length\n",
    "    inter = inter.rename(columns={blocks_id_col: \"census_block\"})\n",
    "    out = inter.groupby(\"census_block\", as_index=False)[\"seg_len\"].sum().rename(columns={\"seg_len\": out_col})\n",
    "    out[out_col] = out[out_col].round(3)\n",
    "    out[\"census_block\"] = out[\"census_block\"].astype(str)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _protected_filter(lines_gdf, rule: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Supported match_type:\n",
    "      - equals\n",
    "      - contains\n",
    "      - not_empty\n",
    "      - not_empty_or_contains\n",
    "    \"\"\"\n",
    "    col = _pick_col(lines_gdf, list(rule.get(\"col_candidates\", [])), required=False, label=\"protected lane col\")\n",
    "    if col is None:\n",
    "        return None, lines_gdf\n",
    "\n",
    "    s = lines_gdf[col].astype(str)\n",
    "    match_type = str(rule.get(\"match_type\", \"equals\")).lower()\n",
    "    match_value = str(rule.get(\"match_value\", \"\"))\n",
    "\n",
    "    if match_type == \"equals\":\n",
    "        mask = s.str.upper() == match_value.upper()\n",
    "    elif match_type == \"contains\":\n",
    "        mask = s.str.upper().str.contains(match_value.upper(), na=False)\n",
    "    elif match_type == \"not_empty\":\n",
    "        s2 = lines_gdf[col]\n",
    "        mask = s2.notna() & (s.astype(str).str.strip() != \"\") & (~s.astype(str).str.lower().isin([\"0\", \"false\", \"none\", \"nan\"]))\n",
    "    elif match_type == \"not_empty_or_contains\":\n",
    "        s2 = lines_gdf[col]\n",
    "        non_empty = s2.notna() & (s.astype(str).str.strip() != \"\") & (~s.astype(str).str.lower().isin([\"0\", \"false\", \"none\", \"nan\"]))\n",
    "        contains = s.astype(str).str.contains(match_value, case=False, na=False, regex=True)\n",
    "        mask = non_empty | contains\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported match_type='{match_type}' in protected_lane_rule\")\n",
    "\n",
    "    return col, lines_gdf[mask].copy()\n",
    "\n",
    "\n",
    "def _geocode_points_to_blocks_spatial(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    lon_col: str,\n",
    "    lat_col: str,\n",
    "    blocks_path: Path,\n",
    "    blocks_id_col: str,\n",
    ") -> pd.DataFrame:\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Point\n",
    "\n",
    "    blocks = gpd.read_file(str(blocks_path)).to_crs(epsg=4326)\n",
    "    if blocks_id_col not in blocks.columns:\n",
    "        raise ValueError(f\"Block shapefile missing '{blocks_id_col}' for spatial geocoding.\")\n",
    "\n",
    "    pts = df[[lon_col, lat_col]].dropna().drop_duplicates().copy()\n",
    "    pts = pts.rename(columns={lon_col: \"lon\", lat_col: \"lat\"})\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        pts,\n",
    "        geometry=[Point(xy) for xy in zip(pts[\"lon\"], pts[\"lat\"])],\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "    joined = gpd.sjoin(gdf, blocks[[blocks_id_col, \"geometry\"]], how=\"left\", predicate=\"within\")\n",
    "    joined = joined.rename(columns={blocks_id_col: \"census_block\"})\n",
    "    out = joined[[\"lon\", \"lat\", \"census_block\"]].copy()\n",
    "    out[\"census_block\"] = out[\"census_block\"].astype(str)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _census_api_block(lat: float, lon: float, benchmark: str, vintage: str) -> Optional[str]:\n",
    "    import requests\n",
    "    try:\n",
    "        url = (\n",
    "            \"https://geocoding.geo.census.gov/geocoder/geographies/coordinates\"\n",
    "            f\"?x={lon}&y={lat}\"\n",
    "            f\"&benchmark={benchmark}\"\n",
    "            f\"&vintage={vintage}\"\n",
    "            \"&format=json\"\n",
    "        )\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        js = r.json()\n",
    "        blocks = js[\"result\"][\"geographies\"].get(\"2020 Census Blocks\", [])\n",
    "        if not blocks:\n",
    "            return None\n",
    "        return blocks[0].get(\"GEOID\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _map_rideable_type_to_two_columns(trips: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create columns:\n",
    "      - trips_starting_bike, trips_starting_ebike\n",
    "      - trips_ending_bike, trips_ending_ebike\n",
    "    using rideable_type values like classic_bike/electric_bike (or similar strings).\n",
    "    \"\"\"\n",
    "    df = trips.copy()\n",
    "    if \"rideable_type\" not in df.columns:\n",
    "        return df\n",
    "\n",
    "    t = df[\"rideable_type\"].astype(str).str.lower()\n",
    "    df[\"_is_ebike\"] = t.str.contains(\"electric|ebike|e-bike|assist\", na=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# MAIN FUNCTION\n",
    "# ==================================================\n",
    "def run_docked_all_utilities_single_function(\n",
    "    *,\n",
    "    city: str,\n",
    "    station_status_txt: Union[str, Path],\n",
    "    station_information_csv: Union[str, Path],\n",
    "    trip_csv: Union[str, Path],\n",
    "    output_dir: Optional[Union[str, Path]] = None,\n",
    "    save_outputs: bool = True,\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    # TIME WINDOW (unified)\n",
    "    # ------------------------------------------------------------\n",
    "    time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    \n",
    "    compute_idle_time: bool = True,\n",
    "    \n",
    "    # Specific overrides (will inherit from time_start/time_end if None)\n",
    "    availability_time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    availability_time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    usage_time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    usage_time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    idle_time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    idle_time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    \n",
    "    availability_time_granularity: str = \"1H\",\n",
    "    availability_group_level: str = \"both\",\n",
    "    peak_time_slot: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    peak_metric: str = \"total_vehicle_available\",\n",
    "    drop_staten_island: Optional[bool] = None,\n",
    "    tracts_to_remove: Optional[Sequence[str]] = None,\n",
    "    remove_tz_suffix: str = \" EDT\",\n",
    "    drop_cols: tuple[str, ...] = (\n",
    "        \"num_scooters_available\",\n",
    "        \"num_scooters_unavailable\",\n",
    "        \"is_installed\",\n",
    "        \"is_returning\",\n",
    "        \"is_renting\",\n",
    "    ),\n",
    "    fill_missing_with_census_api: bool = True,\n",
    "    census_geocoder_benchmark: str = \"Public_AR_Census2020\",\n",
    "    census_geocoder_vintage: str = \"Census2020_Current\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    One-function multi-city docked pipeline (NYC/NJ/PITT/SF), including idle simulation.\n",
    "    Accepts global time_start/time_end which propagates to all utilities.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize city keys\n",
    "    city_key = city.strip().upper()\n",
    "    if city_key in {\"PITTSBURGH\", \"PIT\"}:\n",
    "        city_key = \"PITT\"\n",
    "    if city_key in {\"SANFRAN\", \"SAN_FRAN\", \"SAN FRANCISCO\", \"BAYWHEELS\", \"BAY WHEELS\"}:\n",
    "        city_key = \"SF\"\n",
    "\n",
    "    if city_key not in CITY_CONFIG:\n",
    "        raise ValueError(f\"Unsupported city='{city}'. Supported: {list(CITY_CONFIG.keys())}\")\n",
    "\n",
    "    cfg = CITY_CONFIG[city_key]\n",
    "\n",
    "    station_status_txt = Path(station_status_txt)\n",
    "    station_information_csv = Path(station_information_csv)\n",
    "    trip_csv = Path(trip_csv)\n",
    "\n",
    "    census_blocks_path = Path(cfg[\"census_blocks_path\"])\n",
    "    tracts_path = Path(cfg[\"tracts_path\"])\n",
    "    centroid_tract_csv = Path(cfg[\"centroid_tract_csv\"])\n",
    "    centerline_path = Path(cfg[\"centerline_path\"])\n",
    "    bike_lanes_path = Path(cfg[\"bike_lanes_path\"])\n",
    "\n",
    "    blocks_id_col = str(cfg[\"blocks_id_col\"])\n",
    "    tract_id_col = str(cfg[\"tract_id_col\"])\n",
    "    external_tract_prefix = cfg.get(\"external_tract_prefix\", None)\n",
    "\n",
    "    if drop_staten_island is None:\n",
    "        drop_staten_island = bool(cfg.get(\"drop_staten_island_default\", False))\n",
    "\n",
    "    out_dir = Path(output_dir) if output_dir is not None else Path(f\"./{city_key}_ALL_IN_ONE_OUTPUTS\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # existence checks\n",
    "    _ensure_exists(station_status_txt, \"station_status_txt\")\n",
    "    _ensure_exists(station_information_csv, \"station_information_csv\")\n",
    "    _ensure_exists(trip_csv, \"trip_csv\")\n",
    "    _ensure_exists(census_blocks_path, \"census_blocks_path\")\n",
    "    _ensure_exists(tracts_path, \"tracts_path\")\n",
    "    _ensure_exists(centroid_tract_csv, \"centroid_tract_csv\")\n",
    "    _ensure_exists(centerline_path, \"centerline_path\")\n",
    "    _ensure_exists(bike_lanes_path, \"bike_lanes_path\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TIME WINDOW LOGIC\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1. Normalize global inputs\n",
    "    ts_start = pd.to_datetime(time_start) if time_start is not None else None\n",
    "    ts_end = pd.to_datetime(time_end) if time_end is not None else None\n",
    "\n",
    "    # 2. Propagate to specific params if they are None\n",
    "    if availability_time_start is None:\n",
    "        availability_time_start = ts_start\n",
    "    if availability_time_end is None:\n",
    "        availability_time_end = ts_end\n",
    "        \n",
    "    if usage_time_start is None:\n",
    "        usage_time_start = ts_start\n",
    "    if usage_time_end is None:\n",
    "        usage_time_end = ts_end\n",
    "        \n",
    "    if idle_time_start is None:\n",
    "        idle_time_start = ts_start\n",
    "    if idle_time_end is None:\n",
    "        idle_time_end = ts_end\n",
    "\n",
    "    # 3. Ensure they are datetime objects\n",
    "    availability_time_start = pd.to_datetime(availability_time_start) if availability_time_start else None\n",
    "    availability_time_end = pd.to_datetime(availability_time_end) if availability_time_end else None\n",
    "    usage_time_start = pd.to_datetime(usage_time_start) if usage_time_start else None\n",
    "    usage_time_end = pd.to_datetime(usage_time_end) if usage_time_end else None\n",
    "    idle_time_start = pd.to_datetime(idle_time_start) if idle_time_start else None\n",
    "    idle_time_end = pd.to_datetime(idle_time_end) if idle_time_end else None\n",
    "\n",
    "    availability_group_level = availability_group_level.lower().strip()\n",
    "    if availability_group_level not in {\"block\", \"tract\", \"both\"}:\n",
    "        raise ValueError(\"availability_group_level must be one of: 'block', 'tract', 'both'\")\n",
    "\n",
    "    if peak_metric not in {\"total_vehicle_available\", \"num_docks_available\"}:\n",
    "        raise ValueError(\"peak_metric must be 'total_vehicle_available' or 'num_docks_available'\")\n",
    "\n",
    "    results: Dict[str, Any] = {\n",
    "        \"city\": city_key,\n",
    "        \"system_type\": \"docked\",\n",
    "        \"output_dir\": str(out_dir),\n",
    "        \"time_window\": {\n",
    "            \"global_start\": str(ts_start),\n",
    "            \"global_end\": str(ts_end)\n",
    "        },\n",
    "        \"city_assets\": {\n",
    "            \"census_blocks_path\": str(census_blocks_path),\n",
    "            \"tracts_path\": str(tracts_path),\n",
    "            \"centroid_tract_csv\": str(centroid_tract_csv),\n",
    "            \"centerline_path\": str(centerline_path),\n",
    "            \"bike_lanes_path\": str(bike_lanes_path),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # ==================================================\n",
    "    # STEP 0 â€” station_status JSONL flatten\n",
    "    # ==================================================\n",
    "    with station_status_txt.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    di_list = []\n",
    "    for item in lines:\n",
    "        if not item.strip():\n",
    "            continue\n",
    "        js = json.loads(item)\n",
    "        timestamp = list(js.keys())[0]\n",
    "        for entry in js[timestamp]:\n",
    "            for vehicle_type in entry.get(\"vehicle_types_available\", []):\n",
    "                entry[f\"vehicle_type_{vehicle_type['vehicle_type_id']}_count\"] = vehicle_type[\"count\"]\n",
    "            if \"vehicle_types_available\" in entry:\n",
    "                del entry[\"vehicle_types_available\"]\n",
    "            entry[\"timestamp\"] = timestamp\n",
    "            di_list.append(entry)\n",
    "\n",
    "    station_status_flat = pd.DataFrame(di_list)\n",
    "    results[\"station_status_flat\"] = station_status_flat\n",
    "    if save_outputs:\n",
    "        station_status_flat.to_csv(out_dir / f\"{city_key.lower()}_station_status_flat.csv\", index=False)\n",
    "\n",
    "    ss_df = station_status_flat.copy()\n",
    "    ss_df[\"timestamp\"] = ss_df[\"timestamp\"].astype(str).str.replace(remove_tz_suffix, \"\", regex=False)\n",
    "    ss_df[\"timestamp\"] = pd.to_datetime(ss_df[\"timestamp\"], errors=\"coerce\")\n",
    "    ss_df[\"hour\"] = ss_df[\"timestamp\"].dt.hour\n",
    "    ss_df[\"date\"] = ss_df[\"timestamp\"].dt.date\n",
    "    cols_to_drop = [c for c in drop_cols if c in ss_df.columns]\n",
    "    if cols_to_drop:\n",
    "        ss_df = ss_df.drop(cols_to_drop, axis=1)\n",
    "\n",
    "    if \"station_id\" not in ss_df.columns:\n",
    "        raise ValueError(\"station_status data missing station_id after flattening.\")\n",
    "    ss_df[\"station_id\"] = ss_df[\"station_id\"].astype(str).str.strip()\n",
    "\n",
    "    results[\"station_status_clean\"] = ss_df\n",
    "    if save_outputs:\n",
    "        ss_df.to_csv(out_dir / f\"{city_key.lower()}_station_status_clean.csv\", index=False)\n",
    "\n",
    "    # ==================================================\n",
    "    # STEP 0 â€” station_information -> standardized + census block/tract\n",
    "    # ==================================================\n",
    "    si_raw = pd.read_csv(station_information_csv)\n",
    "    results[\"station_info_raw\"] = si_raw\n",
    "    if save_outputs:\n",
    "        si_raw.to_csv(out_dir / f\"{city_key.lower()}_station_information_raw.csv\", index=False)\n",
    "\n",
    "    si = _standardize_station_info(si_raw)\n",
    "\n",
    "    # If census_block missing, compute via spatial join\n",
    "    if (\"census_block\" not in si.columns) or si[\"census_block\"].isna().any():\n",
    "        import geopandas as gpd\n",
    "        from shapely.geometry import Point\n",
    "\n",
    "        pts = si.drop_duplicates(subset=[\"lat\", \"lon\"])[[\"lon\", \"lat\"]].copy()\n",
    "        pts_gdf = gpd.GeoDataFrame(\n",
    "            pts,\n",
    "            geometry=[Point(xy) for xy in zip(pts[\"lon\"], pts[\"lat\"])],\n",
    "            crs=\"EPSG:4326\",\n",
    "        )\n",
    "\n",
    "        blocks_wgs84 = gpd.read_file(str(census_blocks_path)).to_crs(epsg=4326)\n",
    "        if blocks_id_col not in blocks_wgs84.columns:\n",
    "            raise ValueError(f\"Block shapefile missing '{blocks_id_col}' for station geocoding.\")\n",
    "\n",
    "        joined = gpd.sjoin(pts_gdf, blocks_wgs84[[blocks_id_col, \"geometry\"]], how=\"left\", predicate=\"within\")\n",
    "        joined = joined.rename(columns={blocks_id_col: \"census_block\"})\n",
    "        joined[\"census_block\"] = joined[\"census_block\"].astype(str)\n",
    "        joined[\"census_tract\"] = _tract_from_block(joined[\"census_block\"]).astype(str)\n",
    "\n",
    "        si = si.merge(joined[[\"lon\", \"lat\", \"census_block\", \"census_tract\"]], on=[\"lon\", \"lat\"], how=\"left\", suffixes=(\"\", \"_sj\"))\n",
    "\n",
    "        if \"census_block_sj\" in si.columns:\n",
    "            si[\"census_block\"] = si.get(\"census_block\").fillna(si[\"census_block_sj\"]) if \"census_block\" in si.columns else si[\"census_block_sj\"]\n",
    "            si = si.drop(columns=[\"census_block_sj\"], errors=\"ignore\")\n",
    "\n",
    "        if \"census_tract_sj\" in si.columns:\n",
    "            if \"census_tract\" not in si.columns:\n",
    "                si[\"census_tract\"] = pd.NA\n",
    "            si[\"census_tract\"] = si[\"census_tract\"].fillna(si[\"census_tract_sj\"])\n",
    "            si = si.drop(columns=[\"census_tract_sj\"], errors=\"ignore\")\n",
    "\n",
    "        # Census API fill for remaining missing (optional)\n",
    "        if fill_missing_with_census_api and (\"census_block\" in si.columns) and si[\"census_block\"].isna().any():\n",
    "            miss = si[si[\"census_block\"].isna()][[\"lat\", \"lon\"]].drop_duplicates().copy()\n",
    "            miss[\"census_block_new\"] = [\n",
    "                _census_api_block(lat, lon, census_geocoder_benchmark, census_geocoder_vintage)\n",
    "                for lat, lon in zip(miss[\"lat\"], miss[\"lon\"])\n",
    "            ]\n",
    "            miss[\"census_tract_new\"] = _tract_from_block(miss[\"census_block_new\"]).astype(str)\n",
    "\n",
    "            si = si.merge(miss, on=[\"lat\", \"lon\"], how=\"left\")\n",
    "            si[\"census_block\"] = si[\"census_block\"].fillna(si[\"census_block_new\"])\n",
    "            if \"census_tract\" not in si.columns:\n",
    "                si[\"census_tract\"] = pd.NA\n",
    "            si[\"census_tract\"] = si[\"census_tract\"].fillna(si[\"census_tract_new\"])\n",
    "            si = si.drop(columns=[\"census_block_new\", \"census_tract_new\"], errors=\"ignore\")\n",
    "\n",
    "    si[\"census_block\"] = si[\"census_block\"].astype(str)\n",
    "    if \"census_tract\" not in si.columns or si[\"census_tract\"].isna().any():\n",
    "        si[\"census_tract\"] = _tract_from_block(si[\"census_block\"]).astype(str)\n",
    "    else:\n",
    "        si[\"census_tract\"] = si[\"census_tract\"].astype(str)\n",
    "\n",
    "    results[\"station_info_done\"] = si\n",
    "    if save_outputs:\n",
    "        si.to_csv(out_dir / f\"{city_key.lower()}_station_information_done.csv\", index=False)\n",
    "\n",
    "    # Merge blocks into station status\n",
    "    ss_done = ss_df.merge(si[[\"station_id\", \"census_block\"]], on=\"station_id\", how=\"left\")\n",
    "    ss_done[\"census_block\"] = ss_done[\"census_block\"].astype(str)\n",
    "    ss_done[\"census_tract\"] = _tract_from_block(ss_done[\"census_block\"]).astype(str)\n",
    "\n",
    "    # Availability column standardization (tolerant to missing ebikes)\n",
    "    if \"num_bikes_available\" not in ss_done.columns:\n",
    "        raise ValueError(\n",
    "            \"station_status_done is missing 'num_bikes_available'. \"\n",
    "            f\"Available columns: {list(ss_done.columns)[:200]}\"\n",
    "        )\n",
    "\n",
    "    if \"num_ebikes_available\" not in ss_done.columns:\n",
    "        ss_done[\"num_ebikes_available\"] = 0\n",
    "\n",
    "    if \"num_docks_available\" not in ss_done.columns:\n",
    "        alt = None\n",
    "        for c in [\"docks_available\", \"num_docks_free\", \"num_docks_open\"]:\n",
    "            if c in ss_done.columns:\n",
    "                alt = c\n",
    "                break\n",
    "        if alt is None:\n",
    "            raise ValueError(\n",
    "                \"station_status_done is missing 'num_docks_available' and no alternative dock column was found. \"\n",
    "                f\"Available columns: {list(ss_done.columns)[:200]}\"\n",
    "            )\n",
    "        ss_done[\"num_docks_available\"] = pd.to_numeric(ss_done[alt], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    ss_done[\"num_bikes_available\"] = pd.to_numeric(ss_done[\"num_bikes_available\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    ss_done[\"num_ebikes_available\"] = pd.to_numeric(ss_done[\"num_ebikes_available\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    ss_done[\"num_docks_available\"] = pd.to_numeric(ss_done[\"num_docks_available\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    ss_done[\"total_vehicle_available\"] = ss_done[\"num_bikes_available\"] + ss_done[\"num_ebikes_available\"]\n",
    "\n",
    "    results[\"station_status_done\"] = ss_done\n",
    "    if save_outputs:\n",
    "        ss_done.to_csv(out_dir / f\"{city_key.lower()}_station_status_done.csv\", index=False)\n",
    "\n",
    "    # ==================================================\n",
    "    # AVAILABILITY\n",
    "    # ==================================================\n",
    "    ss_av = ss_done.copy()\n",
    "    ss_av[\"timestamp\"] = pd.to_datetime(ss_av[\"timestamp\"], errors=\"coerce\")\n",
    "    if ss_av[\"timestamp\"].isna().any():\n",
    "        raise ValueError(\"Found unparseable timestamps in station_status_done.\")\n",
    "\n",
    "    data_min = ss_av[\"timestamp\"].min()\n",
    "    data_max = ss_av[\"timestamp\"].max()\n",
    "\n",
    "    if availability_time_start is not None:\n",
    "        ss_av = ss_av[ss_av[\"timestamp\"] >= availability_time_start].copy()\n",
    "    if availability_time_end is not None:\n",
    "        ss_av = ss_av[ss_av[\"timestamp\"] < availability_time_end].copy()\n",
    "\n",
    "    if ss_av.empty:\n",
    "        raise ValueError(f\"No availability rows after time filter. Data range: [{data_min}, {data_max}]\")\n",
    "\n",
    "    def _availability_for_level(level: str) -> Dict[str, pd.DataFrame]:\n",
    "        geo_key = \"census_block\" if level == \"block\" else \"census_tract\"\n",
    "\n",
    "        temp = ss_av.copy()\n",
    "        temp[\"time_slot\"] = temp[\"timestamp\"].dt.floor(availability_time_granularity)\n",
    "\n",
    "        geo_ts = (\n",
    "            temp.groupby([geo_key, \"timestamp\"], as_index=False)\n",
    "            .agg(\n",
    "                num_bikes_available=(\"num_bikes_available\", \"sum\"),\n",
    "                num_ebikes_available=(\"num_ebikes_available\", \"sum\"),\n",
    "                num_docks_available=(\"num_docks_available\", \"sum\"),\n",
    "                total_vehicle_available=(\"total_vehicle_available\", \"sum\"),\n",
    "            )\n",
    "        )\n",
    "        geo_ts[\"time_slot\"] = geo_ts[\"timestamp\"].dt.floor(availability_time_granularity)\n",
    "\n",
    "        raw_df = (\n",
    "            geo_ts.groupby([geo_key, \"time_slot\"], as_index=False)\n",
    "            .mean(numeric_only=True)\n",
    "            .round(0)\n",
    "            .sort_values([geo_key, \"time_slot\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        for c in [\"num_bikes_available\", \"num_ebikes_available\", \"num_docks_available\", \"total_vehicle_available\"]:\n",
    "            raw_df[c] = raw_df[c].astype(int)\n",
    "\n",
    "        if level == \"tract\" and external_tract_prefix:\n",
    "            raw_df = raw_df[~raw_df[\"census_tract\"].astype(str).str.startswith(external_tract_prefix)].copy()\n",
    "\n",
    "        norm_df = raw_df.copy()\n",
    "        for c in [\"num_bikes_available\", \"num_ebikes_available\", \"num_docks_available\", \"total_vehicle_available\"]:\n",
    "            norm_df[c + \"_norm\"] = _minmax_norm(norm_df[c]).round(5)\n",
    "\n",
    "        if save_outputs:\n",
    "            raw_df.to_csv(out_dir / f\"availability__raw__{level}.csv\", index=False)\n",
    "            norm_df.to_csv(out_dir / f\"availability__norm__{level}.csv\", index=False)\n",
    "\n",
    "        return {\"raw\": raw_df, \"norm\": norm_df}\n",
    "\n",
    "    availability_out: Dict[str, Any] = {\"meta\": {\n",
    "        \"time_start\": availability_time_start,\n",
    "        \"time_end\": availability_time_end,\n",
    "        \"data_min\": data_min,\n",
    "        \"data_max\": data_max,\n",
    "        \"time_granularity\": availability_time_granularity,\n",
    "        \"group_level\": availability_group_level,\n",
    "    }}\n",
    "\n",
    "    if availability_group_level in {\"block\", \"both\"}:\n",
    "        blk = _availability_for_level(\"block\")\n",
    "        availability_out[\"availability_block_raw\"] = blk[\"raw\"]\n",
    "        availability_out[\"availability_block_norm\"] = blk[\"norm\"]\n",
    "\n",
    "    if availability_group_level in {\"tract\", \"both\"}:\n",
    "        tr = _availability_for_level(\"tract\")\n",
    "        availability_out[\"availability_tract_raw\"] = tr[\"raw\"]\n",
    "        availability_out[\"availability_tract_norm\"] = tr[\"norm\"]\n",
    "\n",
    "    if \"availability_tract_norm\" not in availability_out:\n",
    "        tr = _availability_for_level(\"tract\")\n",
    "        availability_out[\"availability_tract_raw\"] = tr[\"raw\"]\n",
    "        availability_out[\"availability_tract_norm\"] = tr[\"norm\"]\n",
    "\n",
    "    results[\"availability\"] = availability_out\n",
    "\n",
    "    availability_df_for_capacity = availability_out[\"availability_tract_norm\"].copy()\n",
    "    availability_df_for_capacity[\"census_tract\"] = availability_df_for_capacity[\"census_tract\"].astype(str)\n",
    "    availability_df_for_capacity[\"time_slot\"] = pd.to_datetime(availability_df_for_capacity[\"time_slot\"])\n",
    "\n",
    "    # ==================================================\n",
    "    # CAPACITY\n",
    "    # ==================================================\n",
    "    import geopandas as gpd\n",
    "\n",
    "    tracts_gdf = gpd.read_file(str(tracts_path))\n",
    "    if tracts_gdf.crs is None:\n",
    "        raise ValueError(f\"Tracts shapefile CRS is missing: {tracts_path}\")\n",
    "    if tract_id_col not in tracts_gdf.columns:\n",
    "        raise ValueError(f\"Tract shapefile missing '{tract_id_col}' column.\")\n",
    "\n",
    "    if cfg.get(\"has_boro_logic\", False):\n",
    "        tracts_gdf[\"county_fips\"] = tracts_gdf[\"STATEFP\"].astype(str) + tracts_gdf[\"COUNTYFP\"].astype(str)\n",
    "        county_to_boro = {\n",
    "            \"36005\": \"The Bronx\",\n",
    "            \"36047\": \"Brooklyn\",\n",
    "            \"36061\": \"Manhattan\",\n",
    "            \"36081\": \"Queens\",\n",
    "            \"36085\": \"Staten Island\",\n",
    "        }\n",
    "        tracts_gdf[\"boro\"] = tracts_gdf[\"county_fips\"].map(county_to_boro)\n",
    "        if drop_staten_island:\n",
    "            tracts_gdf = tracts_gdf[tracts_gdf[\"boro\"] != \"Staten Island\"].copy()\n",
    "\n",
    "    centroids_wgs84 = gpd.GeoSeries(tracts_gdf.geometry.centroid, crs=tracts_gdf.crs).to_crs(epsg=4326)\n",
    "    tract_centroids_df = pd.DataFrame({\n",
    "        \"census_tract\": tracts_gdf[tract_id_col].astype(str).values,\n",
    "        \"centroid_lon\": centroids_wgs84.x.values,\n",
    "        \"centroid_lat\": centroids_wgs84.y.values,\n",
    "    })\n",
    "    if \"boro\" in tracts_gdf.columns:\n",
    "        tract_centroids_df[\"boro\"] = tracts_gdf[\"boro\"].values\n",
    "\n",
    "    results[\"tract_centroids_df\"] = tract_centroids_df\n",
    "    if save_outputs:\n",
    "        tract_centroids_df.to_csv(out_dir / \"centroid_tract_computed.csv\", index=False)\n",
    "\n",
    "    station_info = si.copy()\n",
    "    station_info[\"census_block\"] = station_info[\"census_block\"].astype(str)\n",
    "    station_info[\"census_tract\"] = _tract_from_block(station_info[\"census_block\"]).astype(str)\n",
    "\n",
    "    capacity_block = (\n",
    "        station_info.groupby(\"census_block\", as_index=False)\n",
    "        .agg(total_capacity=(\"capacity\", \"sum\"), num_station=(\"station_id\", \"count\"))\n",
    "    )\n",
    "    results[\"capacity_block\"] = capacity_block\n",
    "    if save_outputs:\n",
    "        capacity_block.to_csv(out_dir / \"capacity_block.csv\", index=False)\n",
    "\n",
    "    tract_capacity = (\n",
    "        station_info.groupby(\"census_tract\", as_index=False)\n",
    "        .agg(total_capacity=(\"capacity\", \"sum\"), num_station=(\"station_id\", \"count\"))\n",
    "    )\n",
    "\n",
    "    capacity_tract = tract_centroids_df[[\"census_tract\"]].drop_duplicates().merge(\n",
    "        tract_capacity, on=\"census_tract\", how=\"left\"\n",
    "    )\n",
    "    capacity_tract[[\"total_capacity\", \"num_station\"]] = capacity_tract[[\"total_capacity\", \"num_station\"]].fillna(0)\n",
    "\n",
    "    if external_tract_prefix:\n",
    "        capacity_tract = capacity_tract[~capacity_tract[\"census_tract\"].astype(str).str.startswith(external_tract_prefix)].copy()\n",
    "\n",
    "    capacity_tract = capacity_tract.sort_values(\"census_tract\").reset_index(drop=True)\n",
    "    capacity_tract[\"total_capacity_norm\"] = _minmax_norm(capacity_tract[\"total_capacity\"]).round(5)\n",
    "    capacity_tract[\"num_station_norm\"] = _minmax_norm(capacity_tract[\"num_station\"]).round(5)\n",
    "\n",
    "    results[\"capacity_tract_norm\"] = capacity_tract\n",
    "    if save_outputs:\n",
    "        capacity_tract.to_csv(out_dir / \"capacity_tract_norm.csv\", index=False)\n",
    "\n",
    "    avail = availability_df_for_capacity.copy()\n",
    "    if external_tract_prefix:\n",
    "        avail = avail[~avail[\"census_tract\"].astype(str).str.startswith(external_tract_prefix)].copy()\n",
    "\n",
    "    if peak_time_slot is None:\n",
    "        peak_series = avail.groupby(\"time_slot\")[peak_metric].sum()\n",
    "        if peak_series.empty:\n",
    "            # Fallback if no data, though we checked earlier\n",
    "            peak_time_slot = avail[\"time_slot\"].min()\n",
    "        else:\n",
    "            peak_time_slot = peak_series.idxmax()\n",
    "    peak_time_slot = pd.to_datetime(peak_time_slot)\n",
    "\n",
    "    peak_df = avail[avail[\"time_slot\"] == peak_time_slot].copy()\n",
    "\n",
    "    vehicle_capacity = (\n",
    "        peak_df.groupby(\"census_tract\", as_index=False)[\"total_vehicle_available\"]\n",
    "        .sum()\n",
    "        .rename(columns={\"total_vehicle_available\": \"vehicle_capacity\"})\n",
    "    )\n",
    "    dock_capacity = (\n",
    "        peak_df.groupby(\"census_tract\", as_index=False)[\"num_docks_available\"]\n",
    "        .sum()\n",
    "        .rename(columns={\"num_docks_available\": \"dock_capacity\"})\n",
    "    )\n",
    "\n",
    "    capacity_df = (\n",
    "        capacity_tract.merge(vehicle_capacity, on=\"census_tract\", how=\"left\")\n",
    "        .merge(dock_capacity, on=\"census_tract\", how=\"left\")\n",
    "    )\n",
    "    capacity_df[\"vehicle_capacity\"] = capacity_df[\"vehicle_capacity\"].fillna(0)\n",
    "    capacity_df[\"dock_capacity\"] = capacity_df[\"dock_capacity\"].fillna(0)\n",
    "\n",
    "    capacity_df[\"vehicle_capacity_norm\"] = _minmax_norm(capacity_df[\"vehicle_capacity\"]).round(5)\n",
    "    capacity_df[\"dock_capacity_norm\"] = _minmax_norm(capacity_df[\"dock_capacity\"]).round(5)\n",
    "\n",
    "    capacity_df[\"occupancy_rate\"] = capacity_df.apply(\n",
    "        lambda r: (r[\"vehicle_capacity\"] / r[\"total_capacity\"]) if r[\"total_capacity\"] else 0.0, axis=1\n",
    "    )\n",
    "    capacity_df[\"return_pressure\"] = capacity_df.apply(\n",
    "        lambda r: 1.0 - (r[\"dock_capacity\"] / r[\"total_capacity\"]) if r[\"total_capacity\"] else 0.0, axis=1\n",
    "    )\n",
    "\n",
    "    results[\"capacity_df\"] = capacity_df\n",
    "    results[\"peak_time_slot\"] = peak_time_slot\n",
    "    results[\"peak_metric\"] = peak_metric\n",
    "    if save_outputs:\n",
    "        capacity_df.to_csv(out_dir / \"capacity_tract_with_vehicle_and_docks_norm.csv\", index=False)\n",
    "\n",
    "    # ==================================================\n",
    "    # SAFETY (robust WKT; CRS-safe using blocks CRS)\n",
    "    # ==================================================\n",
    "    blocks_gdf = gpd.read_file(str(census_blocks_path))\n",
    "    if blocks_gdf.crs is None:\n",
    "        raise ValueError(f\"Blocks shapefile CRS is missing: {census_blocks_path}\")\n",
    "    if blocks_id_col not in blocks_gdf.columns:\n",
    "        raise ValueError(f\"Blocks shapefile missing '{blocks_id_col}' column.\")\n",
    "\n",
    "    target_crs = blocks_gdf.crs\n",
    "    safety_kind = str(cfg.get(\"safety_input_kind\", \"shp\"))\n",
    "\n",
    "    streets_gdf = _read_lines_layer(\n",
    "        centerline_path,\n",
    "        kind=safety_kind,\n",
    "        target_crs=target_crs,\n",
    "        csv_wkt_candidates=tuple(cfg.get(\"safety_csv_wkt_candidates\", (\"geometry\", \"the_geom\", \"wkt\", \"shape\", \"line\"))),\n",
    "        csv_crs=str(cfg.get(\"safety_csv_crs\", \"EPSG:4326\")),\n",
    "    )\n",
    "    bike_gdf = _read_lines_layer(\n",
    "        bike_lanes_path,\n",
    "        kind=safety_kind,\n",
    "        target_crs=target_crs,\n",
    "        csv_wkt_candidates=tuple(cfg.get(\"safety_csv_wkt_candidates\", (\"geometry\", \"the_geom\", \"wkt\", \"shape\", \"line\"))),\n",
    "        csv_crs=str(cfg.get(\"safety_csv_crs\", \"EPSG:4326\")),\n",
    "    )\n",
    "\n",
    "    street_len = _overlay_length_by_block(streets_gdf, blocks_gdf, blocks_id_col=blocks_id_col, out_col=\"streets_leng\")\n",
    "    bike_len = _overlay_length_by_block(bike_gdf, blocks_gdf, blocks_id_col=blocks_id_col, out_col=\"total_bike_lane_length\")\n",
    "\n",
    "    protected_rule = cfg.get(\"protected_lane_rule\", {})\n",
    "    facility_col, protected_gdf = _protected_filter(bike_gdf, protected_rule)\n",
    "\n",
    "    if facility_col is None:\n",
    "        prot_len = pd.DataFrame({\"census_block\": street_len[\"census_block\"].astype(str).unique()})\n",
    "        prot_len[\"protected_bike_lane_length\"] = 0.0\n",
    "        results[\"safety_note\"] = (\n",
    "            f\"[{city_key}] No protected-lane column found. Protected length set to 0. \"\n",
    "            f\"Tried: {protected_rule.get('col_candidates')}\"\n",
    "        )\n",
    "    else:\n",
    "        prot_len = _overlay_length_by_block(\n",
    "            protected_gdf, blocks_gdf, blocks_id_col=blocks_id_col, out_col=\"protected_bike_lane_length\"\n",
    "        )\n",
    "        results[\"safety_protected_lane_col_used\"] = facility_col\n",
    "        results[\"safety_protected_lane_rule\"] = protected_rule\n",
    "\n",
    "    safety_block = (\n",
    "        street_len.merge(bike_len, on=\"census_block\", how=\"left\")\n",
    "        .merge(prot_len, on=\"census_block\", how=\"left\")\n",
    "    )\n",
    "    safety_block[\"total_bike_lane_length\"] = safety_block[\"total_bike_lane_length\"].fillna(0)\n",
    "    safety_block[\"protected_bike_lane_length\"] = safety_block[\"protected_bike_lane_length\"].fillna(0)\n",
    "    safety_block[\"bike_lane_ratio\"] = _safe_ratio(safety_block[\"total_bike_lane_length\"], safety_block[\"streets_leng\"]).round(3)\n",
    "    safety_block[\"protected_bike_lane_ratio\"] = _safe_ratio(safety_block[\"protected_bike_lane_length\"], safety_block[\"streets_leng\"]).round(3)\n",
    "    safety_block[\"census_tract\"] = _tract_from_block(safety_block[\"census_block\"]).astype(str)\n",
    "\n",
    "    # tract safety using centroid file (for region cols)\n",
    "    centroids_csv_df = pd.read_csv(centroid_tract_csv)\n",
    "    centroids_csv_df[\"census_tract\"] = centroids_csv_df[\"census_tract\"].astype(str)\n",
    "    region_cols = [\"census_tract\"]\n",
    "    for c in [\"boro\", \"county_name\", \"county\", \"state\"]:\n",
    "        if c in centroids_csv_df.columns:\n",
    "            region_cols.append(c)\n",
    "\n",
    "    safety_tract = (\n",
    "        safety_block.groupby(\"census_tract\", as_index=False)[\n",
    "            [\"streets_leng\", \"total_bike_lane_length\", \"protected_bike_lane_length\"]\n",
    "        ].sum()\n",
    "    )\n",
    "    safety_tract = centroids_csv_df[region_cols].merge(safety_tract, on=\"census_tract\", how=\"left\")\n",
    "    for c in [\"streets_leng\", \"total_bike_lane_length\", \"protected_bike_lane_length\"]:\n",
    "        safety_tract[c] = safety_tract[c].fillna(0)\n",
    "\n",
    "    safety_tract[\"bike_lane_ratio\"] = _safe_ratio(safety_tract[\"total_bike_lane_length\"], safety_tract[\"streets_leng\"])\n",
    "    safety_tract[\"protected_bike_lane_ratio\"] = _safe_ratio(safety_tract[\"protected_bike_lane_length\"], safety_tract[\"streets_leng\"])\n",
    "    safety_tract = safety_tract.sort_values(\"census_tract\").reset_index(drop=True)\n",
    "\n",
    "    service_tracts = set(capacity_df[\"census_tract\"].astype(str).unique().tolist())\n",
    "    safety_service = safety_tract[safety_tract[\"census_tract\"].astype(str).isin(service_tracts)].reset_index(drop=True)\n",
    "\n",
    "    safety_norm = safety_service.copy()\n",
    "    safety_norm[\"bike_lane_ratio_norm\"] = _minmax_norm(safety_norm[\"bike_lane_ratio\"]).round(5)\n",
    "    safety_norm[\"protected_bike_lane_ratio_norm\"] = _minmax_norm(safety_norm[\"protected_bike_lane_ratio\"]).round(5)\n",
    "\n",
    "    results[\"safety_bike_lane_block\"] = safety_block\n",
    "    results[\"safety_bike_lane_tract\"] = safety_tract\n",
    "    results[\"safety_bike_lane_service_area\"] = safety_service\n",
    "    results[\"safety_bike_lane_norm_tract\"] = safety_norm\n",
    "\n",
    "    if save_outputs:\n",
    "        safety_block.to_csv(out_dir / \"safety_bike_lane_block.csv\", index=False)\n",
    "        safety_tract.to_csv(out_dir / \"safety_bike_lane_tract.csv\", index=False)\n",
    "        safety_service.to_csv(out_dir / \"safety_bike_lane_service_area.csv\", index=False)\n",
    "        safety_norm.to_csv(out_dir / \"safety_bike_lane_norm_tract.csv\", index=False)\n",
    "\n",
    "    # ==================================================\n",
    "    # TRIPS â€” load + standardize + compute start/end census blocks if missing\n",
    "    # ==================================================\n",
    "    trip_raw = pd.read_csv(trip_csv)\n",
    "    trip_df = _standardize_trips(trip_raw)\n",
    "\n",
    "    trip_df[\"started_at\"] = pd.to_datetime(trip_df[\"started_at\"], errors=\"coerce\")\n",
    "    trip_df[\"ended_at\"] = pd.to_datetime(trip_df[\"ended_at\"], errors=\"coerce\")\n",
    "\n",
    "    # Compute blocks from lat/lng if not present\n",
    "    if (\"start_census_block\" not in trip_df.columns) or (\"end_census_block\" not in trip_df.columns):\n",
    "        if not {\"start_lat\", \"start_lng\", \"end_lat\", \"end_lng\"}.issubset(trip_df.columns):\n",
    "            raise ValueError(\n",
    "                \"Trip CSV does not have start/end census blocks AND does not have start/end lat/lng to compute them. \"\n",
    "                f\"Found: {list(trip_df.columns)[:200]}\"\n",
    "            )\n",
    "\n",
    "        start_map = _geocode_points_to_blocks_spatial(\n",
    "            trip_df, lon_col=\"start_lng\", lat_col=\"start_lat\",\n",
    "            blocks_path=census_blocks_path, blocks_id_col=blocks_id_col\n",
    "        ).rename(columns={\"census_block\": \"start_census_block\"})\n",
    "\n",
    "        end_map = _geocode_points_to_blocks_spatial(\n",
    "            trip_df, lon_col=\"end_lng\", lat_col=\"end_lat\",\n",
    "            blocks_path=census_blocks_path, blocks_id_col=blocks_id_col\n",
    "        ).rename(columns={\"census_block\": \"end_census_block\"})\n",
    "\n",
    "        trip_df = trip_df.merge(\n",
    "            start_map, left_on=[\"start_lng\", \"start_lat\"], right_on=[\"lon\", \"lat\"], how=\"left\"\n",
    "        ).drop(columns=[\"lon\", \"lat\"], errors=\"ignore\")\n",
    "\n",
    "        trip_df = trip_df.merge(\n",
    "            end_map, left_on=[\"end_lng\", \"end_lat\"], right_on=[\"lon\", \"lat\"], how=\"left\"\n",
    "        ).drop(columns=[\"lon\", \"lat\"], errors=\"ignore\")\n",
    "\n",
    "    required_trip_cols = {\"started_at\", \"ended_at\", \"start_census_block\", \"end_census_block\"}\n",
    "    miss_trip = required_trip_cols - set(trip_df.columns)\n",
    "    if miss_trip:\n",
    "        raise ValueError(\n",
    "            f\"Trip CSV is missing required columns after standardization: {miss_trip}. \"\n",
    "            f\"Available columns: {list(trip_df.columns)[:200]}\"\n",
    "        )\n",
    "\n",
    "    trip_df[\"start_census_block\"] = trip_df[\"start_census_block\"].astype(str)\n",
    "    trip_df[\"end_census_block\"] = trip_df[\"end_census_block\"].astype(str)\n",
    "\n",
    "    # default usage/idle windows fallback to avail or trip min/max\n",
    "    if usage_time_start is None:\n",
    "        usage_time_start = trip_df[\"started_at\"].min()\n",
    "    if usage_time_end is None:\n",
    "        usage_time_end = trip_df[\"started_at\"].max()\n",
    "    if idle_time_start is None:\n",
    "        idle_time_start = trip_df[\"started_at\"].min()\n",
    "    if idle_time_end is None:\n",
    "        idle_time_end = trip_df[\"started_at\"].max()\n",
    "\n",
    "    usage_time_start = pd.to_datetime(usage_time_start)\n",
    "    usage_time_end = pd.to_datetime(usage_time_end)\n",
    "    idle_time_start = pd.to_datetime(idle_time_start)\n",
    "    idle_time_end = pd.to_datetime(idle_time_end)\n",
    "\n",
    "    # ==================================================\n",
    "    # USAGE (5-min, hourly, tract-hourly norm)\n",
    "    # ==================================================\n",
    "    trips = trip_df.copy()\n",
    "    trips = trips[(trips[\"started_at\"] >= usage_time_start) & (trips[\"started_at\"] <= usage_time_end)].copy()\n",
    "    trips[\"trip_duration\"] = (trips[\"ended_at\"] - trips[\"started_at\"]).dt.total_seconds() / 60.0\n",
    "    trips = trips[trips[\"trip_duration\"] <= 240].copy()\n",
    "\n",
    "    # 5-min\n",
    "    trips[\"time_slot_start\"] = trips[\"started_at\"].dt.floor(\"5min\")\n",
    "    trips[\"time_slot_end\"] = trips[\"ended_at\"].dt.floor(\"5min\")\n",
    "\n",
    "    trips_starting_5 = trips.groupby([\"start_census_block\", \"time_slot_start\"]).size().reset_index(name=\"trips_starting\")\n",
    "    trips_ending_5 = trips.groupby([\"end_census_block\", \"time_slot_end\"]).size().reset_index(name=\"trips_ending\")\n",
    "\n",
    "    trips_starting_5 = trips_starting_5.rename(columns={\"start_census_block\": \"census_block\", \"time_slot_start\": \"time_slot\"})\n",
    "    trips_ending_5 = trips_ending_5.rename(columns={\"end_census_block\": \"census_block\", \"time_slot_end\": \"time_slot\"})\n",
    "\n",
    "    all_blocks = pd.unique(pd.concat([trips[\"start_census_block\"], trips[\"end_census_block\"]]).astype(str))\n",
    "    all_blocks = [b for b in all_blocks if b not in (\"nan\", \"0\")]\n",
    "\n",
    "    all_time_slots_5 = pd.date_range(start=usage_time_start.floor(\"5min\"), end=usage_time_end.ceil(\"5min\"), freq=\"5min\")\n",
    "    grid_5 = pd.MultiIndex.from_product([all_blocks, all_time_slots_5], names=[\"census_block\", \"time_slot\"]).to_frame(index=False)\n",
    "\n",
    "    usage_5min_block = (\n",
    "        grid_5.merge(trips_starting_5, on=[\"census_block\", \"time_slot\"], how=\"left\")\n",
    "             .merge(trips_ending_5, on=[\"census_block\", \"time_slot\"], how=\"left\")\n",
    "    )\n",
    "    usage_5min_block[[\"trips_starting\", \"trips_ending\"]] = usage_5min_block[[\"trips_starting\", \"trips_ending\"]].fillna(0).astype(int)\n",
    "    usage_5min_block = usage_5min_block.sort_values([\"census_block\", \"time_slot\"]).reset_index(drop=True)\n",
    "\n",
    "    if save_outputs:\n",
    "        usage_5min_block.to_csv(out_dir / \"usage_5min_block.csv\", index=False)\n",
    "\n",
    "    # hourly block\n",
    "    trips[\"hour_start\"] = trips[\"started_at\"].dt.floor(\"H\")\n",
    "    trips[\"hour_end\"] = trips[\"ended_at\"].dt.floor(\"H\")\n",
    "\n",
    "    trips_starting_h = trips.groupby([\"start_census_block\", \"hour_start\"]).size().reset_index(name=\"trips_starting\")\n",
    "    trips_ending_h = trips.groupby([\"end_census_block\", \"hour_end\"]).size().reset_index(name=\"trips_ending\")\n",
    "\n",
    "    trips_starting_h = trips_starting_h.rename(columns={\"start_census_block\": \"census_block\", \"hour_start\": \"hour\"})\n",
    "    trips_ending_h = trips_ending_h.rename(columns={\"end_census_block\": \"census_block\", \"hour_end\": \"hour\"})\n",
    "\n",
    "    all_hours = pd.date_range(start=usage_time_start.floor(\"H\"), end=usage_time_end.ceil(\"H\"), freq=\"H\")\n",
    "    grid_h = pd.MultiIndex.from_product([all_blocks, all_hours], names=[\"census_block\", \"hour\"]).to_frame(index=False)\n",
    "\n",
    "    usage_hourly_block = (\n",
    "        grid_h.merge(trips_starting_h, on=[\"census_block\", \"hour\"], how=\"left\")\n",
    "             .merge(trips_ending_h, on=[\"census_block\", \"hour\"], how=\"left\")\n",
    "    )\n",
    "    usage_hourly_block[[\"trips_starting\", \"trips_ending\"]] = usage_hourly_block[[\"trips_starting\", \"trips_ending\"]].fillna(0).astype(int)\n",
    "    usage_hourly_block = usage_hourly_block.sort_values([\"census_block\", \"hour\"]).reset_index(drop=True)\n",
    "\n",
    "    if save_outputs:\n",
    "        usage_hourly_block.to_csv(out_dir / \"usage_hourly_block.csv\", index=False)\n",
    "\n",
    "    # hourly tract\n",
    "    usage_hourly = usage_hourly_block.copy()\n",
    "    usage_hourly[\"census_block\"] = usage_hourly[\"census_block\"].astype(str)\n",
    "    usage_hourly[\"census_tract\"] = _tract_from_block(usage_hourly[\"census_block\"]).astype(str)\n",
    "    if external_tract_prefix:\n",
    "        usage_hourly = usage_hourly[~usage_hourly[\"census_tract\"].astype(str).str.startswith(external_tract_prefix)].copy()\n",
    "\n",
    "    usage_hourly[\"time_slot\"] = pd.to_datetime(usage_hourly[\"hour\"])\n",
    "    usage_hourly = usage_hourly.drop(columns=[\"hour\"])\n",
    "\n",
    "    usage_hourly_tract = (\n",
    "        usage_hourly.groupby([\"census_tract\", \"time_slot\"], as_index=False)\n",
    "        .agg(trips_starting=(\"trips_starting\", \"sum\"), trips_ending=(\"trips_ending\", \"sum\"))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    usage_hourly_tract_raw = usage_hourly_tract.copy()\n",
    "\n",
    "    if tracts_to_remove:\n",
    "        usage_hourly_tract = usage_hourly_tract[~usage_hourly_tract[\"census_tract\"].isin(tracts_to_remove)].reset_index(drop=True)\n",
    "\n",
    "    usage_hourly_tract[\"trips_starting_norm\"] = _minmax_norm(usage_hourly_tract[\"trips_starting\"]).round(5)\n",
    "    usage_hourly_tract[\"trips_ending_norm\"] = _minmax_norm(usage_hourly_tract[\"trips_ending\"]).round(5)\n",
    "\n",
    "    if save_outputs:\n",
    "        usage_hourly_tract_raw.to_csv(out_dir / \"usage_hourly_tract_raw.csv\", index=False)\n",
    "        usage_hourly_tract.to_csv(out_dir / \"usage_norm_hourly_tract.csv\", index=False)\n",
    "\n",
    "    results[\"usage_5min_block\"] = usage_5min_block\n",
    "    results[\"usage_hourly_block\"] = usage_hourly_block\n",
    "    results[\"usage_hourly_tract_raw\"] = usage_hourly_tract_raw\n",
    "    results[\"usage_norm_hourly_tract\"] = usage_hourly_tract\n",
    "\n",
    "    # ==================================================\n",
    "    # IDLE TIME (full simulation, same logic as your earlier notebook)\n",
    "    # ==================================================\n",
    "    if not compute_idle_time:\n",
    "        results[\"idle_note\"] = \"Idle time skipped (compute_idle_time=False).\"\n",
    "        return results\n",
    "\n",
    "    # Need rideable_type to split bike vs ebike; if missing -> still compute total moved (no split)\n",
    "    has_rideable = \"rideable_type\" in trip_df.columns\n",
    "\n",
    "    # GBFS input for idle (has vehicles)\n",
    "    ss_idle = ss_done.copy()\n",
    "    ss_idle[\"timestamp\"] = pd.to_datetime(ss_idle[\"timestamp\"])\n",
    "    ss_idle[\"census_block\"] = ss_idle[\"census_block\"].astype(str)\n",
    "    ss_idle[\"census_tract\"] = ss_idle[\"census_tract\"].astype(str)\n",
    "    ss_idle[\"total_available\"] = ss_idle[\"num_bikes_available\"] + ss_idle[\"num_ebikes_available\"]\n",
    "    ss_idle[\"time_5min\"] = ss_idle[\"timestamp\"].dt.floor(\"5min\")\n",
    "\n",
    "    trips_idle = trip_df.copy()\n",
    "    trips_idle[\"start_census_block\"] = trips_idle[\"start_census_block\"].astype(str)\n",
    "    trips_idle[\"end_census_block\"] = trips_idle[\"end_census_block\"].astype(str)\n",
    "    trips_idle[\"started_at\"] = pd.to_datetime(trips_idle[\"started_at\"])\n",
    "    trips_idle[\"ended_at\"] = pd.to_datetime(trips_idle[\"ended_at\"])\n",
    "\n",
    "    # drop external if requested (NYC drops NJ)\n",
    "    if external_tract_prefix:\n",
    "        start_tract = _tract_from_block(trips_idle[\"start_census_block\"]).astype(str)\n",
    "        end_tract = _tract_from_block(trips_idle[\"end_census_block\"]).astype(str)\n",
    "        trips_idle = trips_idle[\n",
    "            (~start_tract.str.startswith(external_tract_prefix)) & (~end_tract.str.startswith(external_tract_prefix))\n",
    "        ].copy()\n",
    "\n",
    "    # time filter\n",
    "    trips_idle = trips_idle[(trips_idle[\"started_at\"] >= idle_time_start) & (trips_idle[\"started_at\"] <= idle_time_end)].copy()\n",
    "\n",
    "    # 5-min buckets\n",
    "    trips_idle[\"time_slot_start\"] = trips_idle[\"started_at\"].dt.floor(\"5min\")\n",
    "    trips_idle[\"time_slot_end\"] = trips_idle[\"ended_at\"].dt.floor(\"5min\")\n",
    "\n",
    "    # If rideable_type exists, compute starting/ending by type (bike vs ebike)\n",
    "    if has_rideable:\n",
    "        trips_idle = _map_rideable_type_to_two_columns(trips_idle)\n",
    "\n",
    "        # starting: bike/ebike counts\n",
    "        ts_start = (\n",
    "            trips_idle.groupby([\"start_census_block\", \"time_slot_start\", \"_is_ebike\"])\n",
    "            .size()\n",
    "            .unstack(fill_value=0)\n",
    "            .reset_index()\n",
    "        )\n",
    "        ts_start = ts_start.rename(columns={\"start_census_block\": \"census_block\", \"time_slot_start\": \"time_slot\"})\n",
    "        if True not in ts_start.columns:\n",
    "            ts_start[True] = 0\n",
    "        if False not in ts_start.columns:\n",
    "            ts_start[False] = 0\n",
    "        ts_start = ts_start.rename(columns={False: \"trips_starting_bike\", True: \"trips_starting_ebike\"})\n",
    "\n",
    "        # ending: bike/ebike counts\n",
    "        ts_end = (\n",
    "            trips_idle.groupby([\"end_census_block\", \"time_slot_end\", \"_is_ebike\"])\n",
    "            .size()\n",
    "            .unstack(fill_value=0)\n",
    "            .reset_index()\n",
    "        )\n",
    "        ts_end = ts_end.rename(columns={\"end_census_block\": \"census_block\", \"time_slot_end\": \"time_slot\"})\n",
    "        if True not in ts_end.columns:\n",
    "            ts_end[True] = 0\n",
    "        if False not in ts_end.columns:\n",
    "            ts_end[False] = 0\n",
    "        ts_end = ts_end.rename(columns={False: \"trips_ending_bike\", True: \"trips_ending_ebike\"})\n",
    "\n",
    "    else:\n",
    "        # No rideable_type: only total starting/ending\n",
    "        ts_start = trips_idle.groupby([\"start_census_block\", \"time_slot_start\"]).size().reset_index(name=\"trips_starting\")\n",
    "        ts_end = trips_idle.groupby([\"end_census_block\", \"time_slot_end\"]).size().reset_index(name=\"trips_ending\")\n",
    "        ts_start = ts_start.rename(columns={\"start_census_block\": \"census_block\", \"time_slot_start\": \"time_slot\"})\n",
    "        ts_end = ts_end.rename(columns={\"end_census_block\": \"census_block\", \"time_slot_end\": \"time_slot\"})\n",
    "\n",
    "    # full grid\n",
    "    all_blocks_idle = pd.unique(pd.concat([trips_idle[\"start_census_block\"], trips_idle[\"end_census_block\"]]))\n",
    "    all_blocks_idle = pd.Series(all_blocks_idle).dropna().astype(str).unique()\n",
    "\n",
    "    all_time_slots_idle = pd.date_range(\n",
    "        start=idle_time_start.floor(\"5min\"),\n",
    "        end=idle_time_end.ceil(\"5min\"),\n",
    "        freq=\"5min\",\n",
    "    )\n",
    "    grid_idle = pd.MultiIndex.from_product(\n",
    "        [all_blocks_idle, all_time_slots_idle],\n",
    "        names=[\"census_block\", \"time_slot\"],\n",
    "    ).to_frame(index=False)\n",
    "\n",
    "    if has_rideable:\n",
    "        result_complete = grid_idle.merge(ts_start, on=[\"census_block\", \"time_slot\"], how=\"left\").merge(\n",
    "            ts_end, on=[\"census_block\", \"time_slot\"], how=\"left\"\n",
    "        )\n",
    "        cols = [\"trips_starting_bike\", \"trips_starting_ebike\", \"trips_ending_bike\", \"trips_ending_ebike\"]\n",
    "        for c in cols:\n",
    "            if c not in result_complete.columns:\n",
    "                result_complete[c] = 0\n",
    "            result_complete[c] = result_complete[c].fillna(0).astype(int)\n",
    "\n",
    "        result_complete[\"trips_starting\"] = result_complete[\"trips_starting_bike\"] + result_complete[\"trips_starting_ebike\"]\n",
    "        result_complete[\"trips_ending\"] = result_complete[\"trips_ending_bike\"] + result_complete[\"trips_ending_ebike\"]\n",
    "\n",
    "    else:\n",
    "        result_complete = grid_idle.merge(ts_start, on=[\"census_block\", \"time_slot\"], how=\"left\").merge(\n",
    "            ts_end, on=[\"census_block\", \"time_slot\"], how=\"left\"\n",
    "        )\n",
    "        for c in [\"trips_starting\", \"trips_ending\"]:\n",
    "            if c not in result_complete.columns:\n",
    "                result_complete[c] = 0\n",
    "            result_complete[c] = result_complete[c].fillna(0).astype(int)\n",
    "\n",
    "    result_complete = result_complete.sort_values([\"census_block\", \"time_slot\"]).reset_index(drop=True)\n",
    "\n",
    "    # station availability by block 5-min\n",
    "    station_availability_df = (\n",
    "        ss_idle.groupby([\"census_block\", \"time_5min\"], as_index=False)[\n",
    "            [\"num_bikes_available\", \"num_ebikes_available\", \"total_available\"]\n",
    "        ].sum()\n",
    "    )\n",
    "    station_availability_df = station_availability_df.rename(columns={\"time_5min\": \"time_slot\"})\n",
    "    station_availability_df[\"time_slot\"] = station_availability_df[\"time_slot\"] - pd.Timedelta(minutes=5)\n",
    "\n",
    "    merged_idle = station_availability_df.merge(result_complete, on=[\"census_block\", \"time_slot\"], how=\"left\")\n",
    "    if has_rideable:\n",
    "        cols = [\"trips_starting_bike\", \"trips_starting_ebike\", \"trips_ending_bike\", \"trips_ending_ebike\", \"trips_starting\", \"trips_ending\"]\n",
    "        for c in cols:\n",
    "            if c not in merged_idle.columns:\n",
    "                merged_idle[c] = 0\n",
    "            merged_idle[c] = merged_idle[c].fillna(0).astype(int)\n",
    "\n",
    "        merged_idle[\"vehicles_moved_bike\"] = merged_idle[\"trips_starting_bike\"] + merged_idle[\"trips_ending_bike\"]\n",
    "        merged_idle[\"vehicles_moved_ebike\"] = merged_idle[\"trips_starting_ebike\"] + merged_idle[\"trips_ending_ebike\"]\n",
    "        merged_idle[\"vehicles_moved\"] = merged_idle[\"trips_starting\"] + merged_idle[\"trips_ending\"]\n",
    "    else:\n",
    "        for c in [\"trips_starting\", \"trips_ending\"]:\n",
    "            if c not in merged_idle.columns:\n",
    "                merged_idle[c] = 0\n",
    "            merged_idle[c] = merged_idle[c].fillna(0).astype(int)\n",
    "        merged_idle[\"vehicles_moved\"] = merged_idle[\"trips_starting\"] + merged_idle[\"trips_ending\"]\n",
    "\n",
    "    merged_idle[\"idle_vehicles\"] = (merged_idle[\"total_available\"] - merged_idle[\"vehicles_moved\"]).clip(lower=0)\n",
    "    merged_idle[\"time_slot\"] = pd.to_datetime(merged_idle[\"time_slot\"])\n",
    "    merged_idle = merged_idle.sort_values([\"census_block\", \"time_slot\"])\n",
    "    merged_idle[\"hour\"] = merged_idle[\"time_slot\"].dt.floor(\"H\")\n",
    "\n",
    "    # hourly idle simulation per block\n",
    "    from collections import deque\n",
    "\n",
    "    idle_results = []\n",
    "    for block, block_df in merged_idle.groupby(\"census_block\"):\n",
    "        for hour, hour_df in block_df.groupby(\"hour\"):\n",
    "            hour_df = hour_df.sort_values(\"time_slot\")\n",
    "            virtual_vehicles = deque()\n",
    "            finalized_durations = []\n",
    "\n",
    "            for _, row in hour_df.iterrows():\n",
    "                current_time = row[\"time_slot\"]\n",
    "                current_idle = int(row[\"idle_vehicles\"])\n",
    "\n",
    "                if current_idle > len(virtual_vehicles):\n",
    "                    for _ in range(current_idle - len(virtual_vehicles)):\n",
    "                        virtual_vehicles.append(current_time)\n",
    "                elif current_idle < len(virtual_vehicles):\n",
    "                    for _ in range(len(virtual_vehicles) - current_idle):\n",
    "                        start_t = virtual_vehicles.popleft()\n",
    "                        duration = (current_time - start_t).total_seconds() / 60.0\n",
    "                        finalized_durations.append(duration)\n",
    "\n",
    "            final_time = hour + pd.Timedelta(hours=1)\n",
    "            while virtual_vehicles:\n",
    "                start_t = virtual_vehicles.popleft()\n",
    "                duration = (final_time - start_t).total_seconds() / 60.0\n",
    "                finalized_durations.append(duration)\n",
    "\n",
    "            avg_idle_time = round(sum(finalized_durations) / len(finalized_durations), 2) if finalized_durations else 0.0\n",
    "            idle_results.append({\"census_block\": block, \"hour\": hour, \"avg_idle_time\": avg_idle_time})\n",
    "\n",
    "    idle_time_block = pd.DataFrame(idle_results)\n",
    "\n",
    "    # tract aggregation + normalize\n",
    "    idle_time_block[\"census_block\"] = idle_time_block[\"census_block\"].astype(str)\n",
    "    idle_time_block[\"census_tract\"] = _tract_from_block(idle_time_block[\"census_block\"]).astype(str)\n",
    "\n",
    "    idle_time_tract = idle_time_block.groupby([\"census_tract\", \"hour\"], as_index=False)[\"avg_idle_time\"].mean()\n",
    "\n",
    "    if external_tract_prefix:\n",
    "        idle_time_tract = idle_time_tract[\n",
    "            ~idle_time_tract[\"census_tract\"].astype(str).str.startswith(external_tract_prefix)\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "    idle_time_tract[\"avg_idle_time_norm\"] = _minmax_norm(idle_time_tract[\"avg_idle_time\"]).round(5)\n",
    "    idle_time_tract = idle_time_tract.rename(columns={\"hour\": \"time_slot\"})\n",
    "\n",
    "    results[\"idle_merged_5min\"] = merged_idle\n",
    "    results[\"idle_time_block\"] = idle_time_block\n",
    "    results[\"idle_time_tract_raw\"] = idle_time_tract.drop(columns=[\"avg_idle_time_norm\"])\n",
    "    results[\"idle_time_norm_tract\"] = idle_time_tract\n",
    "\n",
    "    if save_outputs:\n",
    "        merged_idle.to_csv(out_dir / \"idle_merged_5min.csv\", index=False)\n",
    "        idle_time_block.to_csv(out_dir / \"idle_time_block.csv\", index=False)\n",
    "        results[\"idle_time_tract_raw\"].to_csv(out_dir / \"idle_time_tract_raw.csv\", index=False)\n",
    "        idle_time_tract.to_csv(out_dir / \"idle_time_norm_tract.csv\", index=False)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af847cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_23924\\1654986023.py:757: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  temp[\"time_slot\"] = temp[\"timestamp\"].dt.floor(availability_time_granularity)\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_23924\\1654986023.py:768: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  geo_ts[\"time_slot\"] = geo_ts[\"timestamp\"].dt.floor(availability_time_granularity)\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_23924\\1654986023.py:757: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  temp[\"time_slot\"] = temp[\"timestamp\"].dt.floor(availability_time_granularity)\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_23924\\1654986023.py:768: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  geo_ts[\"time_slot\"] = geo_ts[\"timestamp\"].dt.floor(availability_time_granularity)\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_23924\\1654986023.py:848: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroids_wgs84 = gpd.GeoSeries(tracts_gdf.geometry.centroid, crs=tracts_gdf.crs).to_crs(epsg=4326)\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_23924\\1654986023.py:328: UserWarning: Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  inter[\"seg_len\"] = inter.geometry.length\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_23924\\1654986023.py:328: UserWarning: Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  inter[\"seg_len\"] = inter.geometry.length\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_23924\\1654986023.py:328: UserWarning: Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  inter[\"seg_len\"] = inter.geometry.length\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_23924\\1654986023.py:1135: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  trips[\"hour_start\"] = trips[\"started_at\"].dt.floor(\"H\")\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_23924\\1654986023.py:1136: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  trips[\"hour_end\"] = trips[\"ended_at\"].dt.floor(\"H\")\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_23924\\1654986023.py:1144: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  all_hours = pd.date_range(start=usage_time_start.floor(\"H\"), end=usage_time_end.ceil(\"H\"), freq=\"H\")\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_23924\\1654986023.py:1335: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  merged_idle[\"hour\"] = merged_idle[\"time_slot\"].dt.floor(\"H\")\n"
     ]
    }
   ],
   "source": [
    "# Example Call for San Francisco Docked\n",
    "out_docked = run_docked_all_utilities_single_function(\n",
    "    city=\"SF\",\n",
    "    \n",
    "    # Inputs\n",
    "    station_status_txt=r\"D:\\Research Fellowship\\Summer Research Stuff\\Collected Data\\Week 1\\09-June\\san_fran_baywheels_docked_station_status_6_9.txt\",\n",
    "    station_information_csv=r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\GBFS_Census_Tract\\San_Fran_Baywheels\\San_fran_Baywheels station information 06_09.csv\",\n",
    "    trip_csv=r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Usage\\San_Fran_Baywheels\\202505-baywheels-tripdata.csv\",\n",
    "    \n",
    "    # Output Location\n",
    "    output_dir=\"San_Fran_Baywheels_Docked_Output\",\n",
    "    \n",
    "    # Global Time Window (Propagates to all utilities)\n",
    "    time_start=\"2025-06-09 06:00:00\",\n",
    "    time_end=\"2025-06-09 12:00:00\",\n",
    "    \n",
    "    # Optional: Disable idle simulation if it takes too long\n",
    "    compute_idle_time=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2252348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import warnings\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# 1. CITY CONFIGURATION\n",
    "#    Added \"metric_crs\" to ensure length calculations are in Meters, not Degrees.\n",
    "# ==================================================\n",
    "CITY_CONFIG: Dict[str, Dict[str, Any]] = {\n",
    "    \"NYC\": {\n",
    "        \"assets\": {\n",
    "            \"census_blocks\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\GBFS_Census_Tract\\NYC\\tl_2024_36_tabblock20.shp\",\n",
    "            \"tracts\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\NYC\\tl_2024_36_tract.shp\",\n",
    "            \"centroid_csv\": r\"D:\\Research Fellowship\\Capacity_NYC\\centroid_tract_computed.csv\",\n",
    "            \"centerline\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NYC\\CSCL_PlowNYC_20250619.csv\",\n",
    "            \"bike_lanes\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NYC\\New_York_City_Bike_Routes_20250619.csv\",\n",
    "        },\n",
    "        \"geo\": {\n",
    "            \"blocks_id\": \"GEOID20\",\n",
    "            \"tract_id\": \"GEOID\",\n",
    "            \"metric_crs\": \"EPSG:32618\",  # UTM Zone 18N (Meters)\n",
    "            \"safety_type\": \"csv_wkt\",\n",
    "            \"wkt_candidates\": (\"geometry\", \"the_geom\", \"wkt\", \"geometry_wkt\", \"WKT\", \"geom\", \"shape\", \"line\"),\n",
    "            \"external_tract_prefix\": \"34\",\n",
    "            \"drop_staten_island\": True,\n",
    "        },\n",
    "        \"safety_rule\": {\n",
    "            \"col_candidates\": (\"facilitycl\", \"facility\", \"class\", \"ft\", \"type\"),\n",
    "            \"match_type\": \"equals\",\n",
    "            \"match_value\": \"I\",\n",
    "        },\n",
    "    },\n",
    "    \"NJ\": {\n",
    "        \"assets\": {\n",
    "            \"census_blocks\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NJ\\tl_2024_34_tabblock20.shp\",\n",
    "            \"tracts\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\NJ\\tl_2024_34_tract.shp\",\n",
    "            \"centroid_csv\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\NJ\\centroid_tract_nj.csv\",\n",
    "            \"centerline\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NJ\\Tran_road.shp\",\n",
    "            \"bike_lanes\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NJ\\bike-lanes-2020-division-of-transportation.shp\",\n",
    "        },\n",
    "        \"geo\": {\n",
    "            \"blocks_id\": \"GEOID20\",\n",
    "            \"tract_id\": \"GEOID\",\n",
    "            \"metric_crs\": \"EPSG:32618\",  # UTM Zone 18N (Meters)\n",
    "            \"safety_type\": \"shp\",\n",
    "            \"wkt_candidates\": (),\n",
    "            \"external_tract_prefix\": None,\n",
    "            \"drop_staten_island\": False,\n",
    "        },\n",
    "        \"safety_rule\": {\n",
    "            \"col_candidates\": (\"type\", \"facility\", \"facilitycl\", \"class\", \"lane_type\"),\n",
    "            \"match_type\": \"contains\",\n",
    "            \"match_value\": \"PROTECT\",\n",
    "        },\n",
    "    },\n",
    "    \"PITT\": {\n",
    "        \"assets\": {\n",
    "            \"census_blocks\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Pitt\\tl_2024_42_tabblock20.shp\",\n",
    "            \"tracts\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\Pitt\\tl_2024_42_tract.shp\",\n",
    "            \"centroid_csv\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\Pitt\\centroid_tract_pa.csv\",\n",
    "            \"centerline\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Pitt\\Pittsburgh_Street_Centerline.shp\",\n",
    "            \"bike_lanes\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Pitt\\Bike Map\\Bike Lanes.shp\",\n",
    "        },\n",
    "        \"geo\": {\n",
    "            \"blocks_id\": \"GEOID20\",\n",
    "            \"tract_id\": \"GEOID\",\n",
    "            \"metric_crs\": \"EPSG:32617\",  # UTM Zone 17N (Meters)\n",
    "            \"safety_type\": \"shp\",\n",
    "            \"wkt_candidates\": (),\n",
    "            \"external_tract_prefix\": None,\n",
    "            \"drop_staten_island\": False,\n",
    "        },\n",
    "        \"safety_rule\": {\n",
    "            \"col_candidates\": (\"facility\", \"type\", \"class\", \"status\", \"lane_type\", \"BIKE_FACIL\", \"CATEGORY\"),\n",
    "            \"match_type\": \"contains\",\n",
    "            \"match_value\": \"PROTECT\",\n",
    "        },\n",
    "    },\n",
    "    \"SF\": {\n",
    "        \"assets\": {\n",
    "            \"census_blocks\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Usage\\San_Fran_Baywheels\\tl_2024_06_tabblock20.shp\",\n",
    "            \"tracts\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\San_Fran_Baywheels\\tl_2024_06_tract.shp\",\n",
    "            \"centroid_csv\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\San_Fran_Baywheels\\centroid_tract_ca.csv\",\n",
    "            \"centerline\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\San_Fran_Baywheels\\Streets___Active_and_Retired_20250626 (1).csv\",\n",
    "            \"bike_lanes\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\San_Fran_Baywheels\\Bikelane.csv\",\n",
    "        },\n",
    "        \"geo\": {\n",
    "            \"blocks_id\": \"GEOID20\",\n",
    "            \"tract_id\": \"GEOID\",\n",
    "            \"metric_crs\": \"EPSG:26910\",  # UTM Zone 10N (Meters) - Crucial for SF\n",
    "            \"safety_type\": \"csv_wkt\",\n",
    "            \"wkt_candidates\": (\"geometry\", \"shape\", \"line\", \"the_geom\", \"wkt\", \"geometry_wkt\", \"WKT\", \"geom\"),\n",
    "            \"external_tract_prefix\": None,\n",
    "            \"drop_staten_island\": False,\n",
    "        },\n",
    "        \"safety_rule\": {\n",
    "            \"col_candidates\": (\"BARRIER\", \"FACILITY_T\", \"BUFFERED\", \"RAISED\", \"SYMBOLOGY\"),\n",
    "            \"match_type\": \"not_empty_or_contains\",\n",
    "            \"match_value\": r\"PROTECT|SEPARAT|CYCLETRACK|BARRIER|RAISED|BUFFER\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# 2. UTILITY CLASSES\n",
    "# ==================================================\n",
    "class _Standardizer:\n",
    "    @staticmethod\n",
    "    def pick_col(df: pd.DataFrame, candidates: Sequence[str], required: bool = False, label: str = \"\") -> Optional[str]:\n",
    "        cols = set(df.columns)\n",
    "        for c in candidates:\n",
    "            if c in cols: return c\n",
    "        if required:\n",
    "            raise ValueError(f\"Missing required col for {label}. Tried: {candidates}\")\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_tract(s: pd.Series) -> pd.Series:\n",
    "        \"\"\"Ensures tract IDs are 11 digits (pads with leading zeros).\"\"\"\n",
    "        return s.astype(str).str.split('.').str[0].str.zfill(11)\n",
    "\n",
    "    @staticmethod\n",
    "    def tract_from_block(s: pd.Series) -> pd.Series:\n",
    "        s = s.astype(str).str.split('.').str[0]\n",
    "        return s.str[:11]\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_id(s: pd.Series) -> pd.Series:\n",
    "        \"\"\"Standardizes IDs: to string, uppercase, stripped, no .0 decimals.\"\"\"\n",
    "        return s.astype(str).str.strip().str.upper().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "\n",
    "    @classmethod\n",
    "    def stations(cls, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        mapping = {\n",
    "            cls.pick_col(df, [\"station_id\", \"stationId\", \"id\", \"Station ID\"], True, \"station_id\"): \"station_id\",\n",
    "            cls.pick_col(df, [\"lat\", \"latitude\", \"y\"], True, \"lat\"): \"lat\",\n",
    "            cls.pick_col(df, [\"lon\", \"lng\", \"longitude\", \"x\"], True, \"lon\"): \"lon\",\n",
    "            cls.pick_col(df, [\"capacity\", \"dock_count\", \"num_docks\"], True, \"capacity\"): \"capacity\"\n",
    "        }\n",
    "        out = df.rename(columns=mapping)\n",
    "        out[\"station_id\"] = cls.normalize_id(out[\"station_id\"])\n",
    "        \n",
    "        cb = cls.pick_col(out, [\"census_block\", \"block_geoid\", \"GEOID20\"])\n",
    "        if cb and cb != \"census_block\": out = out.rename(columns={cb: \"census_block\"})\n",
    "        \n",
    "        ct = cls.pick_col(out, [\"census_tract\", \"tract_geoid\", \"GEOID\"])\n",
    "        if ct and ct != \"census_tract\": out = out.rename(columns={ct: \"census_tract\"})\n",
    "        \n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def trips(cls, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        mapping = {\n",
    "            cls.pick_col(df, [\"started_at\", \"start_time\", \"starttime\", \"Start Date\"], True, \"start\"): \"started_at\",\n",
    "            cls.pick_col(df, [\"ended_at\", \"end_time\", \"stoptime\", \"End Date\"], True, \"end\"): \"ended_at\"\n",
    "        }\n",
    "        \n",
    "        opt_map = {\n",
    "            cls.pick_col(df, [\"rideable_type\", \"bike_type\", \"vehicle_type\"]): \"rideable_type\",\n",
    "            cls.pick_col(df, [\"start_census_block\", \"start_block\"]): \"start_census_block\",\n",
    "            cls.pick_col(df, [\"end_census_block\", \"end_block\"]): \"end_census_block\",\n",
    "            cls.pick_col(df, [\"start_lat\", \"startLatitude\", \"StartLat\"]): \"start_lat\",\n",
    "            cls.pick_col(df, [\"start_lng\", \"startLongitude\", \"StartLng\"]): \"start_lng\",\n",
    "            cls.pick_col(df, [\"end_lat\", \"endLatitude\", \"EndLat\"]): \"end_lat\",\n",
    "            cls.pick_col(df, [\"end_lng\", \"endLongitude\", \"EndLng\"]): \"end_lng\",\n",
    "            cls.pick_col(df, [\"start_station_id\", \"Start Station Id\"]): \"start_station_id\",\n",
    "            cls.pick_col(df, [\"end_station_id\", \"End Station Id\"]): \"end_station_id\"\n",
    "        }\n",
    "        mapping.update({k: v for k, v in opt_map.items() if k})\n",
    "        \n",
    "        df = df.rename(columns=mapping)\n",
    "        if \"start_station_id\" in df.columns:\n",
    "            df[\"start_station_id\"] = cls.normalize_id(df[\"start_station_id\"])\n",
    "        if \"end_station_id\" in df.columns:\n",
    "            df[\"end_station_id\"] = cls.normalize_id(df[\"end_station_id\"])\n",
    "            \n",
    "        return df\n",
    "\n",
    "\n",
    "class _GeoUtils:\n",
    "    @staticmethod\n",
    "    def load_lines(path: Path, kind: str, target_crs, csv_candidates: tuple) -> gpd.GeoDataFrame:\n",
    "        if kind == \"shp\":\n",
    "            gdf = gpd.read_file(path)\n",
    "            if gdf.crs is None: raise ValueError(f\"No CRS in {path}\")\n",
    "            return gdf.to_crs(target_crs)\n",
    "        \n",
    "        if isinstance(csv_candidates, str): csv_candidates = (csv_candidates,)\n",
    "\n",
    "        df = pd.read_csv(path, engine=\"c\")\n",
    "        wkt_col = _Standardizer.pick_col(df, csv_candidates, True, \"WKT Column\")\n",
    "        \n",
    "        def safe_load(x):\n",
    "            try: return wkt.loads(str(x))\n",
    "            except: return None\n",
    "            \n",
    "        df[\"geometry\"] = df[wkt_col].apply(safe_load)\n",
    "        gdf = gpd.GeoDataFrame(df[df[\"geometry\"].notna()], geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "        return gdf.to_crs(target_crs)\n",
    "\n",
    "    @staticmethod\n",
    "    def geocode_points(df: pd.DataFrame, lon_col: str, lat_col: str, blocks_gdf: gpd.GeoDataFrame, id_col: str) -> pd.DataFrame:\n",
    "        points = df[[lon_col, lat_col]].dropna().drop_duplicates()\n",
    "        if points.empty:\n",
    "            return pd.DataFrame(columns=[lon_col, lat_col, \"census_block\"])\n",
    "\n",
    "        gdf = gpd.GeoDataFrame(\n",
    "            points, \n",
    "            geometry=[Point(xy) for xy in zip(points[lon_col], points[lat_col])],\n",
    "            crs=\"EPSG:4326\"\n",
    "        )\n",
    "        \n",
    "        joined = gpd.sjoin(gdf, blocks_gdf[[id_col, \"geometry\"]], how=\"left\", predicate=\"within\")\n",
    "        joined = joined.rename(columns={id_col: \"census_block\"})\n",
    "        \n",
    "        out = joined[[lon_col, lat_col, \"census_block\"]].copy()\n",
    "        out = out.rename(columns={lon_col: \"lon\", lat_col: \"lat\"})\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def api_fill_blocks(df: pd.DataFrame, benchmark: str, vintage: str) -> pd.DataFrame:\n",
    "        missing = df[df[\"census_block\"].isna()][[\"lat\", \"lon\"]].drop_duplicates()\n",
    "        if missing.empty: return df\n",
    "        \n",
    "        if len(missing) > 5000:\n",
    "            print(f\"âš ï¸ Warning: Too many missing blocks ({len(missing)}). API fill capped at 5000.\")\n",
    "            missing = missing.head(5000)\n",
    "\n",
    "        def fetch(r):\n",
    "            try:\n",
    "                url = f\"https://geocoding.geo.census.gov/geocoder/geographies/coordinates?x={r.lon}&y={r.lat}&benchmark={benchmark}&vintage={vintage}&format=json\"\n",
    "                js = requests.get(url, timeout=5).json()\n",
    "                return js[\"result\"][\"geographies\"][\"2020 Census Blocks\"][0][\"GEOID\"]\n",
    "            except: return None\n",
    "\n",
    "        tqdm.pandas(desc=\"API Geocoding\")\n",
    "        missing[\"new_block\"] = missing.progress_apply(fetch, axis=1)\n",
    "        \n",
    "        merged = df.merge(missing, on=[\"lat\", \"lon\"], how=\"left\")\n",
    "        if \"new_block\" in merged.columns:\n",
    "            merged[\"census_block\"] = merged[\"census_block\"].fillna(merged[\"new_block\"])\n",
    "            merged = merged.drop(columns=[\"new_block\"])\n",
    "        return merged\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# 3. METRIC CALCULATORS\n",
    "# ==================================================\n",
    "def _calc_availability(df: pd.DataFrame, level: str, gran: str, out_dir: Path, save: bool, ext_prefix: str = None) -> Dict:\n",
    "    geo = \"census_block\" if level == \"block\" else \"census_tract\"\n",
    "    agg = df.groupby([geo, \"timestamp\"], as_index=False)[\n",
    "        [\"num_bikes_available\", \"num_ebikes_available\", \"num_docks_available\", \"total_vehicle_available\"]\n",
    "    ].sum()\n",
    "    \n",
    "    agg[\"time_slot\"] = agg[\"timestamp\"].dt.floor(gran)\n",
    "    raw = agg.groupby([geo, \"time_slot\"], as_index=False).mean(numeric_only=True).round(0)\n",
    "    for c in raw.columns: \n",
    "        if \"available\" in c: raw[c] = raw[c].astype(int)\n",
    "            \n",
    "    if level == \"tract\":\n",
    "        raw[\"census_tract\"] = _Standardizer.normalize_tract(raw[\"census_tract\"])\n",
    "        if ext_prefix:\n",
    "            raw = raw[~raw[\"census_tract\"].str.startswith(ext_prefix)]\n",
    "\n",
    "    norm = raw.copy()\n",
    "    for c in [\"num_bikes_available\", \"num_ebikes_available\", \"num_docks_available\", \"total_vehicle_available\"]:\n",
    "        norm[f\"{c}_norm\"] = _minmax_norm(norm[c]).round(5)\n",
    "\n",
    "    if save:\n",
    "        raw.to_csv(out_dir / f\"availability__raw__{level}.csv\", index=False)\n",
    "        norm.to_csv(out_dir / f\"availability__norm__{level}.csv\", index=False)\n",
    "        \n",
    "    return {\"raw\": raw, \"norm\": norm}\n",
    "\n",
    "\n",
    "def _calc_usage(trips: pd.DataFrame, time_range: Tuple, blocks: List, gran_str: str, out_name: str, out_dir: Path, save: bool) -> pd.DataFrame:\n",
    "    trips = trips.copy()\n",
    "    trips[\"slot_start\"] = trips[\"started_at\"].dt.floor(gran_str)\n",
    "    trips[\"slot_end\"] = trips[\"ended_at\"].dt.floor(gran_str)\n",
    "    \n",
    "    starts = trips.groupby([\"start_census_block\", \"slot_start\"]).size().rename(\"trips_starting\")\n",
    "    ends = trips.groupby([\"end_census_block\", \"slot_end\"]).size().rename(\"trips_ending\")\n",
    "    \n",
    "    slots = pd.date_range(start=time_range[0].floor(gran_str), end=time_range[1].ceil(gran_str), freq=gran_str)\n",
    "    grid = pd.MultiIndex.from_product([blocks, slots], names=[\"census_block\", \"time_slot\"]).to_frame(index=False)\n",
    "    \n",
    "    out = grid.merge(starts, left_on=[\"census_block\", \"time_slot\"], right_index=True, how=\"left\") \\\n",
    "              .merge(ends, left_on=[\"census_block\", \"time_slot\"], right_index=True, how=\"left\")\n",
    "              \n",
    "    out = out.fillna(0).astype({\"trips_starting\": int, \"trips_ending\": int})\n",
    "    \n",
    "    if save: out.to_csv(out_dir / f\"{out_name}.csv\", index=False)\n",
    "    return out\n",
    "\n",
    "def _minmax_norm(series: pd.Series) -> pd.Series:\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    mn, mx = s.min(skipna=True), s.max(skipna=True)\n",
    "    if pd.isna(mn) or pd.isna(mx) or mx <= mn: return pd.Series(0.0, index=s.index)\n",
    "    return (s - mn) / (mx - mn)\n",
    "\n",
    "def _safe_ratio(numer: pd.Series, denom: pd.Series) -> pd.Series:\n",
    "    return numer.div(denom.where(denom > 0, other=pd.NA)).fillna(0)\n",
    "\n",
    "def _map_rideable_type_to_two_columns(trips: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = trips.copy()\n",
    "    if \"rideable_type\" not in df.columns: return df\n",
    "    t = df[\"rideable_type\"].astype(str).str.lower()\n",
    "    df[\"_is_ebike\"] = t.str.contains(\"electric|ebike|e-bike|assist\", na=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _overlay_length_by_block(lines_gdf, blocks_gdf, blocks_id_col, out_col):\n",
    "    \"\"\"\n",
    "    Computes length of lines per block.\n",
    "    ASSUMES both GDFs are already projected to a METRIC CRS (meters).\n",
    "    \"\"\"\n",
    "    inter = gpd.overlay(lines_gdf, blocks_gdf, how=\"intersection\", keep_geom_type=False)\n",
    "    if inter.empty:\n",
    "        return pd.DataFrame({\"census_block\": pd.Series([], dtype=str), out_col: pd.Series([], dtype=float)})\n",
    "\n",
    "    inter[\"seg_len\"] = inter.geometry.length\n",
    "    inter = inter.rename(columns={blocks_id_col: \"census_block\"})\n",
    "    out = inter.groupby(\"census_block\", as_index=False)[\"seg_len\"].sum().rename(columns={\"seg_len\": out_col})\n",
    "    \n",
    "    # Rounding here is fine now because units are Meters, not Degrees\n",
    "    out[out_col] = out[out_col].round(3)\n",
    "    out[\"census_block\"] = out[\"census_block\"].astype(str)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _protected_filter(lines_gdf, rule: Dict[str, Any]):\n",
    "    col = _Standardizer.pick_col(lines_gdf, list(rule.get(\"col_candidates\", [])), required=False, label=\"protected lane col\")\n",
    "    if col is None:\n",
    "        return None, lines_gdf\n",
    "\n",
    "    s = lines_gdf[col].astype(str)\n",
    "    match_type = str(rule.get(\"match_type\", \"equals\")).lower()\n",
    "    match_value = str(rule.get(\"match_value\", \"\"))\n",
    "\n",
    "    if match_type == \"equals\":\n",
    "        mask = s.str.upper() == match_value.upper()\n",
    "    elif match_type == \"contains\":\n",
    "        mask = s.str.upper().str.contains(match_value.upper(), na=False)\n",
    "    elif match_type == \"not_empty_or_contains\":\n",
    "        s2 = lines_gdf[col]\n",
    "        non_empty = s2.notna() & (s.astype(str).str.strip() != \"\") & (~s.astype(str).str.lower().isin([\"0\", \"false\", \"none\", \"nan\"]))\n",
    "        contains = s.astype(str).str.contains(match_value, case=False, na=False, regex=True)\n",
    "        mask = non_empty | contains\n",
    "    else:\n",
    "        # fallback\n",
    "        mask = s == match_value\n",
    "\n",
    "    return col, lines_gdf[mask].copy()\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# 4. MAIN FUNCTION\n",
    "# ==================================================\n",
    "def run_docked_all_utilities_single_function(\n",
    "    *,\n",
    "    city: str,\n",
    "    station_status_txt: Union[str, Path],\n",
    "    station_information_csv: Union[str, Path],\n",
    "    trip_csv: Union[str, Path],\n",
    "    output_dir: Optional[Union[str, Path]] = None,\n",
    "    save_outputs: bool = True,\n",
    "    \n",
    "    # Global Time Window\n",
    "    time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    \n",
    "    # Toggles & Overrides\n",
    "    compute_idle_time: bool = True,\n",
    "    availability_time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    availability_time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    usage_time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    usage_time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    idle_time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    idle_time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    \n",
    "    # Granularity & Settings\n",
    "    availability_time_granularity: str = \"1h\",\n",
    "    availability_group_level: str = \"both\",\n",
    "    peak_time_slot: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    peak_metric: str = \"total_vehicle_available\",\n",
    "    drop_staten_island: Optional[bool] = None,\n",
    "    tracts_to_remove: Optional[Sequence[str]] = None,\n",
    "    remove_tz_suffix: str = \" EDT\",\n",
    "    fill_missing_with_census_api: bool = True,\n",
    "    census_geocoder_benchmark: str = \"Public_AR_Census2020\",\n",
    "    census_geocoder_vintage: str = \"Census2020_Current\",\n",
    ") -> Dict[str, Any]:\n",
    "    \n",
    "    # ---------------- Setup ----------------\n",
    "    city_key = city.strip().upper()\n",
    "    aliases = {\"PITTSBURGH\": \"PITT\", \"PIT\": \"PITT\", \"SANFRAN\": \"SF\", \"SAN FRANCISCO\": \"SF\", \"BAYWHEELS\": \"SF\"}\n",
    "    city_key = aliases.get(city_key, city_key)\n",
    "    \n",
    "    if city_key not in CITY_CONFIG:\n",
    "        raise ValueError(f\"City {city_key} not found in config.\")\n",
    "    \n",
    "    cfg = CITY_CONFIG[city_key]\n",
    "    out_dir = Path(output_dir or f\"./{city_key}_ALL_IN_ONE_OUTPUTS\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load Blocks ONCE\n",
    "    print(\"Loading Census Blocks (Heavy Operation)...\")\n",
    "    blocks_id = str(cfg[\"geo\"][\"blocks_id\"])\n",
    "    blocks_gdf = gpd.read_file(cfg[\"assets\"][\"census_blocks\"]).to_crs(epsg=4326)\n",
    "    \n",
    "    # Normalize Times\n",
    "    ts_start = pd.to_datetime(time_start) if time_start else None\n",
    "    ts_end = pd.to_datetime(time_end) if time_end else None\n",
    "    \n",
    "    t_avail_start = pd.to_datetime(availability_time_start) or ts_start\n",
    "    t_avail_end = pd.to_datetime(availability_time_end) or ts_end\n",
    "    t_usage_start = pd.to_datetime(usage_time_start) or ts_start\n",
    "    t_usage_end = pd.to_datetime(usage_time_end) or ts_end\n",
    "    t_idle_start = pd.to_datetime(idle_time_start) or ts_start\n",
    "    t_idle_end = pd.to_datetime(idle_time_end) or ts_end\n",
    "\n",
    "    results = {\"city\": city_key, \"output_dir\": str(out_dir)}\n",
    "\n",
    "    # ---------------- 1. Parse Station Status ----------------\n",
    "    print(\"Processing Station Status...\")\n",
    "    records = []\n",
    "    with Path(station_status_txt).open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip(): continue\n",
    "            try:\n",
    "                js = json.loads(line)\n",
    "                ts = list(js.keys())[0]\n",
    "                for entry in js[ts]:\n",
    "                    for vt in entry.get(\"vehicle_types_available\", []):\n",
    "                        entry[f\"vehicle_type_{vt['vehicle_type_id']}_count\"] = vt[\"count\"]\n",
    "                    entry.pop(\"vehicle_types_available\", None)\n",
    "                    entry[\"timestamp\"] = ts\n",
    "                    records.append(entry)\n",
    "            except: continue\n",
    "    \n",
    "    ss_df = pd.DataFrame(records)\n",
    "    if save_outputs: ss_df.to_csv(out_dir / f\"{city_key.lower()}_station_status_flat.csv\", index=False)\n",
    "    \n",
    "    ss_df[\"timestamp\"] = ss_df[\"timestamp\"].astype(str).str.replace(remove_tz_suffix, \"\", regex=False)\n",
    "    ss_df[\"timestamp\"] = pd.to_datetime(ss_df[\"timestamp\"], errors=\"coerce\")\n",
    "    ss_df[\"station_id\"] = _Standardizer.normalize_id(ss_df[\"station_id\"])\n",
    "    \n",
    "    if save_outputs: ss_df.to_csv(out_dir / f\"{city_key.lower()}_station_status_clean.csv\", index=False)\n",
    "\n",
    "    # ---------------- 2. Station Info & Geocoding ----------------\n",
    "    print(\"Processing Station Information...\")\n",
    "    si = _Standardizer.stations(pd.read_csv(station_information_csv))\n",
    "    \n",
    "    if \"census_block\" not in si.columns or si[\"census_block\"].isna().any():\n",
    "        mapping = _GeoUtils.geocode_points(si, \"lon\", \"lat\", blocks_gdf, blocks_id)\n",
    "        si = si.merge(mapping, on=[\"lon\", \"lat\"], how=\"left\", suffixes=(\"\", \"_new\"))\n",
    "        \n",
    "        if \"census_block_new\" in si.columns:\n",
    "            si[\"census_block\"] = si[\"census_block\"].fillna(si[\"census_block_new\"])\n",
    "            si = si.drop(columns=[\"census_block_new\"])\n",
    "        \n",
    "        if fill_missing_with_census_api:\n",
    "            si = _GeoUtils.api_fill_blocks(si, census_geocoder_benchmark, census_geocoder_vintage)\n",
    "\n",
    "    si[\"census_block\"] = si[\"census_block\"].astype(str).str.split('.').str[0]\n",
    "    si[\"census_tract\"] = _Standardizer.tract_from_block(si[\"census_block\"])\n",
    "    si[\"census_tract\"] = _Standardizer.normalize_tract(si[\"census_tract\"])\n",
    "    \n",
    "    if save_outputs: si.to_csv(out_dir / f\"{city_key.lower()}_station_information_done.csv\", index=False)\n",
    "\n",
    "    ss_done = ss_df.merge(si[[\"station_id\", \"census_block\", \"census_tract\"]], on=\"station_id\", how=\"left\")\n",
    "    \n",
    "    for c in [\"num_bikes_available\", \"num_ebikes_available\", \"num_docks_available\"]:\n",
    "        if c in ss_done.columns:\n",
    "            ss_done[c] = pd.to_numeric(ss_done[c], errors=\"coerce\").fillna(0).astype(int)\n",
    "        else:\n",
    "            ss_done[c] = 0\n",
    "            \n",
    "    ss_done[\"total_vehicle_available\"] = ss_done[\"num_bikes_available\"] + ss_done[\"num_ebikes_available\"]\n",
    "    \n",
    "    if save_outputs: ss_done.to_csv(out_dir / f\"{city_key.lower()}_station_status_done.csv\", index=False)\n",
    "\n",
    "    # ---------------- 3. Availability Metrics ----------------\n",
    "    print(\"Computing Availability...\")\n",
    "    ss_filt = ss_done.copy()\n",
    "    if t_avail_start: ss_filt = ss_filt[ss_filt[\"timestamp\"] >= t_avail_start]\n",
    "    if t_avail_end: ss_filt = ss_filt[ss_filt[\"timestamp\"] < t_avail_end]\n",
    "    \n",
    "    ext_prefix = cfg[\"geo\"][\"external_tract_prefix\"]\n",
    "    avail_res = {}\n",
    "    \n",
    "    if availability_group_level in {\"block\", \"both\"}:\n",
    "        res = _calc_availability(ss_filt, \"block\", availability_time_granularity, out_dir, save_outputs)\n",
    "        avail_res[\"block\"] = res\n",
    "        \n",
    "    if availability_group_level in {\"tract\", \"both\"}:\n",
    "        res = _calc_availability(ss_filt, \"tract\", availability_time_granularity, out_dir, save_outputs, ext_prefix)\n",
    "        avail_res[\"tract\"] = res\n",
    "\n",
    "    # ---------------- 4. Capacity Metrics ----------------\n",
    "    print(\"Computing Capacity...\")\n",
    "    tract_gdf = gpd.read_file(cfg[\"assets\"][\"tracts\"]).to_crs(epsg=4326)\n",
    "    tract_centroids = pd.DataFrame({\n",
    "        \"census_tract\": tract_gdf[cfg[\"geo\"][\"tract_id\"]].astype(str),\n",
    "        \"centroid_lon\": tract_gdf.geometry.centroid.x,\n",
    "        \"centroid_lat\": tract_gdf.geometry.centroid.y\n",
    "    })\n",
    "    \n",
    "    tract_centroids[\"census_tract\"] = _Standardizer.normalize_tract(tract_centroids[\"census_tract\"])\n",
    "    \n",
    "    if cfg[\"geo\"].get(\"drop_staten_island\", False) and \"COUNTYFP\" in tract_gdf.columns:\n",
    "        tract_centroids = tract_centroids[tract_gdf[\"COUNTYFP\"].astype(str) != \"085\"]\n",
    "\n",
    "    if save_outputs: tract_centroids.to_csv(out_dir / \"centroid_tract_computed.csv\", index=False)\n",
    "\n",
    "    cap_block = si.groupby(\"census_block\", as_index=False).agg(\n",
    "        total_capacity=(\"capacity\", \"sum\"), num_station=(\"station_id\", \"count\")\n",
    "    )\n",
    "    if save_outputs: cap_block.to_csv(out_dir / \"capacity_block.csv\", index=False)\n",
    "\n",
    "    cap_tract_agg = si.groupby(\"census_tract\", as_index=False).agg(\n",
    "        total_capacity=(\"capacity\", \"sum\"), num_station=(\"station_id\", \"count\")\n",
    "    )\n",
    "    cap_tract = tract_centroids[[\"census_tract\"]].merge(cap_tract_agg, on=\"census_tract\", how=\"left\").fillna(0)\n",
    "    \n",
    "    if ext_prefix: cap_tract = cap_tract[~cap_tract[\"census_tract\"].str.startswith(ext_prefix)]\n",
    "        \n",
    "    cap_tract[\"total_capacity_norm\"] = _minmax_norm(cap_tract[\"total_capacity\"]).round(5)\n",
    "    cap_tract[\"num_station_norm\"] = _minmax_norm(cap_tract[\"num_station\"]).round(5)\n",
    "    \n",
    "    if save_outputs: cap_tract.to_csv(out_dir / \"capacity_tract_norm.csv\", index=False)\n",
    "\n",
    "    # Peak logic\n",
    "    tract_av_norm = avail_res[\"tract\"][\"norm\"]\n",
    "    peak_agg = tract_av_norm.groupby(\"time_slot\")[peak_metric].sum()\n",
    "    peak_ts = peak_time_slot if peak_time_slot else peak_agg.idxmax()\n",
    "    \n",
    "    peak_data = tract_av_norm[tract_av_norm[\"time_slot\"] == peak_ts][\n",
    "        [\"census_tract\", \"total_vehicle_available\", \"num_docks_available\"]\n",
    "    ].rename(columns={\"total_vehicle_available\": \"vehicle_capacity\", \"num_docks_available\": \"dock_capacity\"})\n",
    "    \n",
    "    cap_df = cap_tract.merge(peak_data, on=\"census_tract\", how=\"left\").fillna(0)\n",
    "    cap_df[\"vehicle_capacity_norm\"] = _minmax_norm(cap_df[\"vehicle_capacity\"]).round(5)\n",
    "    cap_df[\"dock_capacity_norm\"] = _minmax_norm(cap_df[\"dock_capacity\"]).round(5)\n",
    "    \n",
    "    cap_df[\"occupancy_rate\"] = _safe_ratio(cap_df[\"vehicle_capacity\"], cap_df[\"total_capacity\"])\n",
    "    cap_df[\"return_pressure\"] = 1.0 - _safe_ratio(cap_df[\"dock_capacity\"], cap_df[\"total_capacity\"])\n",
    "    \n",
    "    if save_outputs: cap_df.to_csv(out_dir / \"capacity_tract_with_vehicle_and_docks_norm.csv\", index=False)\n",
    "\n",
    "    # ---------------- 5. Safety Metrics ----------------\n",
    "    print(\"Computing Safety...\")\n",
    "    \n",
    "    # CRITICAL FIX: Project to Metric CRS before calculating length\n",
    "    metric_crs = cfg[\"geo\"].get(\"metric_crs\")\n",
    "    if not metric_crs:\n",
    "        raise ValueError(f\"metric_crs is missing for {city_key}. Add it to CITY_CONFIG!\")\n",
    "    \n",
    "    # 1. Project Blocks\n",
    "    blocks_metric = blocks_gdf.to_crs(metric_crs)\n",
    "    \n",
    "    # 2. Project Lines\n",
    "    streets = _GeoUtils.load_lines(\n",
    "        Path(cfg[\"assets\"][\"centerline\"]), \n",
    "        cfg[\"geo\"][\"safety_type\"], \n",
    "        metric_crs, # Load directly into Metric CRS\n",
    "        cfg[\"geo\"].get(\"wkt_candidates\", ())\n",
    "    )\n",
    "    \n",
    "    bike_lanes = _GeoUtils.load_lines(\n",
    "        Path(cfg[\"assets\"][\"bike_lanes\"]), \n",
    "        cfg[\"geo\"][\"safety_type\"], \n",
    "        metric_crs, # Load directly into Metric CRS\n",
    "        cfg[\"geo\"].get(\"wkt_candidates\", ())\n",
    "    )\n",
    "    \n",
    "    rule = cfg[\"safety_rule\"]\n",
    "    p_col, protected = _protected_filter(bike_lanes, rule)\n",
    "    \n",
    "    st_len = _overlay_length_by_block(streets, blocks_metric, blocks_id, \"streets_leng\")\n",
    "    bl_len = _overlay_length_by_block(bike_lanes, blocks_metric, blocks_id, \"total_bike_lane_length\")\n",
    "    \n",
    "    if p_col:\n",
    "        pr_len = _overlay_length_by_block(protected, blocks_metric, blocks_id, \"protected_bike_lane_length\")\n",
    "    else:\n",
    "        pr_len = pd.DataFrame({\"census_block\": st_len[\"census_block\"].unique(), \"protected_bike_lane_length\": 0.0})\n",
    "\n",
    "    safe_blk = st_len.merge(bl_len, on=\"census_block\", how=\"left\").merge(pr_len, on=\"census_block\", how=\"left\").fillna(0)\n",
    "    \n",
    "    # Normalize\n",
    "    safe_blk[\"census_tract\"] = _Standardizer.tract_from_block(safe_blk[\"census_block\"])\n",
    "    safe_blk[\"census_tract\"] = _Standardizer.normalize_tract(safe_blk[\"census_tract\"])\n",
    "    \n",
    "    safe_blk[\"bike_lane_ratio\"] = _safe_ratio(safe_blk[\"total_bike_lane_length\"], safe_blk[\"streets_leng\"]).round(3)\n",
    "    safe_blk[\"protected_bike_lane_ratio\"] = _safe_ratio(safe_blk[\"protected_bike_lane_length\"], safe_blk[\"streets_leng\"]).round(3)\n",
    "    \n",
    "    if save_outputs: safe_blk.to_csv(out_dir / \"safety_bike_lane_block.csv\", index=False)\n",
    "\n",
    "    cent_meta = pd.read_csv(out_dir / \"centroid_tract_computed.csv\", dtype={\"census_tract\": str})\n",
    "    cent_meta[\"census_tract\"] = _Standardizer.normalize_tract(cent_meta[\"census_tract\"])\n",
    "    \n",
    "    safe_tract = safe_blk.groupby(\"census_tract\", as_index=False)[\n",
    "        [\"streets_leng\", \"total_bike_lane_length\", \"protected_bike_lane_length\"]\n",
    "    ].sum()\n",
    "    \n",
    "    safe_tract = cent_meta.merge(safe_tract, on=\"census_tract\", how=\"left\").fillna(0)\n",
    "    safe_tract[\"bike_lane_ratio\"] = _safe_ratio(safe_tract[\"total_bike_lane_length\"], safe_tract[\"streets_leng\"])\n",
    "    safe_tract[\"protected_bike_lane_ratio\"] = _safe_ratio(safe_tract[\"protected_bike_lane_length\"], safe_tract[\"streets_leng\"])\n",
    "    \n",
    "    service_tracts = set(cap_tract[\"census_tract\"].unique())\n",
    "    safe_service = safe_tract[safe_tract[\"census_tract\"].isin(service_tracts)].copy()\n",
    "    \n",
    "    safe_norm = safe_service.copy()\n",
    "    safe_norm[\"bike_lane_ratio_norm\"] = _minmax_norm(safe_norm[\"bike_lane_ratio\"]).round(5)\n",
    "    safe_norm[\"protected_bike_lane_ratio_norm\"] = _minmax_norm(safe_norm[\"protected_bike_lane_ratio\"]).round(5)\n",
    "    \n",
    "    if save_outputs:\n",
    "        safe_tract.to_csv(out_dir / \"safety_bike_lane_tract.csv\", index=False)\n",
    "        safe_service.to_csv(out_dir / \"safety_bike_lane_service_area.csv\", index=False)\n",
    "        safe_norm.to_csv(out_dir / \"safety_bike_lane_norm_tract.csv\", index=False)\n",
    "\n",
    "    # ---------------- 6. Usage Metrics ----------------\n",
    "    print(\"Computing Usage...\")\n",
    "    trips = _Standardizer.trips(pd.read_csv(trip_csv, engine=\"c\"))\n",
    "    trips[\"started_at\"] = pd.to_datetime(trips[\"started_at\"], errors=\"coerce\")\n",
    "    trips[\"ended_at\"] = pd.to_datetime(trips[\"ended_at\"], errors=\"coerce\")\n",
    "    \n",
    "    t_min, t_max = trips[\"started_at\"].min(), trips[\"started_at\"].max()\n",
    "    t_win_start = t_usage_start if t_usage_start else t_min\n",
    "    t_win_end = t_usage_end if t_usage_end else t_max\n",
    "    \n",
    "    if t_win_start > t_max or t_win_end < t_min:\n",
    "        print(f\"âš ï¸ WARNING: Request Window ({t_win_start} - {t_win_end}) does not overlap Trip Data ({t_min} - {t_max}).\")\n",
    "\n",
    "    station_block_map = si[[\"station_id\", \"census_block\"]].copy()\n",
    "    \n",
    "    if \"start_station_id\" in trips.columns:\n",
    "        if \"start_census_block\" in trips.columns: trips.drop(columns=[\"start_census_block\"], inplace=True)\n",
    "        trips = trips.merge(station_block_map, left_on=\"start_station_id\", right_on=\"station_id\", how=\"left\")\n",
    "        trips = trips.rename(columns={\"census_block\": \"start_census_block\"}).drop(columns=[\"station_id\"])\n",
    "        \n",
    "    if \"end_station_id\" in trips.columns:\n",
    "        if \"end_census_block\" in trips.columns: trips.drop(columns=[\"end_census_block\"], inplace=True)\n",
    "        trips = trips.merge(station_block_map, left_on=\"end_station_id\", right_on=\"station_id\", how=\"left\")\n",
    "        trips = trips.rename(columns={\"census_block\": \"end_census_block\"}).drop(columns=[\"station_id\"])\n",
    "\n",
    "    if \"start_census_block\" not in trips.columns or trips[\"start_census_block\"].isna().all():\n",
    "        print(\"Fallback: Using coordinate geocoding for trips...\")\n",
    "        if \"start_census_block\" in trips.columns: trips.drop(columns=[\"start_census_block\"], inplace=True)\n",
    "        s_map = _GeoUtils.geocode_points(trips, \"start_lng\", \"start_lat\", blocks_gdf, blocks_id)\n",
    "        trips = trips.merge(s_map, left_on=[\"start_lng\", \"start_lat\"], right_on=[\"lon\", \"lat\"], how=\"left\") \\\n",
    "                     .rename(columns={\"census_block\": \"start_census_block\"}).drop(columns=[\"lon\", \"lat\"])\n",
    "        \n",
    "        if \"end_census_block\" in trips.columns: trips.drop(columns=[\"end_census_block\"], inplace=True)\n",
    "        e_map = _GeoUtils.geocode_points(trips, \"end_lng\", \"end_lat\", blocks_gdf, blocks_id)\n",
    "        trips = trips.merge(e_map, left_on=[\"end_lng\", \"end_lat\"], right_on=[\"lon\", \"lat\"], how=\"left\") \\\n",
    "                     .rename(columns={\"census_block\": \"end_census_block\"}).drop(columns=[\"lon\", \"lat\"])\n",
    "\n",
    "    trips[\"start_census_block\"] = trips[\"start_census_block\"].astype(str).str.split('.').str[0]\n",
    "    trips[\"end_census_block\"] = trips[\"end_census_block\"].astype(str).str.split('.').str[0]\n",
    "    \n",
    "    usage_trips = trips[(trips[\"started_at\"] >= t_win_start) & (trips[\"started_at\"] <= t_win_end)].copy()\n",
    "    usage_trips = usage_trips[(usage_trips[\"ended_at\"] - usage_trips[\"started_at\"]).dt.total_seconds() <= 240*60]\n",
    "    \n",
    "    all_blocks = pd.concat([usage_trips[\"start_census_block\"], usage_trips[\"end_census_block\"]]).unique()\n",
    "    all_blocks = [x for x in all_blocks if x != \"nan\"]\n",
    "\n",
    "    _calc_usage(usage_trips, (t_win_start, t_win_end), all_blocks, \"5min\", \"usage_5min_block\", out_dir, save_outputs)\n",
    "    u_hr_blk = _calc_usage(usage_trips, (t_win_start, t_win_end), all_blocks, \"1h\", \"usage_hourly_block\", out_dir, save_outputs)\n",
    "    \n",
    "    u_hr_tract = u_hr_blk.copy()\n",
    "    u_hr_tract[\"census_tract\"] = _Standardizer.tract_from_block(u_hr_tract[\"census_block\"])\n",
    "    u_hr_tract[\"census_tract\"] = _Standardizer.normalize_tract(u_hr_tract[\"census_tract\"])\n",
    "    if ext_prefix: u_hr_tract = u_hr_tract[~u_hr_tract[\"census_tract\"].str.startswith(ext_prefix)]\n",
    "    \n",
    "    u_hr_tract = u_hr_tract.groupby([\"census_tract\", \"time_slot\"], as_index=False)[\n",
    "        [\"trips_starting\", \"trips_ending\"]\n",
    "    ].sum()\n",
    "    \n",
    "    if save_outputs: u_hr_tract.to_csv(out_dir / \"usage_hourly_tract_raw.csv\", index=False)\n",
    "    \n",
    "    if tracts_to_remove:\n",
    "        u_hr_tract = u_hr_tract[~u_hr_tract[\"census_tract\"].isin(tracts_to_remove)]\n",
    "        \n",
    "    u_hr_tract[\"trips_starting_norm\"] = _minmax_norm(u_hr_tract[\"trips_starting\"]).round(5)\n",
    "    u_hr_tract[\"trips_ending_norm\"] = _minmax_norm(u_hr_tract[\"trips_ending\"]).round(5)\n",
    "    \n",
    "    if save_outputs: u_hr_tract.to_csv(out_dir / \"usage_norm_hourly_tract.csv\", index=False)\n",
    "\n",
    "    # ---------------- 7. Idle Simulation ----------------\n",
    "    if not compute_idle_time:\n",
    "        print(\"Skipping Idle Time.\")\n",
    "        return results\n",
    "\n",
    "    print(\"Computing Idle Time...\")\n",
    "    t_i_start = t_idle_start if t_idle_start else t_u_start\n",
    "    t_i_end = t_idle_end if t_idle_end else t_u_end\n",
    "    \n",
    "    idle_trips = trips[(trips[\"started_at\"] >= t_i_start) & (trips[\"started_at\"] <= t_i_end)].copy()\n",
    "    if \"rideable_type\" in idle_trips.columns:\n",
    "        idle_trips = _map_rideable_type_to_two_columns(idle_trips)\n",
    "        has_rideable = True\n",
    "    else:\n",
    "        has_rideable = False\n",
    "\n",
    "    ss_idle = ss_done[(ss_done[\"timestamp\"] >= t_i_start) & (ss_done[\"timestamp\"] <= t_i_end)].copy()\n",
    "    ss_idle[\"time_slot\"] = ss_idle[\"timestamp\"].dt.floor(\"5min\") - pd.Timedelta(minutes=5)\n",
    "    \n",
    "    ss_agg = ss_idle.groupby([\"census_block\", \"time_slot\"], as_index=False)[\n",
    "        [\"total_vehicle_available\"]\n",
    "    ].sum()\n",
    "    \n",
    "    flux = _calc_usage(idle_trips, (t_i_start, t_i_end), all_blocks, \"5min\", \"temp_flux\", out_dir, False)\n",
    "    flux[\"vehicles_moved\"] = flux[\"trips_starting\"] + flux[\"trips_ending\"]\n",
    "    \n",
    "    merged_idle = ss_agg.merge(flux, on=[\"census_block\", \"time_slot\"], how=\"left\").fillna(0)\n",
    "    merged_idle[\"idle_vehicles\"] = (merged_idle[\"total_vehicle_available\"] - merged_idle[\"vehicles_moved\"]).clip(lower=0)\n",
    "    merged_idle[\"hour\"] = merged_idle[\"time_slot\"].dt.floor(\"1h\")\n",
    "    \n",
    "    if save_outputs: merged_idle.to_csv(out_dir / \"idle_merged_5min.csv\", index=False)\n",
    "\n",
    "    idle_results = []\n",
    "    for block, block_df in merged_idle.groupby(\"census_block\"):\n",
    "        for hour, hour_df in block_df.groupby(\"hour\"):\n",
    "            hour_df = hour_df.sort_values(\"time_slot\")\n",
    "            virtual_vehicles = deque()\n",
    "            finalized_durations = []\n",
    "\n",
    "            for _, row in hour_df.iterrows():\n",
    "                current_time = row[\"time_slot\"]\n",
    "                current_idle = int(row[\"idle_vehicles\"])\n",
    "\n",
    "                while current_idle > len(virtual_vehicles):\n",
    "                    virtual_vehicles.append(current_time)\n",
    "                while current_idle < len(virtual_vehicles):\n",
    "                    start_t = virtual_vehicles.popleft()\n",
    "                    duration = (current_time - start_t).total_seconds() / 60.0\n",
    "                    finalized_durations.append(duration)\n",
    "\n",
    "            final_time = hour + pd.Timedelta(hours=1)\n",
    "            while virtual_vehicles:\n",
    "                start_t = virtual_vehicles.popleft()\n",
    "                duration = (final_time - start_t).total_seconds() / 60.0\n",
    "                finalized_durations.append(duration)\n",
    "\n",
    "            avg_idle = round(sum(finalized_durations) / len(finalized_durations), 2) if finalized_durations else 0.0\n",
    "            idle_results.append({\"census_block\": block, \"hour\": hour, \"avg_idle_time\": avg_idle})\n",
    "\n",
    "    idle_df = pd.DataFrame(idle_results)\n",
    "    if save_outputs: idle_df.to_csv(out_dir / \"idle_time_block.csv\", index=False)\n",
    "    \n",
    "    if not idle_df.empty:\n",
    "        idle_df[\"census_tract\"] = _Standardizer.tract_from_block(idle_df[\"census_block\"].astype(str))\n",
    "        idle_df[\"census_tract\"] = _Standardizer.normalize_tract(idle_df[\"census_tract\"])\n",
    "        \n",
    "        if ext_prefix:\n",
    "            idle_df = idle_df[~idle_df[\"census_tract\"].str.startswith(ext_prefix)]\n",
    "            \n",
    "        idle_tract = idle_df.groupby([\"census_tract\", \"hour\"], as_index=False)[\"avg_idle_time\"].mean()\n",
    "        idle_tract[\"avg_idle_time_norm\"] = _minmax_norm(idle_tract[\"avg_idle_time\"]).round(5)\n",
    "        \n",
    "        if save_outputs:\n",
    "            idle_tract.to_csv(out_dir / \"idle_time_norm_tract.csv\", index=False)\n",
    "\n",
    "    print(\"Done.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d8b2383",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "DOCKED BIKE SHARE UTILITY ANALYTICS PIPELINE (PRODUCTION v2)\n",
    "=============================================================================\n",
    "Updates:\n",
    "- ADDED: Multi-file aggregation. If 'trip_csv' is a directory, all CSVs inside \n",
    "  are merged automatically (handles NYC's 5-file split).\n",
    "- ADDED: Zip file support for 'trip_csv'.\n",
    "- IMPROVED: Comments and structure for readability.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import warnings\n",
    "import zipfile\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress specific warnings for cleaner logs\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. GLOBAL CONFIGURATION\n",
    "#    Define cities, file paths, and coordinate systems here.\n",
    "# =============================================================================\n",
    "CITY_CONFIG: Dict[str, Dict[str, Any]] = {\n",
    "    \"SF\": {\n",
    "        \"use_api_fallback\": True, # SF often needs API for boundary stations\n",
    "        \"assets\": {\n",
    "            \"census_blocks\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Usage\\San_Fran_Baywheels\\tl_2024_06_tabblock20.shp\",\n",
    "            \"tracts\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\San_Fran_Baywheels\\tl_2024_06_tract.shp\",\n",
    "            \"centerline\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\San_Fran_Baywheels\\Streets___Active_and_Retired_20250626 (1).csv\",\n",
    "            \"bike_lanes\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\San_Fran_Baywheels\\Bikelane.csv\",\n",
    "        },\n",
    "        \"geo\": {\n",
    "            \"blocks_id\": \"GEOID20\",\n",
    "            \"tract_id\": \"GEOID\",\n",
    "            \"crs\": \"EPSG:4326\",\n",
    "            \"metric_crs\": \"EPSG:26910\",   # UTM Zone 10N (Meters)\n",
    "            \"safety_type\": \"csv_wkt\",\n",
    "            \"wkt_candidates\": (\"geometry\", \"shape\", \"line\", \"the_geom\", \"wkt\", \"geometry_wkt\", \"WKT\", \"geom\"),\n",
    "            \"external_tract_prefix\": None,\n",
    "            \"drop_staten_island\": False,\n",
    "        },\n",
    "        \"safety_rule\": {\n",
    "            \"col_candidates\": (\"BARRIER\", \"FACILITY_T\", \"BUFFERED\", \"RAISED\", \"SYMBOLOGY\"),\n",
    "            \"match_type\": \"not_empty_or_contains\",\n",
    "            \"match_value\": r\"PROTECT|SEPARAT|CYCLETRACK|BARRIER|RAISED|BUFFER\",\n",
    "        },\n",
    "    },\n",
    "\n",
    "    \"NJ\": {\n",
    "        \"use_api_fallback\": False, # Disabled to ignore NYC stations in NJ files\n",
    "        \"assets\": {\n",
    "            \"census_blocks\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NJ\\tl_2024_34_tabblock20.shp\",\n",
    "            \"tracts\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\NJ\\tl_2024_34_tract.shp\",\n",
    "            \"centroid_csv\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\NJ\\centroid_tract_nj.csv\",\n",
    "            \"centerline\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NJ\\Tran_road.shp\",\n",
    "            \"bike_lanes\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NJ\\bike-lanes-2020-division-of-transportation.shp\",\n",
    "        },\n",
    "        \"geo\": {\n",
    "            \"blocks_id\": \"GEOID20\",\n",
    "            \"tract_id\": \"GEOID\",\n",
    "            \"crs\": \"EPSG:4326\",\n",
    "            \"metric_crs\": \"EPSG:32618\",\n",
    "            \"safety_type\": \"shp\",\n",
    "            \"wkt_candidates\": (),\n",
    "            \"external_tract_prefix\": None,\n",
    "            \"drop_staten_island\": False,\n",
    "        },\n",
    "        \"safety_rule\": {\n",
    "            \"col_candidates\": (\"type\", \"facility\", \"facilitycl\", \"class\", \"lane_type\"),\n",
    "            \"match_type\": \"contains\",\n",
    "            \"match_value\": \"PROTECT\",\n",
    "        },\n",
    "    },\n",
    "\n",
    "    \"NYC\": {\n",
    "        \"use_api_fallback\": True,\n",
    "        \"assets\": {\n",
    "            \"census_blocks\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\GBFS_Census_Tract\\NYC\\tl_2024_36_tabblock20.shp\",\n",
    "            \"tracts\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\NYC\\tl_2024_36_tract.shp\",\n",
    "            \"centroid_csv\": r\"D:\\Research Fellowship\\Capacity_NYC\\centroid_tract_computed.csv\",\n",
    "            \"centerline\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NYC\\CSCL_PlowNYC_20250619.csv\",\n",
    "            \"bike_lanes\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\NYC\\New_York_City_Bike_Routes_20250619.csv\",\n",
    "        },\n",
    "        \"geo\": {\n",
    "            \"blocks_id\": \"GEOID20\",\n",
    "            \"tract_id\": \"GEOID\",\n",
    "            \"crs\": \"EPSG:4326\",\n",
    "            \"metric_crs\": \"EPSG:32618\",   # UTM Zone 18N (Meters)\n",
    "            \"safety_type\": \"csv_wkt\",\n",
    "            \"wkt_candidates\": (\"geometry\", \"the_geom\", \"wkt\", \"geometry_wkt\", \"WKT\"),\n",
    "            \"external_tract_prefix\": \"34\", # Exclude NJ\n",
    "            \"drop_staten_island\": True,\n",
    "        },\n",
    "        \"safety_rule\": {\n",
    "            \"col_candidates\": (\"facilitycl\", \"facility\", \"class\", \"ft\", \"type\"),\n",
    "            \"match_type\": \"equals\",\n",
    "            \"match_value\": \"I\",\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    \"PITT\": {\n",
    "        \"use_api_fallback\": True,\n",
    "        \"assets\": {\n",
    "            \"census_blocks\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Pitt\\tl_2024_42_tabblock20.shp\",\n",
    "            \"tracts\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\Pitt\\tl_2024_42_tract.shp\",\n",
    "            \"centroid_csv\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\Pitt\\centroid_tract_pa.csv\",\n",
    "            \"centerline\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Pitt\\Pittsburgh_Street_Centerline.shp\",\n",
    "            \"bike_lanes\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Pitt\\Bike Map\\Bike Lanes.shp\",\n",
    "        },\n",
    "        \"geo\": {\n",
    "            \"blocks_id\": \"GEOID20\",\n",
    "            \"tract_id\": \"GEOID\",\n",
    "            \"crs\": \"EPSG:4326\",\n",
    "            \"metric_crs\": \"EPSG:32617\",\n",
    "            \"safety_type\": \"shp\",\n",
    "            \"wkt_candidates\": (),\n",
    "            \"external_tract_prefix\": None,\n",
    "            \"drop_staten_island\": False,\n",
    "        },\n",
    "        \"safety_rule\": {\n",
    "            \"col_candidates\": (\"facility\", \"type\", \"class\", \"status\", \"lane_type\", \"BIKE_FACIL\", \"CATEGORY\"),\n",
    "            \"match_type\": \"contains\",\n",
    "            \"match_value\": \"PROTECT\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA HELPER (Handling File IO and Aggregation)\n",
    "# =============================================================================\n",
    "class DataHelper:\n",
    "    \"\"\"Helper class for reading data, standardizing columns, and normalization.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def pick_col(df: pd.DataFrame, candidates: Sequence[str], required: bool = False, label: str = \"\") -> Optional[str]:\n",
    "        \"\"\"Smartly selects a column from a list of potential aliases.\"\"\"\n",
    "        cols = set(df.columns)\n",
    "        for c in candidates:\n",
    "            if c in cols: return c\n",
    "        if required:\n",
    "            raise ValueError(f\"Missing required col for {label}. Tried: {candidates}\")\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_id(s: pd.Series) -> pd.Series:\n",
    "        \"\"\"Standardizes IDs: Strings, uppercase, trimmed, no decimals.\"\"\"\n",
    "        return s.astype(str).str.strip().str.upper().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_tract(s: pd.Series) -> pd.Series:\n",
    "        \"\"\"Ensures 11-digit Tract IDs (pads with leading zeros).\"\"\"\n",
    "        return s.astype(str).str.split('.').str[0].str.zfill(11)\n",
    "\n",
    "    @staticmethod\n",
    "    def tract_from_block(s: pd.Series) -> pd.Series:\n",
    "        \"\"\"Extracts Tract ID from a full Block ID.\"\"\"\n",
    "        s = s.astype(str).str.split('.').str[0]\n",
    "        return s.str[:11]\n",
    "\n",
    "    @classmethod\n",
    "    def load_stations(cls, path: Path) -> pd.DataFrame:\n",
    "        \"\"\"Loads Station Information and standardizes names.\"\"\"\n",
    "        df = pd.read_csv(path)\n",
    "        mapping = {\n",
    "            cls.pick_col(df, [\"station_id\", \"stationId\", \"id\", \"Station ID\"], True, \"station_id\"): \"station_id\",\n",
    "            cls.pick_col(df, [\"lat\", \"latitude\", \"y\"], True, \"lat\"): \"lat\",\n",
    "            cls.pick_col(df, [\"lon\", \"lng\", \"longitude\", \"x\"], True, \"lon\"): \"lon\",\n",
    "            cls.pick_col(df, [\"capacity\", \"dock_count\", \"num_docks\"], True, \"capacity\"): \"capacity\"\n",
    "        }\n",
    "        out = df.rename(columns=mapping)\n",
    "        out[\"station_id\"] = cls.normalize_id(out[\"station_id\"])\n",
    "        \n",
    "        # Preserve existing geo columns if they exist\n",
    "        cb = cls.pick_col(out, [\"census_block\", \"block_geoid\", \"GEOID20\"])\n",
    "        if cb and cb != \"census_block\": out = out.rename(columns={cb: \"census_block\"})\n",
    "        \n",
    "        ct = cls.pick_col(out, [\"census_tract\", \"tract_geoid\", \"GEOID\"])\n",
    "        if ct and ct != \"census_tract\": out = out.rename(columns={ct: \"census_tract\"})\n",
    "        \n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def load_trips(cls, path: Union[str, Path]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads Trip Data.\n",
    "        - If 'path' is a file: Loads that single CSV (or ZIP).\n",
    "        - If 'path' is a directory: Loads and aggregates ALL *.csv files inside (e.g., NYC split files).\n",
    "        \"\"\"\n",
    "        path = Path(path)\n",
    "        dfs = []\n",
    "        \n",
    "        print(f\"   -> Reading trip data from: {path}\")\n",
    "\n",
    "        # 1. Gather all dataframes\n",
    "        if path.is_dir():\n",
    "            # Directory Mode: Read all CSVs\n",
    "            files = sorted(path.glob(\"*.csv\"))\n",
    "            if not files: raise ValueError(f\"No .csv files found in directory: {path}\")\n",
    "            \n",
    "            print(f\"   -> Found {len(files)} trip files. Aggregating...\")\n",
    "            for f in files:\n",
    "                # low_memory=False avoids DtypeWarnings on large files\n",
    "                dfs.append(pd.read_csv(f, engine=\"c\", low_memory=False))\n",
    "        \n",
    "        elif path.suffix.lower() == '.zip':\n",
    "            # Zip Mode: Read all CSVs inside archive\n",
    "            print(\"   -> Detected ZIP file. Reading internal CSVs...\")\n",
    "            with zipfile.ZipFile(path) as z:\n",
    "                for name in z.namelist():\n",
    "                    if name.lower().endswith('.csv') and not name.startswith('__MACOSX'):\n",
    "                        with z.open(name) as f:\n",
    "                            dfs.append(pd.read_csv(f, low_memory=False))\n",
    "        else:\n",
    "            # Single File Mode\n",
    "            dfs.append(pd.read_csv(path, engine=\"c\", low_memory=False))\n",
    "            \n",
    "        # 2. Concatenate\n",
    "        if not dfs: raise ValueError(\"No trip data found.\")\n",
    "        df = pd.concat(dfs, ignore_index=True) if len(dfs) > 1 else dfs[0]\n",
    "        \n",
    "        # 3. Standardize Columns\n",
    "        mapping = {\n",
    "            cls.pick_col(df, [\"started_at\", \"start_time\", \"starttime\", \"Start Date\"], True, \"start\"): \"started_at\",\n",
    "            cls.pick_col(df, [\"ended_at\", \"end_time\", \"stoptime\", \"End Date\"], True, \"end\"): \"ended_at\"\n",
    "        }\n",
    "        \n",
    "        opt_map = {\n",
    "            cls.pick_col(df, [\"rideable_type\", \"bike_type\", \"vehicle_type\"]): \"rideable_type\",\n",
    "            cls.pick_col(df, [\"start_lat\", \"startLatitude\", \"StartLat\"]): \"start_lat\",\n",
    "            cls.pick_col(df, [\"start_lng\", \"startLongitude\", \"StartLng\"]): \"start_lng\",\n",
    "            cls.pick_col(df, [\"end_lat\", \"endLatitude\", \"EndLat\"]): \"end_lat\",\n",
    "            cls.pick_col(df, [\"end_lng\", \"endLongitude\", \"EndLng\"]): \"end_lng\",\n",
    "            cls.pick_col(df, [\"start_station_id\", \"Start Station Id\"]): \"start_station_id\",\n",
    "            cls.pick_col(df, [\"end_station_id\", \"End Station Id\"]): \"end_station_id\"\n",
    "        }\n",
    "        mapping.update({k: v for k, v in opt_map.items() if k})\n",
    "        \n",
    "        df = df.rename(columns=mapping)\n",
    "        \n",
    "        # Normalize IDs\n",
    "        if \"start_station_id\" in df.columns:\n",
    "            df[\"start_station_id\"] = cls.normalize_id(df[\"start_station_id\"])\n",
    "        if \"end_station_id\" in df.columns:\n",
    "            df[\"end_station_id\"] = cls.normalize_id(df[\"end_station_id\"])\n",
    "            \n",
    "        return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. GEO HELPER (Spatial Operations)\n",
    "# =============================================================================\n",
    "class GeoHelper:\n",
    "    \"\"\"Handles loading shapes, reprojection, spatial joins, and API lookups.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_network_lines(path: Path, kind: str, target_crs, candidates: tuple) -> gpd.GeoDataFrame:\n",
    "        \"\"\"Loads geometric lines (streets/lanes) and projects to Metric CRS.\"\"\"\n",
    "        if kind == \"shp\":\n",
    "            gdf = gpd.read_file(path)\n",
    "            if gdf.crs is None: raise ValueError(f\"No CRS found in {path}\")\n",
    "            return gdf.to_crs(target_crs)\n",
    "        \n",
    "        if isinstance(candidates, str): candidates = (candidates,)\n",
    "        df = pd.read_csv(path, engine=\"c\")\n",
    "        wkt_col = DataHelper.pick_col(df, candidates, True, \"WKT Column\")\n",
    "        \n",
    "        def safe_load(x):\n",
    "            try: return wkt.loads(str(x))\n",
    "            except: return None\n",
    "            \n",
    "        df[\"geometry\"] = df[wkt_col].apply(safe_load)\n",
    "        gdf = gpd.GeoDataFrame(df[df[\"geometry\"].notna()], geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "        return gdf.to_crs(target_crs)\n",
    "\n",
    "    @staticmethod\n",
    "    def geocode_points(df: pd.DataFrame, lon_col: str, lat_col: str, blocks_gdf: gpd.GeoDataFrame, id_col: str) -> pd.DataFrame:\n",
    "        \"\"\"Spatially joins points to Census Blocks.\"\"\"\n",
    "        points = df[[lon_col, lat_col]].dropna().drop_duplicates()\n",
    "        if points.empty:\n",
    "            return pd.DataFrame(columns=[lon_col, lat_col, \"census_block\"])\n",
    "\n",
    "        gdf = gpd.GeoDataFrame(\n",
    "            points, \n",
    "            geometry=[Point(xy) for xy in zip(points[lon_col], points[lat_col])],\n",
    "            crs=\"EPSG:4326\"\n",
    "        )\n",
    "        \n",
    "        joined = gpd.sjoin(gdf, blocks_gdf[[id_col, \"geometry\"]], how=\"left\", predicate=\"within\")\n",
    "        joined = joined.rename(columns={id_col: \"census_block\"})\n",
    "        \n",
    "        out = joined[[lon_col, lat_col, \"census_block\"]].copy()\n",
    "        out = out.rename(columns={lon_col: \"lon\", lat_col: \"lat\"})\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def api_fill_blocks(df: pd.DataFrame, benchmark: str, vintage: str) -> pd.DataFrame:\n",
    "        \"\"\"Queries the US Census API for missing block IDs.\"\"\"\n",
    "        missing = df[df[\"census_block\"].isna()][[\"lat\", \"lon\"]].drop_duplicates()\n",
    "        if missing.empty: return df\n",
    "        \n",
    "        # Limit API calls to prevent timeouts\n",
    "        if len(missing) > 5000:\n",
    "            print(f\"âš ï¸ Warning: {len(missing)} missing blocks. Capping API fallback at 5000 points.\")\n",
    "            missing = missing.head(5000)\n",
    "\n",
    "        def fetch(r):\n",
    "            try:\n",
    "                url = f\"https://geocoding.geo.census.gov/geocoder/geographies/coordinates?x={r.lon}&y={r.lat}&benchmark={benchmark}&vintage={vintage}&format=json\"\n",
    "                js = requests.get(url, timeout=5).json()\n",
    "                return js[\"result\"][\"geographies\"][\"2020 Census Blocks\"][0][\"GEOID\"]\n",
    "            except: return None\n",
    "\n",
    "        tqdm.pandas(desc=\"API Geocoding Fallback\")\n",
    "        missing[\"new_block\"] = missing.progress_apply(fetch, axis=1)\n",
    "        \n",
    "        merged = df.merge(missing, on=[\"lat\", \"lon\"], how=\"left\")\n",
    "        if \"new_block\" in merged.columns:\n",
    "            merged[\"census_block\"] = merged[\"census_block\"].fillna(merged[\"new_block\"])\n",
    "            merged = merged.drop(columns=[\"new_block\"])\n",
    "        return merged\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. METRIC HELPER (Calculations)\n",
    "# =============================================================================\n",
    "class MetricHelper:\n",
    "    \"\"\"Contains logic for Availability, Usage, and Normalization.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(series: pd.Series) -> pd.Series:\n",
    "        \"\"\"Min-Max Normalization.\"\"\"\n",
    "        s = pd.to_numeric(series, errors=\"coerce\")\n",
    "        mn, mx = s.min(skipna=True), s.max(skipna=True)\n",
    "        if pd.isna(mn) or pd.isna(mx) or mx <= mn: return pd.Series(0.0, index=s.index)\n",
    "        return (s - mn) / (mx - mn)\n",
    "\n",
    "    @staticmethod\n",
    "    def safe_ratio(numer: pd.Series, denom: pd.Series) -> pd.Series:\n",
    "        \"\"\"Safe Division (returns 0 instead of NaN/Inf).\"\"\"\n",
    "        return numer.div(denom.where(denom > 0, other=pd.NA)).fillna(0)\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_availability(df: pd.DataFrame, level: str, gran: str, out_dir: Path, save: bool, ext_prefix: str = None) -> Dict:\n",
    "        \"\"\"Calculates aggregated availability metrics.\"\"\"\n",
    "        geo = \"census_block\" if level == \"block\" else \"census_tract\"\n",
    "        agg = df.groupby([geo, \"timestamp\"], as_index=False)[\n",
    "            [\"num_bikes_available\", \"num_ebikes_available\", \"num_docks_available\", \"total_vehicle_available\"]\n",
    "        ].sum()\n",
    "        \n",
    "        agg[\"time_slot\"] = agg[\"timestamp\"].dt.floor(gran)\n",
    "        raw = agg.groupby([geo, \"time_slot\"], as_index=False).mean(numeric_only=True).round(0)\n",
    "        for c in raw.columns: \n",
    "            if \"available\" in c: raw[c] = raw[c].astype(int)\n",
    "                \n",
    "        if level == \"tract\":\n",
    "            raw[\"census_tract\"] = DataHelper.normalize_tract(raw[\"census_tract\"])\n",
    "            if ext_prefix: raw = raw[~raw[\"census_tract\"].str.startswith(ext_prefix)]\n",
    "\n",
    "        norm = raw.copy()\n",
    "        for c in [\"num_bikes_available\", \"num_ebikes_available\", \"num_docks_available\", \"total_vehicle_available\"]:\n",
    "            norm[f\"{c}_norm\"] = MetricHelper.normalize(norm[c]).round(5)\n",
    "\n",
    "        if save:\n",
    "            raw.to_csv(out_dir / f\"availability__raw__{level}.csv\", index=False)\n",
    "            norm.to_csv(out_dir / f\"availability__norm__{level}.csv\", index=False)\n",
    "        return {\"raw\": raw, \"norm\": norm}\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_usage(trips: pd.DataFrame, time_range: Tuple, blocks: List, gran_str: str, out_name: str, out_dir: Path, save: bool) -> pd.DataFrame:\n",
    "        \"\"\"Calculates usage (starts/ends) using a full grid to preserve 0s.\"\"\"\n",
    "        # Note: 'slot_start' must already exist in trips df\n",
    "        \n",
    "        starts = trips.groupby([\"start_census_block\", \"slot_start\"]).size().rename(\"trips_starting\")\n",
    "        ends = trips.groupby([\"end_census_block\", \"slot_end\"]).size().rename(\"trips_ending\")\n",
    "        \n",
    "        # Grid expansion\n",
    "        slots = pd.date_range(start=time_range[0].floor(gran_str), end=time_range[1].ceil(gran_str), freq=gran_str)\n",
    "        grid = pd.MultiIndex.from_product([blocks, slots], names=[\"census_block\", \"time_slot\"]).to_frame(index=False)\n",
    "        \n",
    "        out = grid.merge(starts, left_on=[\"census_block\", \"time_slot\"], right_index=True, how=\"left\") \\\n",
    "                  .merge(ends, left_on=[\"census_block\", \"time_slot\"], right_index=True, how=\"left\")\n",
    "                  \n",
    "        out = out.fillna(0).astype({\"trips_starting\": int, \"trips_ending\": int})\n",
    "        if save: out.to_csv(out_dir / f\"{out_name}.csv\", index=False)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def map_rideable_type(trips: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = trips.copy()\n",
    "        if \"rideable_type\" not in df.columns: return df\n",
    "        t = df[\"rideable_type\"].astype(str).str.lower()\n",
    "        df[\"_is_ebike\"] = t.str.contains(\"electric|ebike|e-bike|assist\", na=False)\n",
    "        return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. MAIN PIPELINE\n",
    "# =============================================================================\n",
    "def run_docked_all_utilities_single_function(\n",
    "    *,\n",
    "    city: str,\n",
    "    station_status_txt: Union[str, Path],\n",
    "    station_information_csv: Union[str, Path],\n",
    "    trip_csv: Union[str, Path],\n",
    "    output_dir: Optional[Union[str, Path]] = None,\n",
    "    save_outputs: bool = True,\n",
    "    \n",
    "    # Global Time Window\n",
    "    time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    \n",
    "    # Feature Flags\n",
    "    compute_idle_time: bool = True,\n",
    "    fill_missing_with_census_api: bool = True,\n",
    "    \n",
    "    # Overrides (Optional)\n",
    "    availability_time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    availability_time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    usage_time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    usage_time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    idle_time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    idle_time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    \n",
    "    # Settings\n",
    "    availability_time_granularity: str = \"1h\",\n",
    "    availability_group_level: str = \"both\",\n",
    "    peak_time_slot: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    peak_metric: str = \"total_vehicle_available\",\n",
    "    drop_staten_island: Optional[bool] = None,\n",
    "    tracts_to_remove: Optional[Sequence[str]] = None,\n",
    "    remove_tz_suffix: str = \" EDT\",\n",
    "    census_geocoder_benchmark: str = \"Public_AR_Census2020\",\n",
    "    census_geocoder_vintage: str = \"Census2020_Current\",\n",
    ") -> Dict[str, Any]:\n",
    "    \n",
    "    # --- 1. SETUP ---\n",
    "    city_key = city.strip().upper()\n",
    "    aliases = {\"PITTSBURGH\": \"PITT\", \"PIT\": \"PITT\", \"SANFRAN\": \"SF\", \"SAN FRANCISCO\": \"SF\", \"BAYWHEELS\": \"SF\"}\n",
    "    city_key = aliases.get(city_key, city_key)\n",
    "    \n",
    "    if city_key not in CITY_CONFIG:\n",
    "        raise ValueError(f\"City '{city_key}' not found in configuration.\")\n",
    "    \n",
    "    cfg = CITY_CONFIG[city_key]\n",
    "    out_dir = Path(output_dir or f\"./{city_key}_ALL_IN_ONE_OUTPUTS\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Override API Flag based on City Config\n",
    "    should_api_fill = cfg.get(\"use_api_fallback\", fill_missing_with_census_api)\n",
    "    \n",
    "    # Time Norm\n",
    "    ts_start = pd.to_datetime(time_start) if time_start else None\n",
    "    ts_end = pd.to_datetime(time_end) if time_end else None\n",
    "    \n",
    "    t_avail_s = pd.to_datetime(availability_time_start) or ts_start\n",
    "    t_avail_e = pd.to_datetime(availability_time_end) or ts_end\n",
    "    t_usage_s = pd.to_datetime(usage_time_start) or ts_start\n",
    "    t_usage_e = pd.to_datetime(usage_time_end) or ts_end\n",
    "    t_idle_s = pd.to_datetime(idle_time_start) or ts_start\n",
    "    t_idle_e = pd.to_datetime(idle_time_end) or ts_end\n",
    "\n",
    "    results = {\"city\": city_key, \"output_dir\": str(out_dir)}\n",
    "\n",
    "    # --- 2. ASSETS & STATIONS ---\n",
    "    print(f\"[{city_key}] Loading Spatial Assets...\")\n",
    "    blocks_id = str(cfg[\"geo\"][\"blocks_id\"])\n",
    "    blocks_gdf = gpd.read_file(cfg[\"assets\"][\"census_blocks\"]).to_crs(epsg=4326)\n",
    "    \n",
    "    print(f\"[{city_key}] Processing Station Data...\")\n",
    "    records = []\n",
    "    with Path(station_status_txt).open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip(): continue\n",
    "            try:\n",
    "                js = json.loads(line)\n",
    "                ts = list(js.keys())[0]\n",
    "                for entry in js[ts]:\n",
    "                    for vt in entry.get(\"vehicle_types_available\", []):\n",
    "                        entry[f\"vehicle_type_{vt['vehicle_type_id']}_count\"] = vt[\"count\"]\n",
    "                    entry.pop(\"vehicle_types_available\", None)\n",
    "                    entry[\"timestamp\"] = ts\n",
    "                    records.append(entry)\n",
    "            except: continue\n",
    "    \n",
    "    ss_df = pd.DataFrame(records)\n",
    "    ss_df[\"timestamp\"] = pd.to_datetime(ss_df[\"timestamp\"].astype(str).str.replace(remove_tz_suffix, \"\", regex=False), errors=\"coerce\")\n",
    "    ss_df[\"station_id\"] = DataHelper.normalize_id(ss_df[\"station_id\"])\n",
    "    if save_outputs: ss_df.to_csv(out_dir / f\"{city_key.lower()}_station_status_clean.csv\", index=False)\n",
    "\n",
    "    si = DataHelper.load_stations(Path(station_information_csv))\n",
    "    \n",
    "    # Block Assignment (Geocode if needed)\n",
    "    if \"census_block\" not in si.columns or si[\"census_block\"].isna().any():\n",
    "        mapping = GeoHelper.geocode_points(si, \"lon\", \"lat\", blocks_gdf, blocks_id)\n",
    "        si = si.merge(mapping, on=[\"lon\", \"lat\"], how=\"left\", suffixes=(\"\", \"_new\"))\n",
    "        if \"census_block_new\" in si.columns:\n",
    "            si[\"census_block\"] = si[\"census_block\"].fillna(si[\"census_block_new\"])\n",
    "            si = si.drop(columns=[\"census_block_new\"])\n",
    "        \n",
    "        if should_api_fill:\n",
    "            si = GeoHelper.api_fill_blocks(si, census_geocoder_benchmark, census_geocoder_vintage)\n",
    "\n",
    "    si[\"census_block\"] = si[\"census_block\"].astype(str).str.split('.').str[0]\n",
    "    si[\"census_tract\"] = DataHelper.normalize_tract(DataHelper.tract_from_block(si[\"census_block\"]))\n",
    "    \n",
    "    if save_outputs: si.to_csv(out_dir / f\"{city_key.lower()}_station_information_done.csv\", index=False)\n",
    "\n",
    "    # Station Status Merge\n",
    "    ss_done = ss_df.merge(si[[\"station_id\", \"census_block\", \"census_tract\"]], on=\"station_id\", how=\"left\")\n",
    "    for c in [\"num_bikes_available\", \"num_ebikes_available\", \"num_docks_available\"]:\n",
    "        ss_done[c] = pd.to_numeric(ss_done.get(c, 0), errors=\"coerce\").fillna(0).astype(int)\n",
    "    ss_done[\"total_vehicle_available\"] = ss_done[\"num_bikes_available\"] + ss_done[\"num_ebikes_available\"]\n",
    "    \n",
    "    if save_outputs: ss_done.to_csv(out_dir / f\"{city_key.lower()}_station_status_done.csv\", index=False)\n",
    "\n",
    "    # --- 3. METRICS ---\n",
    "    print(f\"[{city_key}] Computing Availability...\")\n",
    "    ss_filt = ss_done.copy()\n",
    "    if t_avail_s: ss_filt = ss_filt[ss_filt[\"timestamp\"] >= t_avail_s]\n",
    "    if t_avail_e: ss_filt = ss_filt[ss_filt[\"timestamp\"] < t_avail_e]\n",
    "    \n",
    "    ext = cfg[\"geo\"][\"external_tract_prefix\"]\n",
    "    avail_res = {}\n",
    "    if availability_group_level in {\"block\", \"both\"}:\n",
    "        avail_res[\"block\"] = MetricHelper.calc_availability(ss_filt, \"block\", availability_time_granularity, out_dir, save_outputs)\n",
    "    if availability_group_level in {\"tract\", \"both\"}:\n",
    "        avail_res[\"tract\"] = MetricHelper.calc_availability(ss_filt, \"tract\", availability_time_granularity, out_dir, save_outputs, ext)\n",
    "\n",
    "    print(f\"[{city_key}] Computing Capacity...\")\n",
    "    tract_gdf = gpd.read_file(cfg[\"assets\"][\"tracts\"]).to_crs(epsg=4326)\n",
    "    centroids = pd.DataFrame({\n",
    "        \"census_tract\": DataHelper.normalize_tract(tract_gdf[cfg[\"geo\"][\"tract_id\"]]),\n",
    "        \"centroid_lon\": tract_gdf.geometry.centroid.x,\n",
    "        \"centroid_lat\": tract_gdf.geometry.centroid.y\n",
    "    })\n",
    "    \n",
    "    if cfg[\"geo\"].get(\"drop_staten_island\", False) and \"COUNTYFP\" in tract_gdf.columns:\n",
    "        centroids = centroids[tract_gdf[\"COUNTYFP\"].astype(str) != \"085\"]\n",
    "    if save_outputs: centroids.to_csv(out_dir / \"centroid_tract_computed.csv\", index=False)\n",
    "\n",
    "    cap_tract_agg = si.groupby(\"census_tract\", as_index=False).agg(total_capacity=(\"capacity\", \"sum\"), num_station=(\"station_id\", \"count\"))\n",
    "    cap_tract = centroids[[\"census_tract\"]].merge(cap_tract_agg, on=\"census_tract\", how=\"left\").fillna(0)\n",
    "    if ext: cap_tract = cap_tract[~cap_tract[\"census_tract\"].str.startswith(ext)]\n",
    "    \n",
    "    cap_tract[\"total_capacity_norm\"] = MetricHelper.normalize(cap_tract[\"total_capacity\"]).round(5)\n",
    "    cap_tract[\"num_station_norm\"] = MetricHelper.normalize(cap_tract[\"num_station\"]).round(5)\n",
    "    \n",
    "    if save_outputs: cap_tract.to_csv(out_dir / \"capacity_tract_norm.csv\", index=False)\n",
    "\n",
    "    # Peak Capacity\n",
    "    tract_av = avail_res[\"tract\"][\"norm\"]\n",
    "    peak_ts = peak_time_slot if peak_time_slot else tract_av.groupby(\"time_slot\")[peak_metric].sum().idxmax()\n",
    "    peak_data = tract_av[tract_av[\"time_slot\"] == peak_ts][[\"census_tract\", \"total_vehicle_available\", \"num_docks_available\"]]\n",
    "    peak_data = peak_data.rename(columns={\"total_vehicle_available\": \"vehicle_capacity\", \"num_docks_available\": \"dock_capacity\"})\n",
    "    \n",
    "    cap_df = cap_tract.merge(peak_data, on=\"census_tract\", how=\"left\").fillna(0)\n",
    "    cap_df[\"vehicle_capacity_norm\"] = MetricHelper.normalize(cap_df[\"vehicle_capacity\"]).round(5)\n",
    "    cap_df[\"dock_capacity_norm\"] = MetricHelper.normalize(cap_df[\"dock_capacity\"]).round(5)\n",
    "    cap_df[\"occupancy_rate\"] = MetricHelper.safe_ratio(cap_df[\"vehicle_capacity\"], cap_df[\"total_capacity\"])\n",
    "    cap_df[\"return_pressure\"] = 1.0 - MetricHelper.safe_ratio(cap_df[\"dock_capacity\"], cap_df[\"total_capacity\"])\n",
    "    \n",
    "    if save_outputs: cap_df.to_csv(out_dir / \"capacity_tract_with_vehicle_and_docks_norm.csv\", index=False)\n",
    "\n",
    "    print(f\"[{city_key}] Computing Safety...\")\n",
    "    metric_crs = cfg[\"geo\"][\"metric_crs\"]\n",
    "    streets = GeoHelper.load_network_lines(Path(cfg[\"assets\"][\"centerline\"]), cfg[\"geo\"][\"safety_type\"], metric_crs, cfg[\"geo\"].get(\"wkt_candidates\", ()))\n",
    "    lanes = GeoHelper.load_network_lines(Path(cfg[\"assets\"][\"bike_lanes\"]), cfg[\"geo\"][\"safety_type\"], metric_crs, cfg[\"geo\"].get(\"wkt_candidates\", ()))\n",
    "    \n",
    "    rule = cfg[\"safety_rule\"]\n",
    "    p_col = DataHelper.pick_col(lanes, list(rule.get(\"col_candidates\", [])), False)\n",
    "    if p_col:\n",
    "        match_val = rule[\"match_value\"]\n",
    "        if rule[\"match_type\"] == \"contains\": mask = lanes[p_col].astype(str).str.upper().str.contains(str(match_val).upper(), na=False)\n",
    "        elif rule[\"match_type\"] == \"equals\": mask = lanes[p_col].astype(str).str.upper() == str(match_val).upper()\n",
    "        else: mask = lanes[p_col].astype(str).str.contains(match_val, case=False, regex=True)\n",
    "        protected = lanes[mask]\n",
    "    else: protected = lanes.iloc[0:0]\n",
    "\n",
    "    blocks_metric = blocks_gdf.to_crs(metric_crs)\n",
    "    def _len(lines, name):\n",
    "        if lines.empty: return pd.DataFrame({\"census_block\": [], name: []})\n",
    "        ov = gpd.overlay(lines, blocks_metric, how=\"intersection\", keep_geom_type=False)\n",
    "        ov[\"len\"] = ov.geometry.length\n",
    "        return ov.groupby(blocks_id)[\"len\"].sum().reset_index().rename(columns={blocks_id: \"census_block\", \"len\": name})\n",
    "\n",
    "    st = _len(streets, \"streets_leng\")\n",
    "    bl = _len(lanes, \"total_bike_lane_length\")\n",
    "    pr = _len(protected, \"protected_bike_lane_length\")\n",
    "    \n",
    "    safe = st.merge(bl, on=\"census_block\", how=\"left\").merge(pr, on=\"census_block\", how=\"left\").fillna(0)\n",
    "    safe[\"census_tract\"] = DataHelper.normalize_tract(DataHelper.tract_from_block(safe[\"census_block\"]))\n",
    "    safe[\"bike_lane_ratio\"] = MetricHelper.safe_ratio(safe[\"total_bike_lane_length\"], safe[\"streets_leng\"]).round(3)\n",
    "    safe[\"protected_bike_lane_ratio\"] = MetricHelper.safe_ratio(safe[\"protected_bike_lane_length\"], safe[\"streets_leng\"]).round(3)\n",
    "    \n",
    "    if save_outputs: safe.to_csv(out_dir / \"safety_bike_lane_block.csv\", index=False)\n",
    "\n",
    "    safe_tract = safe.groupby(\"census_tract\", as_index=False)[[\"streets_leng\", \"total_bike_lane_length\", \"protected_bike_lane_length\"]].sum()\n",
    "    meta = pd.read_csv(out_dir / \"centroid_tract_computed.csv\", dtype={\"census_tract\": str})\n",
    "    meta[\"census_tract\"] = DataHelper.normalize_tract(meta[\"census_tract\"])\n",
    "    \n",
    "    safe_tract = meta.merge(safe_tract, on=\"census_tract\", how=\"left\").fillna(0)\n",
    "    safe_tract[\"bike_lane_ratio\"] = MetricHelper.safe_ratio(safe_tract[\"total_bike_lane_length\"], safe_tract[\"streets_leng\"])\n",
    "    safe_tract[\"protected_bike_lane_ratio\"] = MetricHelper.safe_ratio(safe_tract[\"protected_bike_lane_length\"], safe_tract[\"streets_leng\"])\n",
    "    \n",
    "    safe_service = safe_tract[safe_tract[\"census_tract\"].isin(set(cap_tract[\"census_tract\"].unique()))].copy()\n",
    "    safe_norm = safe_service.copy()\n",
    "    safe_norm[\"bike_lane_ratio_norm\"] = MetricHelper.normalize(safe_norm[\"bike_lane_ratio\"]).round(5)\n",
    "    safe_norm[\"protected_bike_lane_ratio_norm\"] = MetricHelper.normalize(safe_norm[\"protected_bike_lane_ratio\"]).round(5)\n",
    "    \n",
    "    if save_outputs:\n",
    "        safe_tract.to_csv(out_dir / \"safety_bike_lane_tract.csv\", index=False)\n",
    "        safe_service.to_csv(out_dir / \"safety_bike_lane_service_area.csv\", index=False)\n",
    "        safe_norm.to_csv(out_dir / \"safety_bike_lane_norm_tract.csv\", index=False)\n",
    "\n",
    "    print(f\"[{city_key}] Computing Usage...\")\n",
    "    trips = DataHelper.load_trips(trip_csv) # Supports dir/zip aggregation\n",
    "    trips[\"started_at\"] = pd.to_datetime(trips[\"started_at\"], errors=\"coerce\")\n",
    "    trips[\"ended_at\"] = pd.to_datetime(trips[\"ended_at\"], errors=\"coerce\")\n",
    "    \n",
    "    t_min, t_max = trips[\"started_at\"].min(), trips[\"started_at\"].max()\n",
    "    t_win_s, t_win_e = (t_usage_s if t_usage_s else t_min), (t_usage_e if t_usage_e else t_max)\n",
    "    if t_win_s > t_max or t_win_e < t_min:\n",
    "        print(f\"âš ï¸ WARNING: Request Window ({t_win_s}-{t_win_e}) does not overlap Trip Data ({t_min}-{t_max}).\")\n",
    "    \n",
    "    trips = trips[(trips[\"started_at\"] >= t_win_s) & (trips[\"started_at\"] <= t_win_e)].copy()\n",
    "    \n",
    "    # Station Mapping (Primary)\n",
    "    sm = si[[\"station_id\", \"census_block\"]].copy()\n",
    "    if \"start_station_id\" in trips.columns:\n",
    "        if \"start_census_block\" in trips.columns: trips.drop(columns=[\"start_census_block\"], inplace=True)\n",
    "        trips = trips.merge(sm, left_on=\"start_station_id\", right_on=\"station_id\", how=\"left\").rename(columns={\"census_block\": \"start_census_block\"}).drop(columns=[\"station_id\"])\n",
    "    if \"end_station_id\" in trips.columns:\n",
    "        if \"end_census_block\" in trips.columns: trips.drop(columns=[\"end_census_block\"], inplace=True)\n",
    "        trips = trips.merge(sm, left_on=\"end_station_id\", right_on=\"station_id\", how=\"left\").rename(columns={\"census_block\": \"end_census_block\"}).drop(columns=[\"station_id\"])\n",
    "\n",
    "    # Fallback Geocoding\n",
    "    if \"start_census_block\" not in trips.columns or trips[\"start_census_block\"].isna().mean() > 0.5:\n",
    "        print(\"   -> Fallback: Geocoding Trips...\")\n",
    "        s_map = GeoHelper.geocode_points(trips, \"start_lng\", \"start_lat\", blocks_gdf, blocks_id)\n",
    "        if \"start_census_block\" in trips.columns: trips.drop(columns=[\"start_census_block\"], inplace=True)\n",
    "        trips = trips.merge(s_map, left_on=[\"start_lng\", \"start_lat\"], right_on=[\"lon\", \"lat\"], how=\"left\").rename(columns={\"census_block\": \"start_census_block\"}).drop(columns=[\"lon\", \"lat\"])\n",
    "        \n",
    "        e_map = GeoHelper.geocode_points(trips, \"end_lng\", \"end_lat\", blocks_gdf, blocks_id)\n",
    "        if \"end_census_block\" in trips.columns: trips.drop(columns=[\"end_census_block\"], inplace=True)\n",
    "        trips = trips.merge(e_map, left_on=[\"end_lng\", \"end_lat\"], right_on=[\"lon\", \"lat\"], how=\"left\").rename(columns={\"census_block\": \"end_census_block\"}).drop(columns=[\"lon\", \"lat\"])\n",
    "\n",
    "    trips[\"start_census_block\"] = trips[\"start_census_block\"].astype(str).str.split('.').str[0]\n",
    "    trips[\"end_census_block\"] = trips[\"end_census_block\"].astype(str).str.split('.').str[0]\n",
    "    \n",
    "    # Calculate Slots & Grid\n",
    "    all_blocks = pd.concat([trips[\"start_census_block\"], trips[\"end_census_block\"]]).unique()\n",
    "    all_blocks = [x for x in all_blocks if x != \"nan\"]\n",
    "    \n",
    "    trips[\"slot_start\"] = trips[\"started_at\"].dt.floor(\"5min\")\n",
    "    trips[\"slot_end\"] = trips[\"ended_at\"].dt.floor(\"5min\")\n",
    "    \n",
    "    MetricHelper.calc_usage(trips, (t_win_s, t_win_e), all_blocks, \"5min\", \"usage_5min_block\", out_dir, save_outputs)\n",
    "    u_hr = MetricHelper.calc_usage(trips, (t_win_s, t_win_e), all_blocks, \"1h\", \"usage_hourly_block\", out_dir, save_outputs)\n",
    "    \n",
    "    u_hr[\"census_tract\"] = DataHelper.normalize_tract(DataHelper.tract_from_block(u_hr[\"census_block\"]))\n",
    "    if ext: u_hr = u_hr[~u_hr[\"census_tract\"].str.startswith(ext)]\n",
    "    \n",
    "    u_tract = u_hr.groupby([\"census_tract\", \"time_slot\"], as_index=False)[[\"trips_starting\", \"trips_ending\"]].sum()\n",
    "    if tracts_to_remove: u_tract = u_tract[~u_tract[\"census_tract\"].isin(tracts_to_remove)]\n",
    "    \n",
    "    u_tract[\"trips_starting_norm\"] = MetricHelper.normalize(u_tract[\"trips_starting\"]).round(5)\n",
    "    u_tract[\"trips_ending_norm\"] = MetricHelper.normalize(u_tract[\"trips_ending\"]).round(5)\n",
    "    \n",
    "    if save_outputs:\n",
    "        u_tract.to_csv(out_dir / \"usage_hourly_tract_raw.csv\", index=False)\n",
    "        u_tract.to_csv(out_dir / \"usage_norm_hourly_tract.csv\", index=False)\n",
    "\n",
    "    if compute_idle_time:\n",
    "        print(f\"[{city_key}] Computing Idle Time...\")\n",
    "        t_i_s, t_i_e = (t_idle_s if t_idle_s else t_win_s), (t_idle_e if t_idle_e else t_win_e)\n",
    "        idle_trips = trips[(trips[\"started_at\"] >= t_i_s) & (trips[\"started_at\"] <= t_i_e)].copy()\n",
    "        if \"rideable_type\" in idle_trips.columns:\n",
    "            idle_trips = MetricHelper.map_rideable_type(idle_trips)\n",
    "            has_rideable = True\n",
    "        else: has_rideable = False\n",
    "\n",
    "        ss_idle = ss_done[(ss_done[\"timestamp\"] >= t_i_s) & (ss_done[\"timestamp\"] <= t_i_e)].copy()\n",
    "        ss_idle[\"time_slot\"] = ss_idle[\"timestamp\"].dt.floor(\"5min\") - pd.Timedelta(minutes=5)\n",
    "        \n",
    "        inv = ss_idle.groupby([\"census_block\", \"time_slot\"], as_index=False)[[\"total_vehicle_available\"]].sum()\n",
    "        flux = MetricHelper.calc_usage(idle_trips, (t_i_s, t_i_e), all_blocks, \"5min\", \"temp_flux\", out_dir, False)\n",
    "        flux[\"vehicles_moved\"] = flux[\"trips_starting\"] + flux[\"trips_ending\"]\n",
    "        \n",
    "        if has_rideable:\n",
    "            ts = idle_trips.groupby([\"start_census_block\", \"slot_start\", \"_is_ebike\"]).size().unstack(fill_value=0).reset_index()\n",
    "            ts = ts.rename(columns={\"start_census_block\": \"census_block\", \"slot_start\": \"time_slot\", False: \"trips_starting_bike\", True: \"trips_starting_ebike\"})\n",
    "            te = idle_trips.groupby([\"end_census_block\", \"slot_end\", \"_is_ebike\"]).size().unstack(fill_value=0).reset_index()\n",
    "            te = te.rename(columns={\"end_census_block\": \"census_block\", \"slot_end\": \"time_slot\", False: \"trips_ending_bike\", True: \"trips_ending_ebike\"})\n",
    "            flux = flux.merge(ts, on=[\"census_block\", \"time_slot\"], how=\"left\").merge(te, on=[\"census_block\", \"time_slot\"], how=\"left\").fillna(0)\n",
    "            for c in [\"trips_starting_bike\", \"trips_starting_ebike\", \"trips_ending_bike\", \"trips_ending_ebike\"]:\n",
    "                if c in flux.columns: flux[c] = flux[c].astype(int)\n",
    "            flux[\"vehicles_moved_bike\"] = flux.get(\"trips_starting_bike\",0) + flux.get(\"trips_ending_bike\",0)\n",
    "            flux[\"vehicles_moved_ebike\"] = flux.get(\"trips_starting_ebike\",0) + flux.get(\"trips_ending_ebike\",0)\n",
    "\n",
    "        m = inv.merge(flux, on=[\"census_block\", \"time_slot\"], how=\"left\").fillna(0)\n",
    "        m[\"idle_vehicles\"] = (m[\"total_vehicle_available\"] - m[\"vehicles_moved\"]).clip(lower=0)\n",
    "        m[\"hour\"] = m[\"time_slot\"].dt.floor(\"1h\")\n",
    "        if save_outputs: m.to_csv(out_dir / \"idle_merged_5min.csv\", index=False)\n",
    "        \n",
    "        idle_res = []\n",
    "        for block, b_df in m.groupby(\"census_block\"):\n",
    "            for hour, h_df in b_df.groupby(\"hour\"):\n",
    "                h_df = h_df.sort_values(\"time_slot\")\n",
    "                pool, durations = deque(), []\n",
    "                for _, row in h_df.iterrows():\n",
    "                    curr_t, cnt = row[\"time_slot\"], int(row[\"idle_vehicles\"])\n",
    "                    while cnt > len(pool): pool.append(curr_t)\n",
    "                    while cnt < len(pool):\n",
    "                        start_t = pool.popleft()\n",
    "                        durations.append((curr_t - start_t).total_seconds() / 60.0)\n",
    "                final_t = hour + pd.Timedelta(hours=1)\n",
    "                while pool:\n",
    "                    start_t = pool.popleft()\n",
    "                    durations.append((final_t - start_t).total_seconds() / 60.0)\n",
    "                avg = round(sum(durations) / len(durations), 2) if durations else 0.0\n",
    "                idle_res.append({\"census_block\": block, \"hour\": hour, \"avg_idle_time\": avg})\n",
    "        \n",
    "        idle_df = pd.DataFrame(idle_res)\n",
    "        if save_outputs: idle_df.to_csv(out_dir / \"idle_time_block.csv\", index=False)\n",
    "        \n",
    "        if not idle_df.empty:\n",
    "            idle_df[\"census_tract\"] = DataHelper.normalize_tract(DataHelper.tract_from_block(idle_df[\"census_block\"].astype(str)))\n",
    "            if ext: idle_df = idle_df[~idle_df[\"census_tract\"].str.startswith(ext)]\n",
    "            i_tract = idle_df.groupby([\"census_tract\", \"hour\"], as_index=False)[\"avg_idle_time\"].mean()\n",
    "            i_tract[\"avg_idle_time_norm\"] = MetricHelper.normalize(i_tract[\"avg_idle_time\"]).round(5)\n",
    "            if save_outputs: i_tract.to_csv(out_dir / \"idle_time_norm_tract.csv\", index=False)\n",
    "\n",
    "    print(f\"--- Done. Outputs saved to {out_dir} ---\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2bc4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NYC] Loading Spatial Assets...\n",
      "[NYC] Processing Station Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API Geocoding Fallback: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [01:07<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NYC] Computing Availability...\n",
      "[NYC] Computing Capacity...\n",
      "[NYC] Computing Safety...\n",
      "[NYC] Computing Usage...\n",
      "   -> Reading trip data from: D:\\Research Fellowship\\Summer Research Stuff\\202506-citibike-tripdata\n",
      "   -> Found 5 trip files. Aggregating...\n",
      "   -> Fallback: Geocoding Trips...\n",
      "[NYC] Computing Idle Time...\n",
      "--- Done. Outputs saved to NYC_Docked_Output_v2 ---\n"
     ]
    }
   ],
   "source": [
    "# Example Call for San Francisco Docked\n",
    "out_docked = run_docked_all_utilities_single_function(\n",
    "    city=\"NYC\", # NYC / NJ / PITT / SF\n",
    "    \n",
    "    # Inputs\n",
    "    station_status_txt=r\"D:\\Research Fellowship\\Summer Research Stuff\\Collected Data\\Week 1\\09-June\\nyc_docked_station_status_6_9.txt\",\n",
    "    station_information_csv=r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\GBFS_Census_Tract\\NYC\\NYC station information 06_09.csv\",\n",
    "    trip_csv=r\"D:\\Research Fellowship\\Summer Research Stuff\\202506-citibike-tripdata\", # Use a Folder if there are multiple files or use a single file if there is one.\n",
    "    \n",
    "    # Output Location\n",
    "    output_dir=\"NYC_Docked_Output_v2\",\n",
    "    \n",
    "    # Global Time Window (Propagates to all utilities)\n",
    "    time_start=\"2025-06-09 00:00:00\",\n",
    "    time_end=\"2025-06-09 23:00:00\",\n",
    "    \n",
    "    # Optional: Disable idle simulation if it takes too long\n",
    "    compute_idle_time=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c64fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
