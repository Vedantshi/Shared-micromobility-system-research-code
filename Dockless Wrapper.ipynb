{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d45178",
   "metadata": {},
   "source": [
    "# **Dockless Wrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc88592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, Dict, Any, List, Tuple\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def run_sf_dockless_all_utilities_single_function(\n",
    "    *,\n",
    "    # ------------------------------------------------------------\n",
    "    # USER INPUTS\n",
    "    # ------------------------------------------------------------\n",
    "    system_key: str,  # \"SF_LIME_DOCKLESS\" or \"SF_SPIN_DOCKLESS\"\n",
    "    freebike_status_txt: Union[str, Path],\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # CITY ASSETS (same for both SF dockless systems unless overridden)\n",
    "    # ------------------------------------------------------------\n",
    "    census_blocks_shp: Union[str, Path] = r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\GBFS_Census_Tract\\San_Fran_Lime\\tl_2024_06_tabblock20.shp\",\n",
    "    centerline_streets_csv: Union[str, Path] = r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\San_Fran_Lime\\Centerline.csv\",\n",
    "    bike_lanes_csv: Union[str, Path] = r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\San_Fran_Lime\\Bikelane.csv\",\n",
    "    centroid_tract_csv: Union[str, Path] = r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\San_Fran_Baywheels\\centroid_tract_ca.csv\",\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # OUTPUT\n",
    "    # ------------------------------------------------------------\n",
    "    output_dir: Optional[Union[str, Path]] = None,  # if None -> default per system\n",
    "    save_outputs: bool = True,\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # TIME WINDOW (applies to availability/usage/idle)\n",
    "    # ------------------------------------------------------------\n",
    "    time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # STEP 0 SETTINGS\n",
    "    # ------------------------------------------------------------\n",
    "    step0_raw_csv_name: Optional[str] = None,   # if None -> default per system\n",
    "    step0_done_csv_name: Optional[str] = None,  # if None -> default per system\n",
    "    blocks_geoid_col: str = \"GEOID20\",\n",
    "    blocks_target_epsg: int = 4326,\n",
    "    fill_missing_with_census_api: bool = True,\n",
    "    census_benchmark: str = \"Public_AR_Census2020\",\n",
    "    census_vintage: str = \"2020\",\n",
    "    timestamp_parse_mode: str = \"auto\",  # \"auto\" or \"epoch\"\n",
    "    drop_cols_if_present: Optional[List[str]] = None,\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # AVAILABILITY SETTINGS\n",
    "    # ------------------------------------------------------------\n",
    "    availability_block_time_granularity: str = \"5min\",\n",
    "    availability_tract_time_granularity: str = \"1H\",\n",
    "    reserved_col: str = \"is_reserved\",\n",
    "    disabled_col: str = \"is_disabled\",\n",
    "    vehicle_id_col: str = \"bike_id\",\n",
    "    include_vehicle_type: bool = True,\n",
    "    vehicle_type_col_avail: str = \"vehicle_type_id\",\n",
    "    tract_digits: int = 11,\n",
    "    availability_normalize: bool = True,\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # USAGE SETTINGS\n",
    "    # ------------------------------------------------------------\n",
    "    usage_base_time_slot: str = \"5min\",\n",
    "    usage_aggregate_time_slot: str = \"1H\",\n",
    "    usage_rounding_decimals: int = 4,\n",
    "    usage_normalize: bool = True,\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # IDLE TIME SETTINGS\n",
    "    # ------------------------------------------------------------\n",
    "    idle_base_time_slot: str = \"5min\",\n",
    "    idle_hour_bucket_freq: str = \"1H\",\n",
    "    idle_rounding_decimals: int = 4,\n",
    "    idle_vehicle_id_col: str = \"bike_id\",\n",
    "    idle_lat_col: str = \"lat\",\n",
    "    idle_lon_col: str = \"lon\",\n",
    "    idle_census_block_col: str = \"census_block\",\n",
    "    idle_vehicle_type_col: str = \"vehicle_type\",\n",
    "    idle_fill_full_grid: bool = True,\n",
    "    idle_default_vehicle_type_for_missing: str = \"scooter\",\n",
    "    idle_normalize: bool = True,\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # SAFETY SETTINGS (generic)\n",
    "    # ------------------------------------------------------------\n",
    "    safety_centerline_wkt_col: str = \"line\",\n",
    "    safety_bike_lane_wkt_col: str = \"shape\",\n",
    "    safety_input_crs: str = \"EPSG:4326\",\n",
    "    safety_census_block_id_col: str = \"GEOID20\",\n",
    "    safety_centroid_tract_id_col: str = \"census_tract\",\n",
    "    safety_centroid_area_col: Optional[str] = \"county_name\",\n",
    "    safety_working_epsg: Optional[int] = 26910,  # UTM zone for SF-ish\n",
    "    safety_bike_lane_class_col: Optional[str] = None,\n",
    "    safety_protected_values: Optional[List[str]] = None,\n",
    "    safety_protected_keywords: Optional[List[str]] = None,\n",
    "    safety_class_col_candidate_keywords: Optional[List[str]] = None,\n",
    "    safety_protected_match_mode: str = \"contains\",\n",
    "    safety_normalize: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    ONE function to run ALL SF dockless utilities for multiple systems (Lime, Spin):\n",
    "      Step0 (txt->done_df with census block/tract)\n",
    "      Availability (block+tract)\n",
    "      Usage (5min movement -> hourly tract)\n",
    "      Idle time (segment-based idle durations)\n",
    "      Safety (centerline/bikelane intersection + protected inference)\n",
    "\n",
    "    Use:\n",
    "      system_key=\"SF_LIME_DOCKLESS\" or \"SF_SPIN_DOCKLESS\"\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================\n",
    "    # System presets (only differences: labels + default filenames + output tags)\n",
    "    # ============================================================\n",
    "    SYSTEMS: Dict[str, Dict[str, str]] = {\n",
    "        \"SF_LIME_DOCKLESS\": {\n",
    "            \"city\": \"San Francisco\",\n",
    "            \"vendor\": \"Lime\",\n",
    "            \"system_type\": \"dockless\",\n",
    "            \"tag\": \"sf_lime\",\n",
    "            \"default_output_dir\": \"SF_LIME_DOCKLESS_FULL_RUN\",\n",
    "            \"raw_csv\": \"san_fran_lime_status_raw.csv\",\n",
    "            \"done_csv\": \"san_fran_lime_status_done.csv\",\n",
    "        },\n",
    "        \"SF_SPIN_DOCKLESS\": {\n",
    "            \"city\": \"San Francisco\",\n",
    "            \"vendor\": \"Spin\",\n",
    "            \"system_type\": \"dockless\",\n",
    "            \"tag\": \"sf_spin\",\n",
    "            \"default_output_dir\": \"SF_SPIN_DOCKLESS_FULL_RUN\",\n",
    "            \"raw_csv\": \"san_fran_spin_status_raw.csv\",\n",
    "            \"done_csv\": \"san_fran_spin_status_done.csv\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if system_key not in SYSTEMS:\n",
    "        raise ValueError(f\"Unknown system_key='{system_key}'. Choose from: {sorted(SYSTEMS.keys())}\")\n",
    "\n",
    "    preset = SYSTEMS[system_key]\n",
    "    output_tag = preset[\"tag\"]\n",
    "\n",
    "    if output_dir is None:\n",
    "        output_dir = preset[\"default_output_dir\"]\n",
    "\n",
    "    if step0_raw_csv_name is None:\n",
    "        step0_raw_csv_name = preset[\"raw_csv\"]\n",
    "    if step0_done_csv_name is None:\n",
    "        step0_done_csv_name = preset[\"done_csv\"]\n",
    "\n",
    "    # ============================================================\n",
    "    # Setup output dir + paths\n",
    "    # ============================================================\n",
    "    out_dir = Path(output_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    freebike_status_txt = Path(freebike_status_txt)\n",
    "    census_blocks_shp = Path(census_blocks_shp)\n",
    "    centerline_streets_csv = Path(centerline_streets_csv)\n",
    "    bike_lanes_csv = Path(bike_lanes_csv)\n",
    "    centroid_tract_csv = Path(centroid_tract_csv)\n",
    "\n",
    "    if not freebike_status_txt.exists():\n",
    "        raise FileNotFoundError(f\"freebike_status_txt not found: {freebike_status_txt}\")\n",
    "    for label, p in [\n",
    "        (\"census_blocks_shp\", census_blocks_shp),\n",
    "        (\"centerline_streets_csv\", centerline_streets_csv),\n",
    "        (\"bike_lanes_csv\", bike_lanes_csv),\n",
    "        (\"centroid_tract_csv\", centroid_tract_csv),\n",
    "    ]:\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"{label} not found: {p}\")\n",
    "\n",
    "    raw_path = out_dir / step0_raw_csv_name\n",
    "    done_path = out_dir / step0_done_csv_name\n",
    "\n",
    "    ts_start = pd.to_datetime(time_start) if time_start is not None else None\n",
    "    ts_end = pd.to_datetime(time_end) if time_end is not None else None\n",
    "\n",
    "    results: Dict[str, Any] = {\n",
    "        \"system_key\": system_key,\n",
    "        \"city\": preset[\"city\"],\n",
    "        \"vendor\": preset[\"vendor\"],\n",
    "        \"system_type\": preset[\"system_type\"],\n",
    "        \"output_dir\": str(out_dir),\n",
    "    }\n",
    "\n",
    "    # ============================================================\n",
    "    # Helpers\n",
    "    # ============================================================\n",
    "    def _minmax(series: pd.Series) -> pd.Series:\n",
    "        s = pd.to_numeric(series, errors=\"coerce\")\n",
    "        mn = s.min(skipna=True)\n",
    "        mx = s.max(skipna=True)\n",
    "        if pd.isna(mn) or pd.isna(mx) or mx <= mn:\n",
    "            return pd.Series(0.0, index=s.index)\n",
    "        return (s - mn) / (mx - mn)\n",
    "\n",
    "    # ============================================================\n",
    "    # STEP 0 (INLINE): txt -> raw_df -> spatial join -> api fill -> done_df\n",
    "    # ============================================================\n",
    "    with freebike_status_txt.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    rows: List[dict] = []\n",
    "    for line in lines:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        try:\n",
    "            blob = json.loads(line)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"⚠️ Error decoding JSON: {e}. Skipping line. First 100 chars: {line[:100]}...\")\n",
    "            continue\n",
    "\n",
    "        timestamp = list(blob.keys())[0]\n",
    "        entries = blob[timestamp]\n",
    "        for entry in entries:\n",
    "            if isinstance(entry, dict) and \"vehicle_types_available\" in entry:\n",
    "                vta = entry.get(\"vehicle_types_available\", [])\n",
    "                if isinstance(vta, list):\n",
    "                    for vt in vta:\n",
    "                        try:\n",
    "                            vid = vt[\"vehicle_type_id\"]\n",
    "                            cnt = vt[\"count\"]\n",
    "                            entry[f\"vehicle_type_{vid}_count\"] = cnt\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                try:\n",
    "                    del entry[\"vehicle_types_available\"]\n",
    "                except Exception:\n",
    "                    pass\n",
    "            entry[\"timestamp\"] = timestamp\n",
    "            rows.append(entry)\n",
    "\n",
    "    raw_df = pd.DataFrame(rows)\n",
    "\n",
    "    if \"timestamp\" in raw_df.columns:\n",
    "        if timestamp_parse_mode == \"epoch\":\n",
    "            raw_df[\"timestamp\"] = pd.to_datetime(raw_df[\"timestamp\"], unit=\"s\", errors=\"coerce\")\n",
    "        else:\n",
    "            raw_df[\"timestamp\"] = pd.to_datetime(raw_df[\"timestamp\"], errors=\"coerce\")\n",
    "            if raw_df[\"timestamp\"].isna().mean() > 0.5:\n",
    "                raw_df[\"timestamp\"] = pd.to_datetime(raw_df[\"timestamp\"], unit=\"s\", errors=\"coerce\")\n",
    "\n",
    "    if drop_cols_if_present:\n",
    "        drop_now = [c for c in drop_cols_if_present if c in raw_df.columns]\n",
    "        if drop_now:\n",
    "            raw_df = raw_df.drop(columns=drop_now)\n",
    "\n",
    "    if save_outputs:\n",
    "        raw_df.to_csv(raw_path, index=False)\n",
    "\n",
    "    if not {\"lat\", \"lon\"}.issubset(raw_df.columns):\n",
    "        raise ValueError(\n",
    "            \"Dockless input must include 'lat' and 'lon' columns to assign census blocks. \"\n",
    "            \"If Spin uses different names, pass overrides via idle_lat_col/idle_lon_col AND rename upstream.\"\n",
    "        )\n",
    "\n",
    "    latlon_df = raw_df.drop_duplicates(subset=[\"lat\", \"lon\"])[[\"lat\", \"lon\"]].copy()\n",
    "\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Point\n",
    "\n",
    "    geometry = [Point(xy) for xy in zip(latlon_df[\"lon\"], latlon_df[\"lat\"])]\n",
    "    points_gdf = gpd.GeoDataFrame(latlon_df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "    blocks_gdf = gpd.read_file(str(census_blocks_shp)).to_crs(epsg=blocks_target_epsg)\n",
    "    joined = gpd.sjoin(points_gdf, blocks_gdf, how=\"left\", predicate=\"within\")\n",
    "\n",
    "    if blocks_geoid_col not in joined.columns:\n",
    "        raise ValueError(\n",
    "            f\"Expected block GEOID column '{blocks_geoid_col}' not found after join. \"\n",
    "            f\"Available columns: {list(joined.columns)}\"\n",
    "        )\n",
    "\n",
    "    joined = joined.rename(columns={blocks_geoid_col: \"census_block\"})\n",
    "    joined[\"census_block\"] = joined[\"census_block\"].astype(str)\n",
    "    joined[\"census_tract\"] = joined[\"census_block\"].str[:-4]\n",
    "\n",
    "    latlon_blocks_df = joined[[\"lat\", \"lon\", \"census_block\", \"census_tract\"]].copy()\n",
    "    done_df = raw_df.merge(latlon_blocks_df, on=[\"lat\", \"lon\"], how=\"left\")\n",
    "\n",
    "    if fill_missing_with_census_api:\n",
    "        import requests\n",
    "\n",
    "        missing_points = done_df[done_df[\"census_block\"].isna()][[\"lat\", \"lon\"]].drop_duplicates().copy()\n",
    "        if not missing_points.empty:\n",
    "            try:\n",
    "                from tqdm import tqdm\n",
    "                tqdm.pandas()\n",
    "                use_progress = True\n",
    "            except Exception:\n",
    "                use_progress = False\n",
    "\n",
    "            def _get_block(lat: float, lon: float) -> Optional[str]:\n",
    "                try:\n",
    "                    url = (\n",
    "                        \"https://geocoding.geo.census.gov/geocoder/geographies/coordinates\"\n",
    "                        f\"?x={lon}&y={lat}\"\n",
    "                        f\"&benchmark={census_benchmark}\"\n",
    "                        f\"&vintage={census_vintage}\"\n",
    "                        \"&format=json\"\n",
    "                    )\n",
    "                    r = requests.get(url, timeout=30)\n",
    "                    r.raise_for_status()\n",
    "                    js = r.json()\n",
    "                    geos = js.get(\"result\", {}).get(\"geographies\", {})\n",
    "                    blocks = geos.get(\"Census Blocks\", [])\n",
    "                    if blocks:\n",
    "                        return blocks[0].get(\"GEOID\")\n",
    "                    blocks2020 = geos.get(\"2020 Census Blocks\", [])\n",
    "                    if blocks2020:\n",
    "                        return blocks2020[0].get(\"GEOID\")\n",
    "                    return None\n",
    "                except Exception:\n",
    "                    return None\n",
    "\n",
    "            if use_progress:\n",
    "                missing_points[\"census_block_new\"] = missing_points.progress_apply(\n",
    "                    lambda r: _get_block(r[\"lat\"], r[\"lon\"]),\n",
    "                    axis=1,\n",
    "                )\n",
    "            else:\n",
    "                missing_points[\"census_block_new\"] = missing_points.apply(\n",
    "                    lambda r: _get_block(r[\"lat\"], r[\"lon\"]),\n",
    "                    axis=1,\n",
    "                )\n",
    "\n",
    "            missing_points[\"census_tract_new\"] = missing_points[\"census_block_new\"].apply(\n",
    "                lambda x: str(x)[:-4] if pd.notna(x) and x not in {\"None\", \"nan\"} else None\n",
    "            )\n",
    "\n",
    "            done_df = done_df.merge(\n",
    "                missing_points[[\"lat\", \"lon\", \"census_block_new\", \"census_tract_new\"]],\n",
    "                on=[\"lat\", \"lon\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "            done_df[\"census_block\"] = done_df[\"census_block\"].fillna(done_df[\"census_block_new\"])\n",
    "            done_df[\"census_tract\"] = done_df[\"census_tract\"].fillna(done_df[\"census_tract_new\"])\n",
    "            done_df = done_df.drop(columns=[\"census_block_new\", \"census_tract_new\"])\n",
    "\n",
    "    if save_outputs:\n",
    "        done_df.to_csv(done_path, index=False)\n",
    "\n",
    "    results[\"step0\"] = {\n",
    "        \"raw_df\": raw_df,\n",
    "        \"done_df\": done_df,\n",
    "        \"latlon_blocks_df\": latlon_blocks_df,\n",
    "        \"paths\": {\"raw_csv\": str(raw_path), \"done_csv\": str(done_path)},\n",
    "        \"meta\": {\n",
    "            \"raw_rows\": len(raw_df),\n",
    "            \"done_rows\": len(done_df),\n",
    "            \"unique_points\": len(latlon_df),\n",
    "            \"missing_after_sjoin\": int(done_df[\"census_block\"].isna().sum()),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # ============================================================\n",
    "    # Apply global time filter once (for downstream)\n",
    "    # ============================================================\n",
    "    df = done_df.copy()\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "    if df[\"timestamp\"].isna().any():\n",
    "        raise ValueError(f\"Step0 produced {int(df['timestamp'].isna().sum())} rows with unparseable timestamps.\")\n",
    "\n",
    "    data_min = df[\"timestamp\"].min()\n",
    "    data_max = df[\"timestamp\"].max()\n",
    "\n",
    "    if ts_start is not None:\n",
    "        df = df[df[\"timestamp\"] >= ts_start].copy()\n",
    "    if ts_end is not None:\n",
    "        df = df[df[\"timestamp\"] < ts_end].copy()\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\n",
    "            \"No rows after applying time window.\\n\"\n",
    "            f\"Requested window: [{ts_start}, {ts_end})\\n\"\n",
    "            f\"Data timestamp range: [{data_min}, {data_max}]\"\n",
    "        )\n",
    "\n",
    "    df[\"census_block\"] = df[\"census_block\"].astype(str)\n",
    "    if \"census_tract\" not in df.columns:\n",
    "        df[\"census_tract\"] = df[\"census_block\"].astype(str).str[:tract_digits]\n",
    "    else:\n",
    "        df[\"census_tract\"] = df[\"census_tract\"].astype(str)\n",
    "\n",
    "    results[\"meta\"] = {\"time_start\": ts_start, \"time_end\": ts_end, \"data_min\": data_min, \"data_max\": data_max}\n",
    "\n",
    "    # ============================================================\n",
    "    # AVAILABILITY (INLINE)\n",
    "    # ============================================================\n",
    "    df_av = df.sort_values([\"timestamp\", \"census_block\"]).reset_index(drop=True)\n",
    "    df_av[\"time_slot\"] = df_av[\"timestamp\"].dt.floor(availability_block_time_granularity)\n",
    "\n",
    "    if reserved_col not in df_av.columns or disabled_col not in df_av.columns:\n",
    "        raise ValueError(\n",
    "            f\"Missing '{reserved_col}' and/or '{disabled_col}' for dockless availability. \"\n",
    "            \"If Spin uses different names, pass reserved_col/disabled_col overrides.\"\n",
    "        )\n",
    "\n",
    "    available_df = df_av[(df_av[reserved_col] == 0) & (df_av[disabled_col] == 0)].copy()\n",
    "    if vehicle_id_col not in available_df.columns:\n",
    "        raise ValueError(f\"Missing vehicle id column '{vehicle_id_col}' for availability (override vehicle_id_col).\")\n",
    "\n",
    "    all_blocks = df_av[\"census_block\"].drop_duplicates().to_numpy()\n",
    "    all_time_slots = pd.date_range(\n",
    "        df_av[\"time_slot\"].min(),\n",
    "        df_av[\"time_slot\"].max(),\n",
    "        freq=availability_block_time_granularity,\n",
    "    )\n",
    "\n",
    "    full_index = pd.MultiIndex.from_product(\n",
    "        [all_blocks, all_time_slots],\n",
    "        names=[\"census_block\", \"time_slot\"],\n",
    "    ).to_frame(index=False)\n",
    "\n",
    "    base_availability = (\n",
    "        available_df.groupby([\"census_block\", \"time_slot\"])[vehicle_id_col]\n",
    "        .count()\n",
    "        .reset_index()\n",
    "        .rename(columns={vehicle_id_col: \"total_available\"})\n",
    "    )\n",
    "\n",
    "    availability_block = full_index.merge(base_availability, on=[\"census_block\", \"time_slot\"], how=\"left\")\n",
    "    availability_block[\"total_available\"] = availability_block[\"total_available\"].fillna(0).astype(int)\n",
    "\n",
    "    if include_vehicle_type:\n",
    "        if vehicle_type_col_avail not in available_df.columns:\n",
    "            raise ValueError(f\"include_vehicle_type=True but '{vehicle_type_col_avail}' not found (override).\")\n",
    "        type_pivot = available_df.pivot_table(\n",
    "            index=[\"census_block\", \"time_slot\"],\n",
    "            columns=vehicle_type_col_avail,\n",
    "            values=vehicle_id_col,\n",
    "            aggfunc=\"count\",\n",
    "            fill_value=0,\n",
    "        ).reset_index()\n",
    "        type_pivot.columns.name = None\n",
    "        type_pivot = type_pivot.rename(columns=lambda x: f\"available_type_{x}\" if isinstance(x, (int, np.integer)) else x)\n",
    "\n",
    "        availability_block = availability_block.merge(type_pivot, on=[\"census_block\", \"time_slot\"], how=\"left\").fillna(0)\n",
    "        numeric_cols = availability_block.columns.difference([\"census_block\", \"time_slot\"])\n",
    "        availability_block[numeric_cols] = availability_block[numeric_cols].astype(int)\n",
    "\n",
    "    availability_block[\"census_tract\"] = availability_block[\"census_block\"].astype(str).str[:tract_digits]\n",
    "    availability_block = availability_block.sort_values([\"census_block\", \"time_slot\"]).reset_index(drop=True)\n",
    "\n",
    "    # tract hourly mean\n",
    "    available_df[\"time_slot_hour\"] = available_df[\"timestamp\"].dt.floor(availability_tract_time_granularity)\n",
    "    tract_5min_df = (\n",
    "        available_df.groupby([\"census_tract\", \"timestamp\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"total_available_5min\")\n",
    "    )\n",
    "    tract_5min_df[\"time_slot_hour\"] = tract_5min_df[\"timestamp\"].dt.floor(availability_tract_time_granularity)\n",
    "\n",
    "    availability_tract_hourly_raw = (\n",
    "        tract_5min_df.groupby([\"census_tract\", \"time_slot_hour\"])[\"total_available_5min\"]\n",
    "        .mean()\n",
    "        .round(0)\n",
    "        .astype(int)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"total_available_5min\": \"total_available\", \"time_slot_hour\": \"time_slot\"})\n",
    "        .sort_values([\"census_tract\", \"time_slot\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    availability_tract_hourly_norm = availability_tract_hourly_raw.copy()\n",
    "    if availability_normalize:\n",
    "        mn = availability_tract_hourly_norm[\"total_available\"].min()\n",
    "        mx = availability_tract_hourly_norm[\"total_available\"].max()\n",
    "        availability_tract_hourly_norm[\"total_available_norm\"] = (\n",
    "            0.0 if (pd.isna(mn) or pd.isna(mx) or mn == mx)\n",
    "            else (availability_tract_hourly_norm[\"total_available\"] - mn) / (mx - mn)\n",
    "        )\n",
    "        availability_tract_hourly_norm[\"total_available_norm\"] = availability_tract_hourly_norm[\"total_available_norm\"].round(5)\n",
    "\n",
    "    if save_outputs:\n",
    "        availability_block.to_csv(out_dir / f\"availability_block_{output_tag}.csv\", index=False)\n",
    "        availability_tract_hourly_raw.to_csv(out_dir / f\"availability_tract_hourly_raw_{output_tag}.csv\", index=False)\n",
    "        availability_tract_hourly_norm.to_csv(out_dir / f\"availability_tract_hourly_norm_{output_tag}.csv\", index=False)\n",
    "\n",
    "    results[\"availability\"] = {\n",
    "        \"availability_block_5min\": availability_block,\n",
    "        \"availability_tract_hourly_raw\": availability_tract_hourly_raw,\n",
    "        \"availability_tract_hourly_norm\": availability_tract_hourly_norm,\n",
    "    }\n",
    "\n",
    "    # ============================================================\n",
    "    # USAGE (INLINE)\n",
    "    # ============================================================\n",
    "    df_u = df.copy()\n",
    "    required = {\"timestamp\", \"lat\", \"lon\", \"census_block\"}\n",
    "    missing = required - set(df_u.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Usage input missing required columns: {missing}\")\n",
    "\n",
    "    df_u[\"time_slot\"] = df_u[\"timestamp\"].dt.floor(usage_base_time_slot)\n",
    "    df_u[\"rounded_lat\"] = df_u[\"lat\"].round(usage_rounding_decimals)\n",
    "    df_u[\"rounded_lon\"] = df_u[\"lon\"].round(usage_rounding_decimals)\n",
    "\n",
    "    counts = (\n",
    "        df_u.groupby([\"census_block\", \"time_slot\", \"rounded_lat\", \"rounded_lon\"], as_index=False)\n",
    "        .size()\n",
    "        .rename(columns={\"size\": \"cnt\"})\n",
    "    )\n",
    "\n",
    "    all_blocks_u = df_u[\"census_block\"].drop_duplicates().sort_values().to_numpy()\n",
    "    all_slots = np.sort(df_u[\"time_slot\"].unique())\n",
    "    if len(all_slots) < 2:\n",
    "        raise ValueError(\"Not enough time slots to compute dockless usage (need at least 2).\")\n",
    "\n",
    "    base_td = pd.to_timedelta(usage_base_time_slot)\n",
    "\n",
    "    prev = counts.copy()\n",
    "    prev[\"time_slot\"] = prev[\"time_slot\"] + base_td\n",
    "    prev = prev.rename(columns={\"cnt\": \"prev_cnt\"})\n",
    "\n",
    "    curr = counts.rename(columns={\"cnt\": \"curr_cnt\"})\n",
    "\n",
    "    diff = prev.merge(curr, on=[\"census_block\", \"time_slot\", \"rounded_lat\", \"rounded_lon\"], how=\"outer\")\n",
    "    diff[\"prev_cnt\"] = diff[\"prev_cnt\"].fillna(0).astype(int)\n",
    "    diff[\"curr_cnt\"] = diff[\"curr_cnt\"].fillna(0).astype(int)\n",
    "\n",
    "    diff[\"starts_part\"] = (diff[\"prev_cnt\"] - diff[\"curr_cnt\"]).clip(lower=0)\n",
    "    diff[\"ends_part\"] = (diff[\"curr_cnt\"] - diff[\"prev_cnt\"]).clip(lower=0)\n",
    "\n",
    "    usage_5min_block = (\n",
    "        diff.groupby([\"census_block\", \"time_slot\"], as_index=False)\n",
    "        .agg(num_trip_starts=(\"starts_part\", \"sum\"), num_trip_ends=(\"ends_part\", \"sum\"))\n",
    "    )\n",
    "\n",
    "    full_index = pd.MultiIndex.from_product(\n",
    "        [all_blocks_u, all_slots[1:]],\n",
    "        names=[\"census_block\", \"time_slot\"],\n",
    "    ).to_frame(index=False)\n",
    "    usage_5min_block = full_index.merge(usage_5min_block, on=[\"census_block\", \"time_slot\"], how=\"left\")\n",
    "    usage_5min_block[[\"num_trip_starts\", \"num_trip_ends\"]] = (\n",
    "        usage_5min_block[[\"num_trip_starts\", \"num_trip_ends\"]].fillna(0).astype(int)\n",
    "    )\n",
    "    usage_5min_block = usage_5min_block.sort_values([\"census_block\", \"time_slot\"]).reset_index(drop=True)\n",
    "\n",
    "    usage_5min_block[\"agg_slot\"] = pd.to_datetime(usage_5min_block[\"time_slot\"]).dt.floor(usage_aggregate_time_slot)\n",
    "\n",
    "    usage_hourly_block = (\n",
    "        usage_5min_block.groupby([\"census_block\", \"agg_slot\"], as_index=False)[[\"num_trip_starts\", \"num_trip_ends\"]]\n",
    "        .sum()\n",
    "        .rename(columns={\"agg_slot\": \"time_slot\"})\n",
    "        .sort_values([\"census_block\", \"time_slot\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    usage_hourly_block[\"census_tract\"] = usage_hourly_block[\"census_block\"].astype(str).str[:tract_digits]\n",
    "\n",
    "    usage_hourly_tract = (\n",
    "        usage_hourly_block.groupby([\"census_tract\", \"time_slot\"], as_index=False)\n",
    "        .agg(num_trip_starts=(\"num_trip_starts\", \"sum\"), num_trip_ends=(\"num_trip_ends\", \"sum\"))\n",
    "        .sort_values([\"census_tract\", \"time_slot\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    if usage_normalize:\n",
    "        usage_hourly_tract[\"num_trip_starts_norm\"] = _minmax(usage_hourly_tract[\"num_trip_starts\"]).round(5)\n",
    "        usage_hourly_tract[\"num_trip_ends_norm\"] = _minmax(usage_hourly_tract[\"num_trip_ends\"]).round(5)\n",
    "\n",
    "    if save_outputs:\n",
    "        usage_5min_block.to_csv(out_dir / f\"usage_5min_block_{output_tag}.csv\", index=False)\n",
    "        usage_hourly_block.to_csv(out_dir / f\"usage_hourly_block_{output_tag}.csv\", index=False)\n",
    "        usage_hourly_tract.to_csv(out_dir / f\"usage_hourly_tract_{output_tag}.csv\", index=False)\n",
    "\n",
    "    results[\"usage\"] = {\n",
    "        \"usage_5min_block\": usage_5min_block,\n",
    "        \"usage_hourly_block\": usage_hourly_block,\n",
    "        \"usage_hourly_tract\": usage_hourly_tract,\n",
    "    }\n",
    "\n",
    "    # ============================================================\n",
    "    # IDLE TIME (INLINE)\n",
    "    # ============================================================\n",
    "    df_i = df.copy()\n",
    "    req = {idle_vehicle_id_col, \"timestamp\", idle_lat_col, idle_lon_col, idle_census_block_col, idle_vehicle_type_col}\n",
    "    missing = req - set(df_i.columns)\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"Idle-time input missing required columns: {missing}. \"\n",
    "            \"If Spin uses different names, override idle_*_col params.\"\n",
    "        )\n",
    "\n",
    "    df_i = df_i.sort_values([idle_vehicle_id_col, \"timestamp\"]).reset_index(drop=True)\n",
    "    df_i[\"rounded_lat\"] = df_i[idle_lat_col].round(idle_rounding_decimals)\n",
    "    df_i[\"rounded_lon\"] = df_i[idle_lon_col].round(idle_rounding_decimals)\n",
    "\n",
    "    df_i[\"time_slot_5min\"] = df_i[\"timestamp\"].dt.floor(idle_base_time_slot)\n",
    "    df_i[\"hour_bucket\"] = df_i[\"timestamp\"].dt.floor(idle_hour_bucket_freq)\n",
    "\n",
    "    df_i[\"location_key\"] = df_i[\"rounded_lat\"].astype(str) + \"_\" + df_i[\"rounded_lon\"].astype(str)\n",
    "\n",
    "    df_i[\"location_shift\"] = df_i.groupby([idle_vehicle_id_col, idle_census_block_col, \"hour_bucket\"])[\"location_key\"].shift()\n",
    "    df_i[\"new_segment\"] = (df_i[\"location_key\"] != df_i[\"location_shift\"]).astype(int)\n",
    "    df_i[\"segment_id\"] = df_i.groupby([idle_vehicle_id_col, idle_census_block_col, \"hour_bucket\"])[\"new_segment\"].cumsum()\n",
    "\n",
    "    segments = (\n",
    "        df_i.groupby([idle_vehicle_id_col, idle_census_block_col, \"hour_bucket\", idle_vehicle_type_col, \"segment_id\"], as_index=False)\n",
    "        .agg(segment_start=(\"timestamp\", \"min\"), segment_end=(\"timestamp\", \"max\"), ping_count=(\"timestamp\", \"count\"))\n",
    "    )\n",
    "    segments = segments[segments[\"ping_count\"] > 1].copy()\n",
    "    segments[\"idle_duration_minutes\"] = (segments[\"segment_end\"] - segments[\"segment_start\"]).dt.total_seconds() / 60.0\n",
    "\n",
    "    avg_idle = (\n",
    "        segments.groupby([idle_census_block_col, \"hour_bucket\", idle_vehicle_type_col], as_index=False)[\"idle_duration_minutes\"]\n",
    "        .mean()\n",
    "        .rename(columns={\"idle_duration_minutes\": \"avg_idle_time_minutes\"})\n",
    "    )\n",
    "    idle_counts = (\n",
    "        segments.groupby([idle_census_block_col, \"hour_bucket\", idle_vehicle_type_col], as_index=False)[\"segment_id\"]\n",
    "        .count()\n",
    "        .rename(columns={\"segment_id\": \"num_idle_segments\"})\n",
    "    )\n",
    "    idle_summary = avg_idle.merge(idle_counts, on=[idle_census_block_col, \"hour_bucket\", idle_vehicle_type_col], how=\"inner\")\n",
    "    idle_summary = idle_summary.sort_values([idle_census_block_col, \"hour_bucket\"]).reset_index(drop=True)\n",
    "\n",
    "    idle_summary_full = None\n",
    "    if idle_fill_full_grid:\n",
    "        service_area_blocks = df_i[idle_census_block_col].drop_duplicates().to_numpy()\n",
    "        all_hours = np.sort(df_i[\"hour_bucket\"].unique())\n",
    "        full_index = pd.MultiIndex.from_product(\n",
    "            [service_area_blocks, all_hours],\n",
    "            names=[idle_census_block_col, \"hour_bucket\"],\n",
    "        ).to_frame(index=False)\n",
    "\n",
    "        idle_summary_full = full_index.merge(idle_summary, on=[idle_census_block_col, \"hour_bucket\"], how=\"left\")\n",
    "        idle_summary_full[idle_vehicle_type_col] = idle_summary_full[idle_vehicle_type_col].fillna(idle_default_vehicle_type_for_missing)\n",
    "        idle_summary_full[\"avg_idle_time_minutes\"] = idle_summary_full[\"avg_idle_time_minutes\"].fillna(0.0)\n",
    "        idle_summary_full[\"num_idle_segments\"] = idle_summary_full[\"num_idle_segments\"].fillna(0).astype(int)\n",
    "        idle_summary_full = idle_summary_full.sort_values([idle_census_block_col, \"hour_bucket\"]).reset_index(drop=True)\n",
    "\n",
    "    block_for_tract = idle_summary_full if idle_summary_full is not None else idle_summary\n",
    "    block_for_tract[\"census_tract\"] = block_for_tract[idle_census_block_col].astype(str).str[:tract_digits]\n",
    "\n",
    "    tract_idle_summary = (\n",
    "        block_for_tract.groupby([\"census_tract\", \"hour_bucket\", idle_vehicle_type_col], as_index=False)\n",
    "        .agg(avg_idle_time_minutes=(\"avg_idle_time_minutes\", \"mean\"), num_idle_segments=(\"num_idle_segments\", \"sum\"))\n",
    "        .sort_values([\"census_tract\", \"hour_bucket\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    tract_idle_norm = tract_idle_summary.copy()\n",
    "    if idle_normalize:\n",
    "        tract_idle_norm[\"avg_idle_time_minutes_norm\"] = _minmax(tract_idle_norm[\"avg_idle_time_minutes\"]).round(5)\n",
    "\n",
    "    if save_outputs:\n",
    "        idle_summary.to_csv(out_dir / f\"idle_summary_block_{output_tag}.csv\", index=False)\n",
    "        if idle_summary_full is not None:\n",
    "            idle_summary_full.to_csv(out_dir / f\"idle_summary_block_full_{output_tag}.csv\", index=False)\n",
    "        tract_idle_summary.to_csv(out_dir / f\"idle_summary_tract_{output_tag}.csv\", index=False)\n",
    "        tract_idle_norm.to_csv(out_dir / f\"idle_summary_tract_norm_{output_tag}.csv\", index=False)\n",
    "\n",
    "    results[\"idle_time\"] = {\n",
    "        \"idle_summary_block\": idle_summary,\n",
    "        \"idle_summary_block_full\": idle_summary_full,\n",
    "        \"idle_summary_tract\": tract_idle_summary,\n",
    "        \"idle_summary_tract_norm\": tract_idle_norm,\n",
    "    }\n",
    "\n",
    "    # ============================================================\n",
    "    # SAFETY (INLINE)\n",
    "    # ============================================================\n",
    "    import geopandas as gpd\n",
    "    from shapely import wkt\n",
    "\n",
    "    if safety_protected_keywords is None:\n",
    "        safety_protected_keywords = [\n",
    "            \"protected\", \"separated\", \"cycle track\", \"cycletrack\", \"parking protected\",\n",
    "            \"class i\", \"class 1\", \"path\", \"off-street\", \"off street\", \"buffered\", \"raised\"\n",
    "        ]\n",
    "    if safety_class_col_candidate_keywords is None:\n",
    "        safety_class_col_candidate_keywords = [\n",
    "            \"facility\", \"class\", \"type\", \"category\", \"treatment\", \"protected\",\n",
    "            \"separation\", \"separator\", \"lane\", \"bikelane\", \"bike lane\", \"route\"\n",
    "        ]\n",
    "\n",
    "    safety_protected_match_mode = safety_protected_match_mode.lower().strip()\n",
    "    if safety_protected_match_mode not in {\"exact\", \"contains\"}:\n",
    "        raise ValueError(\"safety_protected_match_mode must be 'exact' or 'contains'\")\n",
    "\n",
    "    blocks = gpd.read_file(str(census_blocks_shp))\n",
    "    if safety_census_block_id_col not in blocks.columns:\n",
    "        raise ValueError(\n",
    "            f\"census block ID column '{safety_census_block_id_col}' not found. \"\n",
    "            f\"Available: {list(blocks.columns)[:25]} ...\"\n",
    "        )\n",
    "\n",
    "    if safety_working_epsg is not None:\n",
    "        blocks = blocks.to_crs(epsg=int(safety_working_epsg))\n",
    "    else:\n",
    "        if blocks.crs is None:\n",
    "            raise ValueError(\"census_blocks_shp has no CRS. Provide safety_working_epsg.\")\n",
    "\n",
    "    streets_df = pd.read_csv(str(centerline_streets_csv))\n",
    "    if safety_centerline_wkt_col not in streets_df.columns:\n",
    "        raise ValueError(f\"Centerline WKT column '{safety_centerline_wkt_col}' not found in centerline CSV.\")\n",
    "    streets_df[\"geometry\"] = streets_df[safety_centerline_wkt_col].apply(\n",
    "        lambda x: wkt.loads(x) if isinstance(x, str) and x.startswith(\"LINESTRING\") else None\n",
    "    )\n",
    "    streets_gdf = gpd.GeoDataFrame(streets_df, geometry=\"geometry\", crs=safety_input_crs)\n",
    "    streets_gdf = streets_gdf[~streets_gdf[\"geometry\"].isna()].copy()\n",
    "    streets_gdf = streets_gdf.to_crs(blocks.crs)\n",
    "\n",
    "    bike_df = pd.read_csv(str(bike_lanes_csv))\n",
    "    if safety_bike_lane_wkt_col not in bike_df.columns:\n",
    "        raise ValueError(f\"Bike lane WKT column '{safety_bike_lane_wkt_col}' not found in bike lane CSV.\")\n",
    "    bike_df[\"geometry\"] = bike_df[safety_bike_lane_wkt_col].apply(\n",
    "        lambda x: wkt.loads(x) if isinstance(x, str) and x.startswith(\"LINESTRING\") else None\n",
    "    )\n",
    "    bike_gdf = gpd.GeoDataFrame(bike_df, geometry=\"geometry\", crs=safety_input_crs)\n",
    "    bike_gdf = bike_gdf[~bike_gdf[\"geometry\"].isna()].copy()\n",
    "    bike_gdf = bike_gdf.to_crs(blocks.crs)\n",
    "\n",
    "    def _norm_text(x: Any) -> str:\n",
    "        if pd.isna(x):\n",
    "            return \"\"\n",
    "        return str(x).strip().lower()\n",
    "\n",
    "    detected_class_col = None\n",
    "    detection_scores: List[Tuple[str, float]] = []\n",
    "\n",
    "    if safety_bike_lane_class_col is None:\n",
    "        text_cols = [c for c in bike_gdf.columns if bike_gdf[c].dtype == \"object\" and c != \"geometry\"]\n",
    "\n",
    "        def _score_col(col: str) -> float:\n",
    "            name_score = sum(1 for kw in safety_class_col_candidate_keywords if kw in col.lower())\n",
    "            sample = bike_gdf[col].dropna().astype(str).head(5000)\n",
    "            val_join = \" \".join(sample.str.lower().tolist())\n",
    "            val_score = sum(val_join.count(kw) for kw in safety_protected_keywords)\n",
    "            return float(name_score) + float(val_score) / 1000.0\n",
    "\n",
    "        for c in text_cols:\n",
    "            s = _score_col(c)\n",
    "            detection_scores.append((c, s))\n",
    "        detection_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        if detection_scores and detection_scores[0][1] > 0:\n",
    "            detected_class_col = detection_scores[0][0]\n",
    "            safety_bike_lane_class_col = detected_class_col\n",
    "\n",
    "    can_compute_protected = safety_bike_lane_class_col is not None and safety_bike_lane_class_col in bike_gdf.columns\n",
    "\n",
    "    protected_gdf = bike_gdf.iloc[0:0].copy()\n",
    "    inferred_protected_values = None\n",
    "\n",
    "    if can_compute_protected:\n",
    "        col_series = bike_gdf[safety_bike_lane_class_col].astype(str)\n",
    "        if safety_protected_values:\n",
    "            pv = [p.strip().lower() for p in safety_protected_values]\n",
    "            if safety_protected_match_mode == \"exact\":\n",
    "                mask = col_series.map(_norm_text).isin(pv)\n",
    "            else:\n",
    "                mask = col_series.map(lambda v: any(k in _norm_text(v) for k in pv))\n",
    "            protected_gdf = bike_gdf[mask].copy()\n",
    "            inferred_protected_values = pv\n",
    "        else:\n",
    "            kws = [k.strip().lower() for k in safety_protected_keywords]\n",
    "            mask = col_series.map(lambda v: any(k in _norm_text(v) for k in kws))\n",
    "            protected_gdf = bike_gdf[mask].copy()\n",
    "            inferred_protected_values = kws\n",
    "\n",
    "    streets_in_blocks = gpd.overlay(streets_gdf, blocks, how=\"intersection\")\n",
    "    streets_in_blocks[\"segment_length\"] = streets_in_blocks.geometry.length\n",
    "    street_len_block = (\n",
    "        streets_in_blocks.groupby(safety_census_block_id_col, as_index=False)\n",
    "        .agg(streets_leng=(\"segment_length\", \"sum\"))\n",
    "        .rename(columns={safety_census_block_id_col: \"census_block\"})\n",
    "    )\n",
    "    street_len_block[\"census_block\"] = street_len_block[\"census_block\"].astype(str)\n",
    "    street_len_block[\"streets_leng\"] = street_len_block[\"streets_leng\"].round(3)\n",
    "\n",
    "    bike_in_blocks = gpd.overlay(bike_gdf, blocks, how=\"intersection\")\n",
    "    bike_in_blocks[\"bike_lane_length\"] = bike_in_blocks.geometry.length\n",
    "    bike_len_block = (\n",
    "        bike_in_blocks.groupby(safety_census_block_id_col, as_index=False)\n",
    "        .agg(total_bike_lane_length=(\"bike_lane_length\", \"sum\"))\n",
    "        .rename(columns={safety_census_block_id_col: \"census_block\"})\n",
    "    )\n",
    "    bike_len_block[\"census_block\"] = bike_len_block[\"census_block\"].astype(str)\n",
    "    bike_len_block[\"total_bike_lane_length\"] = bike_len_block[\"total_bike_lane_length\"].round(3)\n",
    "\n",
    "    protected_len_block = pd.DataFrame({\"census_block\": [], \"protected_bike_lane_length\": []})\n",
    "    if can_compute_protected and not protected_gdf.empty:\n",
    "        protected_in_blocks = gpd.overlay(protected_gdf, blocks, how=\"intersection\")\n",
    "        protected_in_blocks[\"protected_lane_length\"] = protected_in_blocks.geometry.length\n",
    "        protected_len_block = (\n",
    "            protected_in_blocks.groupby(safety_census_block_id_col, as_index=False)\n",
    "            .agg(protected_bike_lane_length=(\"protected_lane_length\", \"sum\"))\n",
    "            .rename(columns={safety_census_block_id_col: \"census_block\"})\n",
    "        )\n",
    "        protected_len_block[\"census_block\"] = protected_len_block[\"census_block\"].astype(str)\n",
    "        protected_len_block[\"protected_bike_lane_length\"] = protected_len_block[\"protected_bike_lane_length\"].round(3)\n",
    "\n",
    "    safety_block = (\n",
    "        street_len_block.merge(bike_len_block, on=\"census_block\", how=\"left\")\n",
    "        .merge(protected_len_block, on=\"census_block\", how=\"left\")\n",
    "    )\n",
    "    safety_block[\"total_bike_lane_length\"] = safety_block[\"total_bike_lane_length\"].fillna(0.0)\n",
    "    safety_block[\"protected_bike_lane_length\"] = safety_block[\"protected_bike_lane_length\"].fillna(0.0)\n",
    "\n",
    "    safety_block[\"bike_lane_ratio\"] = safety_block.apply(\n",
    "        lambda r: (r[\"total_bike_lane_length\"] / r[\"streets_leng\"]) if r[\"streets_leng\"] > 0 else 0.0,\n",
    "        axis=1,\n",
    "    ).round(3)\n",
    "    safety_block[\"protected_bike_lane_ratio\"] = safety_block.apply(\n",
    "        lambda r: (r[\"protected_bike_lane_length\"] / r[\"streets_leng\"]) if r[\"streets_leng\"] > 0 else 0.0,\n",
    "        axis=1,\n",
    "    ).round(3)\n",
    "\n",
    "    ct = pd.read_csv(str(centroid_tract_csv))\n",
    "    if safety_centroid_tract_id_col not in ct.columns:\n",
    "        raise ValueError(f\"centroid_tract_csv missing '{safety_centroid_tract_id_col}'. Available: {list(ct.columns)}\")\n",
    "    ct[safety_centroid_tract_id_col] = ct[safety_centroid_tract_id_col].astype(str)\n",
    "\n",
    "    safety_block[\"census_tract\"] = safety_block[\"census_block\"].astype(str).str[:tract_digits]\n",
    "    tract_sums = safety_block.groupby(\"census_tract\", as_index=False)[\n",
    "        [\"streets_leng\", \"total_bike_lane_length\", \"protected_bike_lane_length\"]\n",
    "    ].sum()\n",
    "\n",
    "    keep_cols = [safety_centroid_tract_id_col]\n",
    "    if safety_centroid_area_col is not None and safety_centroid_area_col in ct.columns:\n",
    "        keep_cols.append(safety_centroid_area_col)\n",
    "\n",
    "    safety_tract = ct[keep_cols].merge(\n",
    "        tract_sums,\n",
    "        left_on=safety_centroid_tract_id_col,\n",
    "        right_on=\"census_tract\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    safety_tract[[\"streets_leng\", \"total_bike_lane_length\", \"protected_bike_lane_length\"]] = (\n",
    "        safety_tract[[\"streets_leng\", \"total_bike_lane_length\", \"protected_bike_lane_length\"]].fillna(0.0)\n",
    "    )\n",
    "\n",
    "    safety_tract[\"bike_lane_ratio\"] = safety_tract.apply(\n",
    "        lambda r: (r[\"total_bike_lane_length\"] / r[\"streets_leng\"]) if r[\"streets_leng\"] > 0 else 0.0,\n",
    "        axis=1,\n",
    "    )\n",
    "    safety_tract[\"protected_bike_lane_ratio\"] = safety_tract.apply(\n",
    "        lambda r: (r[\"protected_bike_lane_length\"] / r[\"streets_leng\"]) if r[\"streets_leng\"] > 0 else 0.0,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    safety_tract = safety_tract.sort_values(by=safety_centroid_tract_id_col).reset_index(drop=True)\n",
    "\n",
    "    safety_tract_norm = safety_tract.copy()\n",
    "    if safety_normalize:\n",
    "        safety_tract_norm[\"bike_lane_ratio_norm\"] = _minmax(safety_tract_norm[\"bike_lane_ratio\"]).round(5)\n",
    "        safety_tract_norm[\"protected_bike_lane_ratio_norm\"] = _minmax(safety_tract_norm[\"protected_bike_lane_ratio\"]).round(5)\n",
    "\n",
    "    if save_outputs:\n",
    "        safety_block.to_csv(out_dir / f\"safety_block_bike_lane_{output_tag}.csv\", index=False)\n",
    "        safety_tract.to_csv(out_dir / f\"safety_bike_lane_tract_{output_tag}.csv\", index=False)\n",
    "        safety_tract_norm.to_csv(out_dir / f\"safety_bike_lane_norm_tract_{output_tag}.csv\", index=False)\n",
    "\n",
    "    results[\"safety\"] = {\n",
    "        \"safety_block_df\": safety_block,\n",
    "        \"safety_tract_df\": safety_tract,\n",
    "        \"safety_tract_norm_df\": safety_tract_norm,\n",
    "        \"meta\": {\n",
    "            \"working_crs\": str(blocks.crs),\n",
    "            \"bike_lane_class_col_used\": safety_bike_lane_class_col,\n",
    "            \"bike_lane_class_col_detected\": detected_class_col,\n",
    "            \"protected_match_mode\": safety_protected_match_mode,\n",
    "            \"protected_values_used\": inferred_protected_values,\n",
    "            \"detection_scores_top5\": detection_scores[:5],\n",
    "            \"normalize\": safety_normalize,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Example calls\n",
    "# ----------------------------\n",
    "# LIME\n",
    "# out_lime = run_sf_dockless_all_utilities_single_function(\n",
    "#     system_key=\"SF_LIME_DOCKLESS\",\n",
    "#     freebike_status_txt=r\"D:\\Research Fellowship\\Summer Research Stuff\\Collected Data\\Week 1\\09-June\\san_fran_lime_dkless_freebike_status_6_9.txt\",\n",
    "#     output_dir=\"SF_LIME_FULL_RUN\",\n",
    "#     time_start=\"2025-06-09 06:00:00\",\n",
    "#     time_end=\"2025-06-09 12:00:00\",\n",
    "# )\n",
    "\n",
    "# SPIN\n",
    "# out_spin = run_sf_dockless_all_utilities_single_function(\n",
    "#     system_key=\"SF_SPIN_DOCKLESS\",\n",
    "#     freebike_status_txt=r\"D:\\Research Fellowship\\Summer Research Stuff\\Collected Data\\Week 1\\09-June\\san_fran_spin_dkless_freebike_status_6_9.txt\",\n",
    "#     output_dir=\"SF_SPIN_FULL_RUN\",\n",
    "#     time_start=\"2025-06-09 06:00:00\",\n",
    "#     time_end=\"2025-06-09 12:00:00\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25ae59a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_41180\\688743888.py:245: FutureWarning: Parsed string \"2025-06-09 00:00:00 CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  raw_df[\"timestamp\"] = pd.to_datetime(raw_df[\"timestamp\"], errors=\"coerce\")\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_41180\\688743888.py:452: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  available_df[\"time_slot_hour\"] = available_df[\"timestamp\"].dt.floor(availability_tract_time_granularity)\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_41180\\688743888.py:458: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  tract_5min_df[\"time_slot_hour\"] = tract_5min_df[\"timestamp\"].dt.floor(availability_tract_time_granularity)\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_41180\\688743888.py:546: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  usage_5min_block[\"agg_slot\"] = pd.to_datetime(usage_5min_block[\"time_slot\"]).dt.floor(usage_aggregate_time_slot)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Idle-time input missing required columns: {'vehicle_type'}. If Spin uses different names, override idle_*_col params.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m out_spin \u001b[38;5;241m=\u001b[39m \u001b[43mrun_sf_dockless_all_utilities_single_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSF_SPIN_DOCKLESS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Change the system key accordingly SF_LIME_DOCKLESS or SF_SPIN_DOCKLESS\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreebike_status_txt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mResearch Fellowship\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mSummer Research Stuff\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mCollected Data\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mWeek 1\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m09-June\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msan_fran_spin_dkless_freebike_status_6_9.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSF_SPIN_FULL_RUN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#Change the output directory name accordingly\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2025-06-09 06:00:00\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2025-06-09 12:00:00\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 587\u001b[0m, in \u001b[0;36mrun_sf_dockless_all_utilities_single_function\u001b[1;34m(system_key, freebike_status_txt, census_blocks_shp, centerline_streets_csv, bike_lanes_csv, centroid_tract_csv, output_dir, save_outputs, time_start, time_end, step0_raw_csv_name, step0_done_csv_name, blocks_geoid_col, blocks_target_epsg, fill_missing_with_census_api, census_benchmark, census_vintage, timestamp_parse_mode, drop_cols_if_present, availability_block_time_granularity, availability_tract_time_granularity, reserved_col, disabled_col, vehicle_id_col, include_vehicle_type, vehicle_type_col_avail, tract_digits, availability_normalize, usage_base_time_slot, usage_aggregate_time_slot, usage_rounding_decimals, usage_normalize, idle_base_time_slot, idle_hour_bucket_freq, idle_rounding_decimals, idle_vehicle_id_col, idle_lat_col, idle_lon_col, idle_census_block_col, idle_vehicle_type_col, idle_fill_full_grid, idle_default_vehicle_type_for_missing, idle_normalize, safety_centerline_wkt_col, safety_bike_lane_wkt_col, safety_input_crs, safety_census_block_id_col, safety_centroid_tract_id_col, safety_centroid_area_col, safety_working_epsg, safety_bike_lane_class_col, safety_protected_values, safety_protected_keywords, safety_class_col_candidate_keywords, safety_protected_match_mode, safety_normalize)\u001b[0m\n\u001b[0;32m    585\u001b[0m missing \u001b[38;5;241m=\u001b[39m req \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(df_i\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[1;32m--> 587\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    588\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIdle-time input missing required columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    589\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf Spin uses different names, override idle_*_col params.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    590\u001b[0m     )\n\u001b[0;32m    592\u001b[0m df_i \u001b[38;5;241m=\u001b[39m df_i\u001b[38;5;241m.\u001b[39msort_values([idle_vehicle_id_col, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    593\u001b[0m df_i[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrounded_lat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_i[idle_lat_col]\u001b[38;5;241m.\u001b[39mround(idle_rounding_decimals)\n",
      "\u001b[1;31mValueError\u001b[0m: Idle-time input missing required columns: {'vehicle_type'}. If Spin uses different names, override idle_*_col params."
     ]
    }
   ],
   "source": [
    "out_spin = run_sf_dockless_all_utilities_single_function( \n",
    "    system_key=\"SF_SPIN_DOCKLESS\", # Change the system key accordingly SF_LIME_DOCKLESS or SF_SPIN_DOCKLESS\n",
    "    freebike_status_txt=r\"D:\\Research Fellowship\\Summer Research Stuff\\Collected Data\\Week 1\\09-June\\san_fran_spin_dkless_freebike_status_6_9.txt\",\n",
    "    output_dir=\"SF_SPIN_FULL_RUN\", #Change the output directory name accordingly\n",
    "    time_start=\"2025-06-09 06:00:00\",\n",
    "    time_end=\"2025-06-09 12:00:00\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bbf2072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, Dict, Any, List, Tuple\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely import wkt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def run_sf_dockless_all_utilities_single_function(\n",
    "    *,\n",
    "    # ------------------------------------------------------------\n",
    "    # USER INPUTS\n",
    "    # ------------------------------------------------------------\n",
    "    system_key: str,  # \"SF_LIME_DOCKLESS\" or \"SF_SPIN_DOCKLESS\"\n",
    "    freebike_status_txt: Union[str, Path],\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # CITY ASSETS (same for both SF dockless systems unless overridden)\n",
    "    # ------------------------------------------------------------\n",
    "    census_blocks_shp: Union[str, Path] = r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\GBFS_Census_Tract\\San_Fran_Lime\\tl_2024_06_tabblock20.shp\",\n",
    "    centerline_streets_csv: Union[str, Path] = r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\San_Fran_Lime\\Centerline.csv\",\n",
    "    bike_lanes_csv: Union[str, Path] = r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\San_Fran_Lime\\Bikelane.csv\",\n",
    "    centroid_tract_csv: Union[str, Path] = r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\San_Fran_Baywheels\\centroid_tract_ca.csv\",\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # OUTPUT\n",
    "    # ------------------------------------------------------------\n",
    "    output_dir: Optional[Union[str, Path]] = None,  # if None -> default per system\n",
    "    save_outputs: bool = True,\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # TIME WINDOW (applies to availability/usage/idle)\n",
    "    # ------------------------------------------------------------\n",
    "    time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # STEP 0 SETTINGS\n",
    "    # ------------------------------------------------------------\n",
    "    step0_raw_csv_name: Optional[str] = None,   # if None -> default per system\n",
    "    step0_done_csv_name: Optional[str] = None,  # if None -> default per system\n",
    "    blocks_geoid_col: str = \"GEOID20\",\n",
    "    blocks_target_epsg: int = 4326,\n",
    "    fill_missing_with_census_api: bool = True,\n",
    "    census_benchmark: str = \"Public_AR_Census2020\",\n",
    "    census_vintage: str = \"2020\",\n",
    "    timestamp_parse_mode: str = \"auto\",  # \"auto\" or \"epoch\"\n",
    "    drop_cols_if_present: Optional[List[str]] = None,\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # AVAILABILITY SETTINGS\n",
    "    # ------------------------------------------------------------\n",
    "    availability_block_time_granularity: str = \"5min\",\n",
    "    availability_tract_time_granularity: str = \"1H\",\n",
    "    reserved_col: str = \"is_reserved\",\n",
    "    disabled_col: str = \"is_disabled\",\n",
    "    vehicle_id_col: str = \"bike_id\",\n",
    "    include_vehicle_type: bool = True,\n",
    "    vehicle_type_col_avail: str = \"vehicle_type_id\",\n",
    "    tract_digits: int = 11,\n",
    "    availability_normalize: bool = True,\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # USAGE SETTINGS\n",
    "    # ------------------------------------------------------------\n",
    "    usage_base_time_slot: str = \"5min\",\n",
    "    usage_aggregate_time_slot: str = \"1H\",\n",
    "    usage_rounding_decimals: int = 4,\n",
    "    usage_normalize: bool = True,\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # IDLE TIME SETTINGS\n",
    "    # ------------------------------------------------------------\n",
    "    idle_base_time_slot: str = \"5min\",\n",
    "    idle_hour_bucket_freq: str = \"1H\",\n",
    "    idle_rounding_decimals: int = 4,\n",
    "    idle_vehicle_id_col: str = \"bike_id\",\n",
    "    idle_lat_col: str = \"lat\",\n",
    "    idle_lon_col: str = \"lon\",\n",
    "    idle_census_block_col: str = \"census_block\",\n",
    "    \n",
    "    # --- UPDATED DEFAULT BELOW ---\n",
    "    idle_vehicle_type_col: str = \"vehicle_type_id\", \n",
    "    # -----------------------------\n",
    "    \n",
    "    idle_fill_full_grid: bool = True,\n",
    "    idle_default_vehicle_type_for_missing: str = \"scooter\",\n",
    "    idle_normalize: bool = True,\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # SAFETY SETTINGS (generic)\n",
    "    # ------------------------------------------------------------\n",
    "    safety_centerline_wkt_col: str = \"line\",\n",
    "    safety_bike_lane_wkt_col: str = \"shape\",\n",
    "    safety_input_crs: str = \"EPSG:4326\",\n",
    "    safety_census_block_id_col: str = \"GEOID20\",\n",
    "    safety_centroid_tract_id_col: str = \"census_tract\",\n",
    "    safety_centroid_area_col: Optional[str] = \"county_name\",\n",
    "    safety_working_epsg: Optional[int] = 26910,  # UTM zone for SF-ish\n",
    "    safety_bike_lane_class_col: Optional[str] = None,\n",
    "    safety_protected_values: Optional[List[str]] = None,\n",
    "    safety_protected_keywords: Optional[List[str]] = None,\n",
    "    safety_class_col_candidate_keywords: Optional[List[str]] = None,\n",
    "    safety_protected_match_mode: str = \"contains\",\n",
    "    safety_normalize: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    ONE function to run ALL SF dockless utilities for multiple systems (Lime, Spin):\n",
    "      Step0 (txt->done_df with census block/tract)\n",
    "      Availability (block+tract)\n",
    "      Usage (5min movement -> hourly tract)\n",
    "      Idle time (segment-based idle durations)\n",
    "      Safety (centerline/bikelane intersection + protected inference)\n",
    "\n",
    "    Use:\n",
    "      system_key=\"SF_LIME_DOCKLESS\" or \"SF_SPIN_DOCKLESS\"\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================\n",
    "    # System presets (only differences: labels + default filenames + output tags)\n",
    "    # ============================================================\n",
    "    SYSTEMS: Dict[str, Dict[str, str]] = {\n",
    "        \"SF_LIME_DOCKLESS\": {\n",
    "            \"city\": \"San Francisco\",\n",
    "            \"vendor\": \"Lime\",\n",
    "            \"system_type\": \"dockless\",\n",
    "            \"tag\": \"sf_lime\",\n",
    "            \"default_output_dir\": \"SF_LIME_DOCKLESS_FULL_RUN\",\n",
    "            \"raw_csv\": \"san_fran_lime_status_raw.csv\",\n",
    "            \"done_csv\": \"san_fran_lime_status_done.csv\",\n",
    "        },\n",
    "        \"SF_SPIN_DOCKLESS\": {\n",
    "            \"city\": \"San Francisco\",\n",
    "            \"vendor\": \"Spin\",\n",
    "            \"system_type\": \"dockless\",\n",
    "            \"tag\": \"sf_spin\",\n",
    "            \"default_output_dir\": \"SF_SPIN_DOCKLESS_FULL_RUN\",\n",
    "            \"raw_csv\": \"san_fran_spin_status_raw.csv\",\n",
    "            \"done_csv\": \"san_fran_spin_status_done.csv\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if system_key not in SYSTEMS:\n",
    "        raise ValueError(f\"Unknown system_key='{system_key}'. Choose from: {sorted(SYSTEMS.keys())}\")\n",
    "\n",
    "    preset = SYSTEMS[system_key]\n",
    "    output_tag = preset[\"tag\"]\n",
    "\n",
    "    if output_dir is None:\n",
    "        output_dir = preset[\"default_output_dir\"]\n",
    "\n",
    "    if step0_raw_csv_name is None:\n",
    "        step0_raw_csv_name = preset[\"raw_csv\"]\n",
    "    if step0_done_csv_name is None:\n",
    "        step0_done_csv_name = preset[\"done_csv\"]\n",
    "\n",
    "    # ============================================================\n",
    "    # Setup output dir + paths\n",
    "    # ============================================================\n",
    "    out_dir = Path(output_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    freebike_status_txt = Path(freebike_status_txt)\n",
    "    census_blocks_shp = Path(census_blocks_shp)\n",
    "    centerline_streets_csv = Path(centerline_streets_csv)\n",
    "    bike_lanes_csv = Path(bike_lanes_csv)\n",
    "    centroid_tract_csv = Path(centroid_tract_csv)\n",
    "\n",
    "    if not freebike_status_txt.exists():\n",
    "        raise FileNotFoundError(f\"freebike_status_txt not found: {freebike_status_txt}\")\n",
    "    for label, p in [\n",
    "        (\"census_blocks_shp\", census_blocks_shp),\n",
    "        (\"centerline_streets_csv\", centerline_streets_csv),\n",
    "        (\"bike_lanes_csv\", bike_lanes_csv),\n",
    "        (\"centroid_tract_csv\", centroid_tract_csv),\n",
    "    ]:\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"{label} not found: {p}\")\n",
    "\n",
    "    raw_path = out_dir / step0_raw_csv_name\n",
    "    done_path = out_dir / step0_done_csv_name\n",
    "\n",
    "    ts_start = pd.to_datetime(time_start) if time_start is not None else None\n",
    "    ts_end = pd.to_datetime(time_end) if time_end is not None else None\n",
    "\n",
    "    results: Dict[str, Any] = {\n",
    "        \"system_key\": system_key,\n",
    "        \"city\": preset[\"city\"],\n",
    "        \"vendor\": preset[\"vendor\"],\n",
    "        \"system_type\": preset[\"system_type\"],\n",
    "        \"output_dir\": str(out_dir),\n",
    "    }\n",
    "\n",
    "    # ============================================================\n",
    "    # Helpers\n",
    "    # ============================================================\n",
    "    def _minmax(series: pd.Series) -> pd.Series:\n",
    "        s = pd.to_numeric(series, errors=\"coerce\")\n",
    "        mn = s.min(skipna=True)\n",
    "        mx = s.max(skipna=True)\n",
    "        if pd.isna(mn) or pd.isna(mx) or mx <= mn:\n",
    "            return pd.Series(0.0, index=s.index)\n",
    "        return (s - mn) / (mx - mn)\n",
    "\n",
    "    # ============================================================\n",
    "    # STEP 0 (INLINE): txt -> raw_df -> spatial join -> api fill -> done_df\n",
    "    # ============================================================\n",
    "    with freebike_status_txt.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    rows: List[dict] = []\n",
    "    for line in lines:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        try:\n",
    "            blob = json.loads(line)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"⚠️ Error decoding JSON: {e}. Skipping line. First 100 chars: {line[:100]}...\")\n",
    "            continue\n",
    "\n",
    "        timestamp = list(blob.keys())[0]\n",
    "        entries = blob[timestamp]\n",
    "        for entry in entries:\n",
    "            if isinstance(entry, dict) and \"vehicle_types_available\" in entry:\n",
    "                vta = entry.get(\"vehicle_types_available\", [])\n",
    "                if isinstance(vta, list):\n",
    "                    for vt in vta:\n",
    "                        try:\n",
    "                            vid = vt[\"vehicle_type_id\"]\n",
    "                            cnt = vt[\"count\"]\n",
    "                            entry[f\"vehicle_type_{vid}_count\"] = cnt\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                try:\n",
    "                    del entry[\"vehicle_types_available\"]\n",
    "                except Exception:\n",
    "                    pass\n",
    "            entry[\"timestamp\"] = timestamp\n",
    "            rows.append(entry)\n",
    "\n",
    "    raw_df = pd.DataFrame(rows)\n",
    "\n",
    "    if \"timestamp\" in raw_df.columns:\n",
    "        if timestamp_parse_mode == \"epoch\":\n",
    "            raw_df[\"timestamp\"] = pd.to_datetime(raw_df[\"timestamp\"], unit=\"s\", errors=\"coerce\")\n",
    "        else:\n",
    "            raw_df[\"timestamp\"] = pd.to_datetime(raw_df[\"timestamp\"], errors=\"coerce\")\n",
    "            if raw_df[\"timestamp\"].isna().mean() > 0.5:\n",
    "                raw_df[\"timestamp\"] = pd.to_datetime(raw_df[\"timestamp\"], unit=\"s\", errors=\"coerce\")\n",
    "\n",
    "    if drop_cols_if_present:\n",
    "        drop_now = [c for c in drop_cols_if_present if c in raw_df.columns]\n",
    "        if drop_now:\n",
    "            raw_df = raw_df.drop(columns=drop_now)\n",
    "\n",
    "    if save_outputs:\n",
    "        raw_df.to_csv(raw_path, index=False)\n",
    "\n",
    "    if not {\"lat\", \"lon\"}.issubset(raw_df.columns):\n",
    "        raise ValueError(\n",
    "            \"Dockless input must include 'lat' and 'lon' columns to assign census blocks. \"\n",
    "            \"If Spin uses different names, pass overrides via idle_lat_col/idle_lon_col AND rename upstream.\"\n",
    "        )\n",
    "\n",
    "    latlon_df = raw_df.drop_duplicates(subset=[\"lat\", \"lon\"])[[\"lat\", \"lon\"]].copy()\n",
    "\n",
    "    geometry = [Point(xy) for xy in zip(latlon_df[\"lon\"], latlon_df[\"lat\"])]\n",
    "    points_gdf = gpd.GeoDataFrame(latlon_df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "    blocks_gdf = gpd.read_file(str(census_blocks_shp)).to_crs(epsg=blocks_target_epsg)\n",
    "    joined = gpd.sjoin(points_gdf, blocks_gdf, how=\"left\", predicate=\"within\")\n",
    "\n",
    "    if blocks_geoid_col not in joined.columns:\n",
    "        raise ValueError(\n",
    "            f\"Expected block GEOID column '{blocks_geoid_col}' not found after join. \"\n",
    "            f\"Available columns: {list(joined.columns)}\"\n",
    "        )\n",
    "\n",
    "    joined = joined.rename(columns={blocks_geoid_col: \"census_block\"})\n",
    "    joined[\"census_block\"] = joined[\"census_block\"].astype(str)\n",
    "    joined[\"census_tract\"] = joined[\"census_block\"].str[:-4]\n",
    "\n",
    "    latlon_blocks_df = joined[[\"lat\", \"lon\", \"census_block\", \"census_tract\"]].copy()\n",
    "    done_df = raw_df.merge(latlon_blocks_df, on=[\"lat\", \"lon\"], how=\"left\")\n",
    "\n",
    "    if fill_missing_with_census_api:\n",
    "        missing_points = done_df[done_df[\"census_block\"].isna()][[\"lat\", \"lon\"]].drop_duplicates().copy()\n",
    "        if not missing_points.empty:\n",
    "            try:\n",
    "                tqdm.pandas()\n",
    "                use_progress = True\n",
    "            except Exception:\n",
    "                use_progress = False\n",
    "\n",
    "            def _get_block(lat: float, lon: float) -> Optional[str]:\n",
    "                try:\n",
    "                    url = (\n",
    "                        \"https://geocoding.geo.census.gov/geocoder/geographies/coordinates\"\n",
    "                        f\"?x={lon}&y={lat}\"\n",
    "                        f\"&benchmark={census_benchmark}\"\n",
    "                        f\"&vintage={census_vintage}\"\n",
    "                        \"&format=json\"\n",
    "                    )\n",
    "                    r = requests.get(url, timeout=30)\n",
    "                    r.raise_for_status()\n",
    "                    js = r.json()\n",
    "                    geos = js.get(\"result\", {}).get(\"geographies\", {})\n",
    "                    blocks = geos.get(\"Census Blocks\", [])\n",
    "                    if blocks:\n",
    "                        return blocks[0].get(\"GEOID\")\n",
    "                    blocks2020 = geos.get(\"2020 Census Blocks\", [])\n",
    "                    if blocks2020:\n",
    "                        return blocks2020[0].get(\"GEOID\")\n",
    "                    return None\n",
    "                except Exception:\n",
    "                    return None\n",
    "\n",
    "            if use_progress:\n",
    "                missing_points[\"census_block_new\"] = missing_points.progress_apply(\n",
    "                    lambda r: _get_block(r[\"lat\"], r[\"lon\"]),\n",
    "                    axis=1,\n",
    "                )\n",
    "            else:\n",
    "                missing_points[\"census_block_new\"] = missing_points.apply(\n",
    "                    lambda r: _get_block(r[\"lat\"], r[\"lon\"]),\n",
    "                    axis=1,\n",
    "                )\n",
    "\n",
    "            missing_points[\"census_tract_new\"] = missing_points[\"census_block_new\"].apply(\n",
    "                lambda x: str(x)[:-4] if pd.notna(x) and x not in {\"None\", \"nan\"} else None\n",
    "            )\n",
    "\n",
    "            done_df = done_df.merge(\n",
    "                missing_points[[\"lat\", \"lon\", \"census_block_new\", \"census_tract_new\"]],\n",
    "                on=[\"lat\", \"lon\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "            done_df[\"census_block\"] = done_df[\"census_block\"].fillna(done_df[\"census_block_new\"])\n",
    "            done_df[\"census_tract\"] = done_df[\"census_tract\"].fillna(done_df[\"census_tract_new\"])\n",
    "            done_df = done_df.drop(columns=[\"census_block_new\", \"census_tract_new\"])\n",
    "\n",
    "    if save_outputs:\n",
    "        done_df.to_csv(done_path, index=False)\n",
    "\n",
    "    results[\"step0\"] = {\n",
    "        \"raw_df\": raw_df,\n",
    "        \"done_df\": done_df,\n",
    "        \"latlon_blocks_df\": latlon_blocks_df,\n",
    "        \"paths\": {\"raw_csv\": str(raw_path), \"done_csv\": str(done_path)},\n",
    "        \"meta\": {\n",
    "            \"raw_rows\": len(raw_df),\n",
    "            \"done_rows\": len(done_df),\n",
    "            \"unique_points\": len(latlon_df),\n",
    "            \"missing_after_sjoin\": int(done_df[\"census_block\"].isna().sum()),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # ============================================================\n",
    "    # Apply global time filter once (for downstream)\n",
    "    # ============================================================\n",
    "    df = done_df.copy()\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "    if df[\"timestamp\"].isna().any():\n",
    "        raise ValueError(f\"Step0 produced {int(df['timestamp'].isna().sum())} rows with unparseable timestamps.\")\n",
    "\n",
    "    data_min = df[\"timestamp\"].min()\n",
    "    data_max = df[\"timestamp\"].max()\n",
    "\n",
    "    if ts_start is not None:\n",
    "        df = df[df[\"timestamp\"] >= ts_start].copy()\n",
    "    if ts_end is not None:\n",
    "        df = df[df[\"timestamp\"] < ts_end].copy()\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\n",
    "            \"No rows after applying time window.\\n\"\n",
    "            f\"Requested window: [{ts_start}, {ts_end})\\n\"\n",
    "            f\"Data timestamp range: [{data_min}, {data_max}]\"\n",
    "        )\n",
    "\n",
    "    df[\"census_block\"] = df[\"census_block\"].astype(str)\n",
    "    if \"census_tract\" not in df.columns:\n",
    "        df[\"census_tract\"] = df[\"census_block\"].astype(str).str[:tract_digits]\n",
    "    else:\n",
    "        df[\"census_tract\"] = df[\"census_tract\"].astype(str)\n",
    "\n",
    "    results[\"meta\"] = {\"time_start\": ts_start, \"time_end\": ts_end, \"data_min\": data_min, \"data_max\": data_max}\n",
    "\n",
    "    # ============================================================\n",
    "    # AVAILABILITY (INLINE)\n",
    "    # ============================================================\n",
    "    df_av = df.sort_values([\"timestamp\", \"census_block\"]).reset_index(drop=True)\n",
    "    df_av[\"time_slot\"] = df_av[\"timestamp\"].dt.floor(availability_block_time_granularity)\n",
    "\n",
    "    if reserved_col not in df_av.columns or disabled_col not in df_av.columns:\n",
    "        raise ValueError(\n",
    "            f\"Missing '{reserved_col}' and/or '{disabled_col}' for dockless availability. \"\n",
    "            \"If Spin uses different names, pass reserved_col/disabled_col overrides.\"\n",
    "        )\n",
    "\n",
    "    available_df = df_av[(df_av[reserved_col] == 0) & (df_av[disabled_col] == 0)].copy()\n",
    "    if vehicle_id_col not in available_df.columns:\n",
    "        raise ValueError(f\"Missing vehicle id column '{vehicle_id_col}' for availability (override vehicle_id_col).\")\n",
    "\n",
    "    all_blocks = df_av[\"census_block\"].drop_duplicates().to_numpy()\n",
    "    all_time_slots = pd.date_range(\n",
    "        df_av[\"time_slot\"].min(),\n",
    "        df_av[\"time_slot\"].max(),\n",
    "        freq=availability_block_time_granularity,\n",
    "    )\n",
    "\n",
    "    full_index = pd.MultiIndex.from_product(\n",
    "        [all_blocks, all_time_slots],\n",
    "        names=[\"census_block\", \"time_slot\"],\n",
    "    ).to_frame(index=False)\n",
    "\n",
    "    base_availability = (\n",
    "        available_df.groupby([\"census_block\", \"time_slot\"])[vehicle_id_col]\n",
    "        .count()\n",
    "        .reset_index()\n",
    "        .rename(columns={vehicle_id_col: \"total_available\"})\n",
    "    )\n",
    "\n",
    "    availability_block = full_index.merge(base_availability, on=[\"census_block\", \"time_slot\"], how=\"left\")\n",
    "    availability_block[\"total_available\"] = availability_block[\"total_available\"].fillna(0).astype(int)\n",
    "\n",
    "    if include_vehicle_type:\n",
    "        if vehicle_type_col_avail not in available_df.columns:\n",
    "            raise ValueError(f\"include_vehicle_type=True but '{vehicle_type_col_avail}' not found (override).\")\n",
    "        type_pivot = available_df.pivot_table(\n",
    "            index=[\"census_block\", \"time_slot\"],\n",
    "            columns=vehicle_type_col_avail,\n",
    "            values=vehicle_id_col,\n",
    "            aggfunc=\"count\",\n",
    "            fill_value=0,\n",
    "        ).reset_index()\n",
    "        type_pivot.columns.name = None\n",
    "        type_pivot = type_pivot.rename(columns=lambda x: f\"available_type_{x}\" if isinstance(x, (int, np.integer)) else x)\n",
    "\n",
    "        availability_block = availability_block.merge(type_pivot, on=[\"census_block\", \"time_slot\"], how=\"left\").fillna(0)\n",
    "        numeric_cols = availability_block.columns.difference([\"census_block\", \"time_slot\"])\n",
    "        availability_block[numeric_cols] = availability_block[numeric_cols].astype(int)\n",
    "\n",
    "    availability_block[\"census_tract\"] = availability_block[\"census_block\"].astype(str).str[:tract_digits]\n",
    "    availability_block = availability_block.sort_values([\"census_block\", \"time_slot\"]).reset_index(drop=True)\n",
    "\n",
    "    # tract hourly mean\n",
    "    available_df[\"time_slot_hour\"] = available_df[\"timestamp\"].dt.floor(availability_tract_time_granularity)\n",
    "    tract_5min_df = (\n",
    "        available_df.groupby([\"census_tract\", \"timestamp\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"total_available_5min\")\n",
    "    )\n",
    "    tract_5min_df[\"time_slot_hour\"] = tract_5min_df[\"timestamp\"].dt.floor(availability_tract_time_granularity)\n",
    "\n",
    "    availability_tract_hourly_raw = (\n",
    "        tract_5min_df.groupby([\"census_tract\", \"time_slot_hour\"])[\"total_available_5min\"]\n",
    "        .mean()\n",
    "        .round(0)\n",
    "        .astype(int)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"total_available_5min\": \"total_available\", \"time_slot_hour\": \"time_slot\"})\n",
    "        .sort_values([\"census_tract\", \"time_slot\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    availability_tract_hourly_norm = availability_tract_hourly_raw.copy()\n",
    "    if availability_normalize:\n",
    "        mn = availability_tract_hourly_norm[\"total_available\"].min()\n",
    "        mx = availability_tract_hourly_norm[\"total_available\"].max()\n",
    "        availability_tract_hourly_norm[\"total_available_norm\"] = (\n",
    "            0.0 if (pd.isna(mn) or pd.isna(mx) or mn == mx)\n",
    "            else (availability_tract_hourly_norm[\"total_available\"] - mn) / (mx - mn)\n",
    "        )\n",
    "        availability_tract_hourly_norm[\"total_available_norm\"] = availability_tract_hourly_norm[\"total_available_norm\"].round(5)\n",
    "\n",
    "    if save_outputs:\n",
    "        availability_block.to_csv(out_dir / f\"availability_block_{output_tag}.csv\", index=False)\n",
    "        availability_tract_hourly_raw.to_csv(out_dir / f\"availability_tract_hourly_raw_{output_tag}.csv\", index=False)\n",
    "        availability_tract_hourly_norm.to_csv(out_dir / f\"availability_tract_hourly_norm_{output_tag}.csv\", index=False)\n",
    "\n",
    "    results[\"availability\"] = {\n",
    "        \"availability_block_5min\": availability_block,\n",
    "        \"availability_tract_hourly_raw\": availability_tract_hourly_raw,\n",
    "        \"availability_tract_hourly_norm\": availability_tract_hourly_norm,\n",
    "    }\n",
    "\n",
    "    # ============================================================\n",
    "    # USAGE (INLINE)\n",
    "    # ============================================================\n",
    "    df_u = df.copy()\n",
    "    required = {\"timestamp\", \"lat\", \"lon\", \"census_block\"}\n",
    "    missing = required - set(df_u.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Usage input missing required columns: {missing}\")\n",
    "\n",
    "    df_u[\"time_slot\"] = df_u[\"timestamp\"].dt.floor(usage_base_time_slot)\n",
    "    df_u[\"rounded_lat\"] = df_u[\"lat\"].round(usage_rounding_decimals)\n",
    "    df_u[\"rounded_lon\"] = df_u[\"lon\"].round(usage_rounding_decimals)\n",
    "\n",
    "    counts = (\n",
    "        df_u.groupby([\"census_block\", \"time_slot\", \"rounded_lat\", \"rounded_lon\"], as_index=False)\n",
    "        .size()\n",
    "        .rename(columns={\"size\": \"cnt\"})\n",
    "    )\n",
    "\n",
    "    all_blocks_u = df_u[\"census_block\"].drop_duplicates().sort_values().to_numpy()\n",
    "    all_slots = np.sort(df_u[\"time_slot\"].unique())\n",
    "    if len(all_slots) < 2:\n",
    "        raise ValueError(\"Not enough time slots to compute dockless usage (need at least 2).\")\n",
    "\n",
    "    base_td = pd.to_timedelta(usage_base_time_slot)\n",
    "\n",
    "    prev = counts.copy()\n",
    "    prev[\"time_slot\"] = prev[\"time_slot\"] + base_td\n",
    "    prev = prev.rename(columns={\"cnt\": \"prev_cnt\"})\n",
    "\n",
    "    curr = counts.rename(columns={\"cnt\": \"curr_cnt\"})\n",
    "\n",
    "    diff = prev.merge(curr, on=[\"census_block\", \"time_slot\", \"rounded_lat\", \"rounded_lon\"], how=\"outer\")\n",
    "    diff[\"prev_cnt\"] = diff[\"prev_cnt\"].fillna(0).astype(int)\n",
    "    diff[\"curr_cnt\"] = diff[\"curr_cnt\"].fillna(0).astype(int)\n",
    "\n",
    "    diff[\"starts_part\"] = (diff[\"prev_cnt\"] - diff[\"curr_cnt\"]).clip(lower=0)\n",
    "    diff[\"ends_part\"] = (diff[\"curr_cnt\"] - diff[\"prev_cnt\"]).clip(lower=0)\n",
    "\n",
    "    usage_5min_block = (\n",
    "        diff.groupby([\"census_block\", \"time_slot\"], as_index=False)\n",
    "        .agg(num_trip_starts=(\"starts_part\", \"sum\"), num_trip_ends=(\"ends_part\", \"sum\"))\n",
    "    )\n",
    "\n",
    "    full_index = pd.MultiIndex.from_product(\n",
    "        [all_blocks_u, all_slots[1:]],\n",
    "        names=[\"census_block\", \"time_slot\"],\n",
    "    ).to_frame(index=False)\n",
    "    usage_5min_block = full_index.merge(usage_5min_block, on=[\"census_block\", \"time_slot\"], how=\"left\")\n",
    "    usage_5min_block[[\"num_trip_starts\", \"num_trip_ends\"]] = (\n",
    "        usage_5min_block[[\"num_trip_starts\", \"num_trip_ends\"]].fillna(0).astype(int)\n",
    "    )\n",
    "    usage_5min_block = usage_5min_block.sort_values([\"census_block\", \"time_slot\"]).reset_index(drop=True)\n",
    "\n",
    "    usage_5min_block[\"agg_slot\"] = pd.to_datetime(usage_5min_block[\"time_slot\"]).dt.floor(usage_aggregate_time_slot)\n",
    "\n",
    "    usage_hourly_block = (\n",
    "        usage_5min_block.groupby([\"census_block\", \"agg_slot\"], as_index=False)[[\"num_trip_starts\", \"num_trip_ends\"]]\n",
    "        .sum()\n",
    "        .rename(columns={\"agg_slot\": \"time_slot\"})\n",
    "        .sort_values([\"census_block\", \"time_slot\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    usage_hourly_block[\"census_tract\"] = usage_hourly_block[\"census_block\"].astype(str).str[:tract_digits]\n",
    "\n",
    "    usage_hourly_tract = (\n",
    "        usage_hourly_block.groupby([\"census_tract\", \"time_slot\"], as_index=False)\n",
    "        .agg(num_trip_starts=(\"num_trip_starts\", \"sum\"), num_trip_ends=(\"num_trip_ends\", \"sum\"))\n",
    "        .sort_values([\"census_tract\", \"time_slot\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    if usage_normalize:\n",
    "        usage_hourly_tract[\"num_trip_starts_norm\"] = _minmax(usage_hourly_tract[\"num_trip_starts\"]).round(5)\n",
    "        usage_hourly_tract[\"num_trip_ends_norm\"] = _minmax(usage_hourly_tract[\"num_trip_ends\"]).round(5)\n",
    "\n",
    "    if save_outputs:\n",
    "        usage_5min_block.to_csv(out_dir / f\"usage_5min_block_{output_tag}.csv\", index=False)\n",
    "        usage_hourly_block.to_csv(out_dir / f\"usage_hourly_block_{output_tag}.csv\", index=False)\n",
    "        usage_hourly_tract.to_csv(out_dir / f\"usage_hourly_tract_{output_tag}.csv\", index=False)\n",
    "\n",
    "    results[\"usage\"] = {\n",
    "        \"usage_5min_block\": usage_5min_block,\n",
    "        \"usage_hourly_block\": usage_hourly_block,\n",
    "        \"usage_hourly_tract\": usage_hourly_tract,\n",
    "    }\n",
    "\n",
    "    # ============================================================\n",
    "    # IDLE TIME (INLINE)\n",
    "    # ============================================================\n",
    "    df_i = df.copy()\n",
    "    req = {idle_vehicle_id_col, \"timestamp\", idle_lat_col, idle_lon_col, idle_census_block_col, idle_vehicle_type_col}\n",
    "    missing = req - set(df_i.columns)\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"Idle-time input missing required columns: {missing}. \"\n",
    "            \"If Spin uses different names, override idle_*_col params.\"\n",
    "        )\n",
    "\n",
    "    df_i = df_i.sort_values([idle_vehicle_id_col, \"timestamp\"]).reset_index(drop=True)\n",
    "    df_i[\"rounded_lat\"] = df_i[idle_lat_col].round(idle_rounding_decimals)\n",
    "    df_i[\"rounded_lon\"] = df_i[idle_lon_col].round(idle_rounding_decimals)\n",
    "\n",
    "    df_i[\"time_slot_5min\"] = df_i[\"timestamp\"].dt.floor(idle_base_time_slot)\n",
    "    df_i[\"hour_bucket\"] = df_i[\"timestamp\"].dt.floor(idle_hour_bucket_freq)\n",
    "\n",
    "    df_i[\"location_key\"] = df_i[\"rounded_lat\"].astype(str) + \"_\" + df_i[\"rounded_lon\"].astype(str)\n",
    "\n",
    "    df_i[\"location_shift\"] = df_i.groupby([idle_vehicle_id_col, idle_census_block_col, \"hour_bucket\"])[\"location_key\"].shift()\n",
    "    df_i[\"new_segment\"] = (df_i[\"location_key\"] != df_i[\"location_shift\"]).astype(int)\n",
    "    df_i[\"segment_id\"] = df_i.groupby([idle_vehicle_id_col, idle_census_block_col, \"hour_bucket\"])[\"new_segment\"].cumsum()\n",
    "\n",
    "    segments = (\n",
    "        df_i.groupby([idle_vehicle_id_col, idle_census_block_col, \"hour_bucket\", idle_vehicle_type_col, \"segment_id\"], as_index=False)\n",
    "        .agg(segment_start=(\"timestamp\", \"min\"), segment_end=(\"timestamp\", \"max\"), ping_count=(\"timestamp\", \"count\"))\n",
    "    )\n",
    "    segments = segments[segments[\"ping_count\"] > 1].copy()\n",
    "    segments[\"idle_duration_minutes\"] = (segments[\"segment_end\"] - segments[\"segment_start\"]).dt.total_seconds() / 60.0\n",
    "\n",
    "    avg_idle = (\n",
    "        segments.groupby([idle_census_block_col, \"hour_bucket\", idle_vehicle_type_col], as_index=False)[\"idle_duration_minutes\"]\n",
    "        .mean()\n",
    "        .rename(columns={\"idle_duration_minutes\": \"avg_idle_time_minutes\"})\n",
    "    )\n",
    "    idle_counts = (\n",
    "        segments.groupby([idle_census_block_col, \"hour_bucket\", idle_vehicle_type_col], as_index=False)[\"segment_id\"]\n",
    "        .count()\n",
    "        .rename(columns={\"segment_id\": \"num_idle_segments\"})\n",
    "    )\n",
    "    idle_summary = avg_idle.merge(idle_counts, on=[idle_census_block_col, \"hour_bucket\", idle_vehicle_type_col], how=\"inner\")\n",
    "    idle_summary = idle_summary.sort_values([idle_census_block_col, \"hour_bucket\"]).reset_index(drop=True)\n",
    "\n",
    "    idle_summary_full = None\n",
    "    if idle_fill_full_grid:\n",
    "        service_area_blocks = df_i[idle_census_block_col].drop_duplicates().to_numpy()\n",
    "        all_hours = np.sort(df_i[\"hour_bucket\"].unique())\n",
    "        full_index = pd.MultiIndex.from_product(\n",
    "            [service_area_blocks, all_hours],\n",
    "            names=[idle_census_block_col, \"hour_bucket\"],\n",
    "        ).to_frame(index=False)\n",
    "\n",
    "        idle_summary_full = full_index.merge(idle_summary, on=[idle_census_block_col, \"hour_bucket\"], how=\"left\")\n",
    "        idle_summary_full[idle_vehicle_type_col] = idle_summary_full[idle_vehicle_type_col].fillna(idle_default_vehicle_type_for_missing)\n",
    "        idle_summary_full[\"avg_idle_time_minutes\"] = idle_summary_full[\"avg_idle_time_minutes\"].fillna(0.0)\n",
    "        idle_summary_full[\"num_idle_segments\"] = idle_summary_full[\"num_idle_segments\"].fillna(0).astype(int)\n",
    "        idle_summary_full = idle_summary_full.sort_values([idle_census_block_col, \"hour_bucket\"]).reset_index(drop=True)\n",
    "\n",
    "    block_for_tract = idle_summary_full if idle_summary_full is not None else idle_summary\n",
    "    block_for_tract[\"census_tract\"] = block_for_tract[idle_census_block_col].astype(str).str[:tract_digits]\n",
    "\n",
    "    tract_idle_summary = (\n",
    "        block_for_tract.groupby([\"census_tract\", \"hour_bucket\", idle_vehicle_type_col], as_index=False)\n",
    "        .agg(avg_idle_time_minutes=(\"avg_idle_time_minutes\", \"mean\"), num_idle_segments=(\"num_idle_segments\", \"sum\"))\n",
    "        .sort_values([\"census_tract\", \"hour_bucket\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    tract_idle_norm = tract_idle_summary.copy()\n",
    "    if idle_normalize:\n",
    "        tract_idle_norm[\"avg_idle_time_minutes_norm\"] = _minmax(tract_idle_norm[\"avg_idle_time_minutes\"]).round(5)\n",
    "\n",
    "    if save_outputs:\n",
    "        idle_summary.to_csv(out_dir / f\"idle_summary_block_{output_tag}.csv\", index=False)\n",
    "        if idle_summary_full is not None:\n",
    "            idle_summary_full.to_csv(out_dir / f\"idle_summary_block_full_{output_tag}.csv\", index=False)\n",
    "        tract_idle_summary.to_csv(out_dir / f\"idle_summary_tract_{output_tag}.csv\", index=False)\n",
    "        tract_idle_norm.to_csv(out_dir / f\"idle_summary_tract_norm_{output_tag}.csv\", index=False)\n",
    "\n",
    "    results[\"idle_time\"] = {\n",
    "        \"idle_summary_block\": idle_summary,\n",
    "        \"idle_summary_block_full\": idle_summary_full,\n",
    "        \"idle_summary_tract\": tract_idle_summary,\n",
    "        \"idle_summary_tract_norm\": tract_idle_norm,\n",
    "    }\n",
    "\n",
    "    # ============================================================\n",
    "    # SAFETY (INLINE)\n",
    "    # ============================================================\n",
    "    if safety_protected_keywords is None:\n",
    "        safety_protected_keywords = [\n",
    "            \"protected\", \"separated\", \"cycle track\", \"cycletrack\", \"parking protected\",\n",
    "            \"class i\", \"class 1\", \"path\", \"off-street\", \"off street\", \"buffered\", \"raised\"\n",
    "        ]\n",
    "    if safety_class_col_candidate_keywords is None:\n",
    "        safety_class_col_candidate_keywords = [\n",
    "            \"facility\", \"class\", \"type\", \"category\", \"treatment\", \"protected\",\n",
    "            \"separation\", \"separator\", \"lane\", \"bikelane\", \"bike lane\", \"route\"\n",
    "        ]\n",
    "\n",
    "    safety_protected_match_mode = safety_protected_match_mode.lower().strip()\n",
    "    if safety_protected_match_mode not in {\"exact\", \"contains\"}:\n",
    "        raise ValueError(\"safety_protected_match_mode must be 'exact' or 'contains'\")\n",
    "\n",
    "    blocks = gpd.read_file(str(census_blocks_shp))\n",
    "    if safety_census_block_id_col not in blocks.columns:\n",
    "        raise ValueError(\n",
    "            f\"census block ID column '{safety_census_block_id_col}' not found. \"\n",
    "            f\"Available: {list(blocks.columns)[:25]} ...\"\n",
    "        )\n",
    "\n",
    "    if safety_working_epsg is not None:\n",
    "        blocks = blocks.to_crs(epsg=int(safety_working_epsg))\n",
    "    else:\n",
    "        if blocks.crs is None:\n",
    "            raise ValueError(\"census_blocks_shp has no CRS. Provide safety_working_epsg.\")\n",
    "\n",
    "    streets_df = pd.read_csv(str(centerline_streets_csv))\n",
    "    if safety_centerline_wkt_col not in streets_df.columns:\n",
    "        raise ValueError(f\"Centerline WKT column '{safety_centerline_wkt_col}' not found in centerline CSV.\")\n",
    "    streets_df[\"geometry\"] = streets_df[safety_centerline_wkt_col].apply(\n",
    "        lambda x: wkt.loads(x) if isinstance(x, str) and x.startswith(\"LINESTRING\") else None\n",
    "    )\n",
    "    streets_gdf = gpd.GeoDataFrame(streets_df, geometry=\"geometry\", crs=safety_input_crs)\n",
    "    streets_gdf = streets_gdf[~streets_gdf[\"geometry\"].isna()].copy()\n",
    "    streets_gdf = streets_gdf.to_crs(blocks.crs)\n",
    "\n",
    "    bike_df = pd.read_csv(str(bike_lanes_csv))\n",
    "    if safety_bike_lane_wkt_col not in bike_df.columns:\n",
    "        raise ValueError(f\"Bike lane WKT column '{safety_bike_lane_wkt_col}' not found in bike lane CSV.\")\n",
    "    bike_df[\"geometry\"] = bike_df[safety_bike_lane_wkt_col].apply(\n",
    "        lambda x: wkt.loads(x) if isinstance(x, str) and x.startswith(\"LINESTRING\") else None\n",
    "    )\n",
    "    bike_gdf = gpd.GeoDataFrame(bike_df, geometry=\"geometry\", crs=safety_input_crs)\n",
    "    bike_gdf = bike_gdf[~bike_gdf[\"geometry\"].isna()].copy()\n",
    "    bike_gdf = bike_gdf.to_crs(blocks.crs)\n",
    "\n",
    "    def _norm_text(x: Any) -> str:\n",
    "        if pd.isna(x):\n",
    "            return \"\"\n",
    "        return str(x).strip().lower()\n",
    "\n",
    "    detected_class_col = None\n",
    "    detection_scores: List[Tuple[str, float]] = []\n",
    "\n",
    "    if safety_bike_lane_class_col is None:\n",
    "        text_cols = [c for c in bike_gdf.columns if bike_gdf[c].dtype == \"object\" and c != \"geometry\"]\n",
    "\n",
    "        def _score_col(col: str) -> float:\n",
    "            name_score = sum(1 for kw in safety_class_col_candidate_keywords if kw in col.lower())\n",
    "            sample = bike_gdf[col].dropna().astype(str).head(5000)\n",
    "            val_join = \" \".join(sample.str.lower().tolist())\n",
    "            val_score = sum(val_join.count(kw) for kw in safety_protected_keywords)\n",
    "            return float(name_score) + float(val_score) / 1000.0\n",
    "\n",
    "        for c in text_cols:\n",
    "            s = _score_col(c)\n",
    "            detection_scores.append((c, s))\n",
    "        detection_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        if detection_scores and detection_scores[0][1] > 0:\n",
    "            detected_class_col = detection_scores[0][0]\n",
    "            safety_bike_lane_class_col = detected_class_col\n",
    "\n",
    "    can_compute_protected = safety_bike_lane_class_col is not None and safety_bike_lane_class_col in bike_gdf.columns\n",
    "\n",
    "    protected_gdf = bike_gdf.iloc[0:0].copy()\n",
    "    inferred_protected_values = None\n",
    "\n",
    "    if can_compute_protected:\n",
    "        col_series = bike_gdf[safety_bike_lane_class_col].astype(str)\n",
    "        if safety_protected_values:\n",
    "            pv = [p.strip().lower() for p in safety_protected_values]\n",
    "            if safety_protected_match_mode == \"exact\":\n",
    "                mask = col_series.map(_norm_text).isin(pv)\n",
    "            else:\n",
    "                mask = col_series.map(lambda v: any(k in _norm_text(v) for k in pv))\n",
    "            protected_gdf = bike_gdf[mask].copy()\n",
    "            inferred_protected_values = pv\n",
    "        else:\n",
    "            kws = [k.strip().lower() for k in safety_protected_keywords]\n",
    "            mask = col_series.map(lambda v: any(k in _norm_text(v) for k in kws))\n",
    "            protected_gdf = bike_gdf[mask].copy()\n",
    "            inferred_protected_values = kws\n",
    "\n",
    "    streets_in_blocks = gpd.overlay(streets_gdf, blocks, how=\"intersection\")\n",
    "    streets_in_blocks[\"segment_length\"] = streets_in_blocks.geometry.length\n",
    "    street_len_block = (\n",
    "        streets_in_blocks.groupby(safety_census_block_id_col, as_index=False)\n",
    "        .agg(streets_leng=(\"segment_length\", \"sum\"))\n",
    "        .rename(columns={safety_census_block_id_col: \"census_block\"})\n",
    "    )\n",
    "    street_len_block[\"census_block\"] = street_len_block[\"census_block\"].astype(str)\n",
    "    street_len_block[\"streets_leng\"] = street_len_block[\"streets_leng\"].round(3)\n",
    "\n",
    "    bike_in_blocks = gpd.overlay(bike_gdf, blocks, how=\"intersection\")\n",
    "    bike_in_blocks[\"bike_lane_length\"] = bike_in_blocks.geometry.length\n",
    "    bike_len_block = (\n",
    "        bike_in_blocks.groupby(safety_census_block_id_col, as_index=False)\n",
    "        .agg(total_bike_lane_length=(\"bike_lane_length\", \"sum\"))\n",
    "        .rename(columns={safety_census_block_id_col: \"census_block\"})\n",
    "    )\n",
    "    bike_len_block[\"census_block\"] = bike_len_block[\"census_block\"].astype(str)\n",
    "    bike_len_block[\"total_bike_lane_length\"] = bike_len_block[\"total_bike_lane_length\"].round(3)\n",
    "\n",
    "    protected_len_block = pd.DataFrame({\"census_block\": [], \"protected_bike_lane_length\": []})\n",
    "    if can_compute_protected and not protected_gdf.empty:\n",
    "        protected_in_blocks = gpd.overlay(protected_gdf, blocks, how=\"intersection\")\n",
    "        protected_in_blocks[\"protected_lane_length\"] = protected_in_blocks.geometry.length\n",
    "        protected_len_block = (\n",
    "            protected_in_blocks.groupby(safety_census_block_id_col, as_index=False)\n",
    "            .agg(protected_bike_lane_length=(\"protected_lane_length\", \"sum\"))\n",
    "            .rename(columns={safety_census_block_id_col: \"census_block\"})\n",
    "        )\n",
    "        protected_len_block[\"census_block\"] = protected_len_block[\"census_block\"].astype(str)\n",
    "        protected_len_block[\"protected_bike_lane_length\"] = protected_len_block[\"protected_bike_lane_length\"].round(3)\n",
    "\n",
    "    safety_block = (\n",
    "        street_len_block.merge(bike_len_block, on=\"census_block\", how=\"left\")\n",
    "        .merge(protected_len_block, on=\"census_block\", how=\"left\")\n",
    "    )\n",
    "    safety_block[\"total_bike_lane_length\"] = safety_block[\"total_bike_lane_length\"].fillna(0.0)\n",
    "    safety_block[\"protected_bike_lane_length\"] = safety_block[\"protected_bike_lane_length\"].fillna(0.0)\n",
    "\n",
    "    safety_block[\"bike_lane_ratio\"] = safety_block.apply(\n",
    "        lambda r: (r[\"total_bike_lane_length\"] / r[\"streets_leng\"]) if r[\"streets_leng\"] > 0 else 0.0,\n",
    "        axis=1,\n",
    "    ).round(3)\n",
    "    safety_block[\"protected_bike_lane_ratio\"] = safety_block.apply(\n",
    "        lambda r: (r[\"protected_bike_lane_length\"] / r[\"streets_leng\"]) if r[\"streets_leng\"] > 0 else 0.0,\n",
    "        axis=1,\n",
    "    ).round(3)\n",
    "\n",
    "    ct = pd.read_csv(str(centroid_tract_csv))\n",
    "    if safety_centroid_tract_id_col not in ct.columns:\n",
    "        raise ValueError(f\"centroid_tract_csv missing '{safety_centroid_tract_id_col}'. Available: {list(ct.columns)}\")\n",
    "    ct[safety_centroid_tract_id_col] = ct[safety_centroid_tract_id_col].astype(str)\n",
    "\n",
    "    safety_block[\"census_tract\"] = safety_block[\"census_block\"].astype(str).str[:tract_digits]\n",
    "    tract_sums = safety_block.groupby(\"census_tract\", as_index=False)[\n",
    "        [\"streets_leng\", \"total_bike_lane_length\", \"protected_bike_lane_length\"]\n",
    "    ].sum()\n",
    "\n",
    "    keep_cols = [safety_centroid_tract_id_col]\n",
    "    if safety_centroid_area_col is not None and safety_centroid_area_col in ct.columns:\n",
    "        keep_cols.append(safety_centroid_area_col)\n",
    "\n",
    "    safety_tract = ct[keep_cols].merge(\n",
    "        tract_sums,\n",
    "        left_on=safety_centroid_tract_id_col,\n",
    "        right_on=\"census_tract\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    safety_tract[[\"streets_leng\", \"total_bike_lane_length\", \"protected_bike_lane_length\"]] = (\n",
    "        safety_tract[[\"streets_leng\", \"total_bike_lane_length\", \"protected_bike_lane_length\"]].fillna(0.0)\n",
    "    )\n",
    "\n",
    "    safety_tract[\"bike_lane_ratio\"] = safety_tract.apply(\n",
    "        lambda r: (r[\"total_bike_lane_length\"] / r[\"streets_leng\"]) if r[\"streets_leng\"] > 0 else 0.0,\n",
    "        axis=1,\n",
    "    )\n",
    "    safety_tract[\"protected_bike_lane_ratio\"] = safety_tract.apply(\n",
    "        lambda r: (r[\"protected_bike_lane_length\"] / r[\"streets_leng\"]) if r[\"streets_leng\"] > 0 else 0.0,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    safety_tract = safety_tract.sort_values(by=safety_centroid_tract_id_col).reset_index(drop=True)\n",
    "\n",
    "    safety_tract_norm = safety_tract.copy()\n",
    "    if safety_normalize:\n",
    "        safety_tract_norm[\"bike_lane_ratio_norm\"] = _minmax(safety_tract_norm[\"bike_lane_ratio\"]).round(5)\n",
    "        safety_tract_norm[\"protected_bike_lane_ratio_norm\"] = _minmax(safety_tract_norm[\"protected_bike_lane_ratio\"]).round(5)\n",
    "\n",
    "    if save_outputs:\n",
    "        safety_block.to_csv(out_dir / f\"safety_block_bike_lane_{output_tag}.csv\", index=False)\n",
    "        safety_tract.to_csv(out_dir / f\"safety_bike_lane_tract_{output_tag}.csv\", index=False)\n",
    "        safety_tract_norm.to_csv(out_dir / f\"safety_bike_lane_norm_tract_{output_tag}.csv\", index=False)\n",
    "\n",
    "    results[\"safety\"] = {\n",
    "        \"safety_block_df\": safety_block,\n",
    "        \"safety_tract_df\": safety_tract,\n",
    "        \"safety_tract_norm_df\": safety_tract_norm,\n",
    "        \"meta\": {\n",
    "            \"working_crs\": str(blocks.crs),\n",
    "            \"bike_lane_class_col_used\": safety_bike_lane_class_col,\n",
    "            \"bike_lane_class_col_detected\": detected_class_col,\n",
    "            \"protected_match_mode\": safety_protected_match_mode,\n",
    "            \"protected_values_used\": inferred_protected_values,\n",
    "            \"detection_scores_top5\": detection_scores[:5],\n",
    "            \"normalize\": safety_normalize,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e16ae70",
   "metadata": {},
   "source": [
    "# **Improved one**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88649f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, Dict, Any, List\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely import wkt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def run_sf_dockless_all_utilities_single_function(\n",
    "    *,\n",
    "    # ------------------------------------------------------------\n",
    "    # USER INPUTS\n",
    "    # ------------------------------------------------------------\n",
    "    system_key: str,\n",
    "    freebike_status_txt: Union[str, Path],\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # CITY ASSETS\n",
    "    # ------------------------------------------------------------\n",
    "    census_blocks_shp: Optional[Union[str, Path]] = None,\n",
    "    centerline_streets_path: Optional[Union[str, Path]] = None,\n",
    "    bike_lanes_path: Optional[Union[str, Path]] = None,\n",
    "    planned_bike_lanes_path: Optional[Union[str, Path]] = None,\n",
    "    centroid_tract_path: Optional[Union[str, Path]] = None,\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    # OUTPUT\n",
    "    # ------------------------------------------------------------\n",
    "    output_dir: Optional[Union[str, Path]] = None,\n",
    "    save_outputs: bool = True,\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # TIME WINDOW\n",
    "    # ------------------------------------------------------------\n",
    "    time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # CONFIGURATION\n",
    "    # ------------------------------------------------------------\n",
    "    step0_raw_csv_name: Optional[str] = None,\n",
    "    step0_done_csv_name: Optional[str] = None,\n",
    "    blocks_geoid_col: str = \"GEOID20\",\n",
    "    blocks_target_epsg: int = 4326,\n",
    "    fill_missing_with_census_api: bool = True,\n",
    "    census_benchmark: str = \"Public_AR_Census2020\",\n",
    "    census_vintage: str = \"2020\",\n",
    "    timestamp_parse_mode: str = \"auto\",\n",
    "    drop_cols_if_present: Optional[List[str]] = None,\n",
    "    \n",
    "    # Availability\n",
    "    availability_block_time_granularity: str = \"5min\",\n",
    "    availability_tract_time_granularity: str = \"1H\",\n",
    "    reserved_col: str = \"is_reserved\",\n",
    "    disabled_col: str = \"is_disabled\",\n",
    "    vehicle_id_col: str = \"bike_id\",\n",
    "    include_vehicle_type: bool = True,\n",
    "    vehicle_type_col_avail: str = \"vehicle_type_id\",\n",
    "    tract_digits: int = 11,\n",
    "    availability_normalize: bool = True,\n",
    "\n",
    "    # Usage\n",
    "    usage_base_time_slot: str = \"5min\",\n",
    "    usage_aggregate_time_slot: str = \"1H\",\n",
    "    usage_rounding_decimals: int = 4,\n",
    "    usage_normalize: bool = True,\n",
    "\n",
    "    # Idle Time\n",
    "    idle_base_time_slot: str = \"5min\",\n",
    "    idle_hour_bucket_freq: str = \"1H\",\n",
    "    idle_rounding_decimals: int = 4,\n",
    "    idle_vehicle_id_col: str = \"bike_id\",\n",
    "    idle_lat_col: str = \"lat\",\n",
    "    idle_lon_col: str = \"lon\",\n",
    "    idle_census_block_col: str = \"census_block\",\n",
    "    idle_vehicle_type_col: str = \"vehicle_type_id\",\n",
    "    idle_default_vehicle_type_for_missing: str = \"\", \n",
    "    idle_normalize: bool = True,\n",
    "\n",
    "    # Safety\n",
    "    safety_centerline_wkt_col: str = \"line\",\n",
    "    safety_bike_lane_wkt_col: str = \"shape\",\n",
    "    safety_input_crs: str = \"EPSG:4326\",\n",
    "    safety_census_block_id_col: str = \"GEOID20\",\n",
    "    safety_centroid_tract_id_col: str = \"census_tract\",\n",
    "    safety_centroid_area_col: Optional[str] = \"county_name\",\n",
    "    safety_working_epsg: Optional[int] = None,\n",
    "    safety_bike_lane_class_col: Optional[str] = None,\n",
    "    safety_protected_values: Optional[List[str]] = None,\n",
    "    safety_protected_keywords: Optional[List[str]] = None,\n",
    "    safety_protected_match_mode: str = \"contains\",\n",
    "    safety_normalize: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    # ============================================================\n",
    "    # 1. System Presets\n",
    "    # ============================================================\n",
    "    SYSTEMS: Dict[str, Dict[str, Any]] = {\n",
    "        \"SF_LIME_DOCKLESS\": {\n",
    "            \"city\": \"San Francisco\", \"vendor\": \"Lime\", \"tag\": \"sf_lime\",\n",
    "            \"default_output_dir\": \"SF_LIME_DOCKLESS_FULL_RUN\",\n",
    "            \"raw_csv\": \"san_fran_lime_status_raw.csv\", \"done_csv\": \"san_fran_lime_status_done.csv\",\n",
    "            \"assets\": {\n",
    "                \"census_blocks_shp\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\GBFS_Census_Tract\\San_Fran_Lime\\tl_2024_06_tabblock20.shp\",\n",
    "                \"centerline_streets_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\San_Fran_Lime\\Centerline.csv\",\n",
    "                \"bike_lanes_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\San_Fran_Lime\\Bikelane.csv\",\n",
    "                \"centroid_tract_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\San_Fran_Baywheels\\centroid_tract_ca.csv\",\n",
    "            },\n",
    "            \"safety_epsg\": 26910, \"idle_decimals\": 4\n",
    "        },\n",
    "        \"SF_SPIN_DOCKLESS\": {\n",
    "            \"city\": \"San Francisco\", \"vendor\": \"Spin\", \"tag\": \"sf_spin\",\n",
    "            \"default_output_dir\": \"SF_SPIN_DOCKLESS_FULL_RUN\",\n",
    "            \"raw_csv\": \"san_fran_spin_status_raw.csv\", \"done_csv\": \"san_fran_spin_status_done.csv\",\n",
    "            \"assets\": {\n",
    "                \"census_blocks_shp\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\GBFS_Census_Tract\\San_Fran_Lime\\tl_2024_06_tabblock20.shp\",\n",
    "                \"centerline_streets_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\San_Fran_Lime\\Centerline.csv\",\n",
    "                \"bike_lanes_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\San_Fran_Lime\\Bikelane.csv\",\n",
    "                \"centroid_tract_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\San_Fran_Baywheels\\centroid_tract_ca.csv\",\n",
    "            },\n",
    "            \"safety_epsg\": 26910, \"idle_decimals\": 4\n",
    "        },\n",
    "        \"SEATTLE_BIRD_DOCKLESS\": {\n",
    "            \"city\": \"Seattle\", \"vendor\": \"Bird\", \"tag\": \"seattle_bird\",\n",
    "            \"default_output_dir\": \"SEATTLE_BIRD_DOCKLESS_FULL_RUN\",\n",
    "            \"raw_csv\": \"seattle_bird_status_raw.csv\", \"done_csv\": \"seattle_bird_status_done.csv\",\n",
    "            \"assets\": {\n",
    "                \"census_blocks_shp\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Seattle_Bird\\Seattle Census Block\\tl_2024_53_tabblock20.shp\",\n",
    "                \"centerline_streets_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Seattle_Bird\\Seattle_Streets.shp\",\n",
    "                \"bike_lanes_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Seattle_Bird\\SDOT_Bike_Facilities_5512142703833213564.geojson\",\n",
    "                \"planned_bike_lanes_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Seattle_Bird\\Planned Seattle Bike Lanes\\Planned_Bike_Facilities.shp\",\n",
    "                \"centroid_tract_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Avalibility\\Seattle_Bird\\tl_2024_53_tract.shp\",\n",
    "            },\n",
    "            \"safety_epsg\": 2285, \n",
    "            \"safety_config\": {\"bike_lane_class_col\": \"CATEGORY\", \"protected_values\": [\"BKF-PBL\"], \"protected_match_mode\": \"exact\"},\n",
    "            \"idle_decimals\": 3 \n",
    "        },\n",
    "        \"SEATTLE_LIME_DOCKLESS\": {\n",
    "            \"city\": \"Seattle\", \"vendor\": \"Lime\", \"tag\": \"seattle_lime\",\n",
    "            \"default_output_dir\": \"SEATTLE_LIME_DOCKLESS_FULL_RUN\",\n",
    "            \"raw_csv\": \"seattle_lime_status_raw.csv\", \"done_csv\": \"seattle_lime_status_done.csv\",\n",
    "            \"assets\": {\n",
    "                \"census_blocks_shp\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Seattle_Bird\\Seattle Census Block\\tl_2024_53_tabblock20.shp\",\n",
    "                \"centerline_streets_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Seattle_Bird\\Seattle_Streets.shp\",\n",
    "                \"bike_lanes_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Seattle_Bird\\SDOT_Bike_Facilities_5512142703833213564.geojson\",\n",
    "                \"planned_bike_lanes_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Seattle_Bird\\Planned Seattle Bike Lanes\\Planned_Bike_Facilities.shp\",\n",
    "                \"centroid_tract_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Avalibility\\Seattle_Bird\\tl_2024_53_tract.shp\",\n",
    "            },\n",
    "            \"safety_epsg\": 2285,\n",
    "            \"safety_config\": {\"bike_lane_class_col\": \"CATEGORY\", \"protected_values\": [\"BKF-PBL\"], \"protected_match_mode\": \"exact\"},\n",
    "            \"idle_decimals\": 4\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if system_key not in SYSTEMS: raise ValueError(f\"Unknown system_key: {system_key}\")\n",
    "    preset = SYSTEMS[system_key]\n",
    "    output_tag = preset[\"tag\"]\n",
    "    \n",
    "    # Initialize Paths and Defaults\n",
    "    output_dir = output_dir or preset[\"default_output_dir\"]\n",
    "    step0_raw_csv_name = preset[\"raw_csv\"]\n",
    "    step0_done_csv_name = preset[\"done_csv\"]\n",
    "    p_assets = preset.get(\"assets\", {})\n",
    "    \n",
    "    census_blocks_shp = census_blocks_shp or p_assets.get(\"census_blocks_shp\")\n",
    "    centerline_streets_path = centerline_streets_path or p_assets.get(\"centerline_streets_path\")\n",
    "    bike_lanes_path = bike_lanes_path or p_assets.get(\"bike_lanes_path\")\n",
    "    planned_bike_lanes_path = planned_bike_lanes_path or p_assets.get(\"planned_bike_lanes_path\")\n",
    "    centroid_tract_path = centroid_tract_path or p_assets.get(\"centroid_tract_path\")\n",
    "    \n",
    "    safety_working_epsg = safety_working_epsg or preset.get(\"safety_epsg\", 26910)\n",
    "    p_safe_conf = preset.get(\"safety_config\", {})\n",
    "    if safety_bike_lane_class_col is None: safety_bike_lane_class_col = p_safe_conf.get(\"bike_lane_class_col\")\n",
    "    if safety_protected_values is None: safety_protected_values = p_safe_conf.get(\"protected_values\")\n",
    "    if \"protected_match_mode\" in p_safe_conf: safety_protected_match_mode = p_safe_conf[\"protected_match_mode\"]\n",
    "\n",
    "    p_idle_conf = preset.get(\"idle_config\", {})\n",
    "    if \"decimals\" in p_idle_conf: idle_rounding_decimals = p_idle_conf[\"decimals\"]\n",
    "\n",
    "    out_dir = Path(output_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if not Path(freebike_status_txt).exists(): raise FileNotFoundError(f\"Missing: {freebike_status_txt}\")\n",
    "    \n",
    "    raw_path = out_dir / step0_raw_csv_name\n",
    "    done_path = out_dir / step0_done_csv_name\n",
    "\n",
    "    results: Dict[str, Any] = {\"system_key\": system_key, \"city\": preset[\"city\"], \"vendor\": preset[\"vendor\"], \"output_dir\": str(out_dir)}\n",
    "\n",
    "    def _minmax(series: pd.Series) -> pd.Series:\n",
    "        s = pd.to_numeric(series, errors=\"coerce\")\n",
    "        mn, mx = s.min(skipna=True), s.max(skipna=True)\n",
    "        if pd.isna(mn) or pd.isna(mx) or mx <= mn: return pd.Series(0.0, index=s.index)\n",
    "        return (s - mn) / (mx - mn)\n",
    "    \n",
    "    # ============================================================\n",
    "    # STEP 0: PARSING\n",
    "    # ============================================================\n",
    "    rows = []\n",
    "    with Path(freebike_status_txt).open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip(): continue\n",
    "            try:\n",
    "                blob = json.loads(line)\n",
    "                ts = list(blob.keys())[0]\n",
    "                for entry in blob[ts]:\n",
    "                    if \"vehicle_types_available\" in entry:\n",
    "                        for vt in entry.get(\"vehicle_types_available\", []):\n",
    "                            entry[f\"vehicle_type_{vt.get('vehicle_type_id', 'unknown')}_count\"] = vt.get(\"count\", 0)\n",
    "                        del entry[\"vehicle_types_available\"]\n",
    "                    entry[\"timestamp\"] = ts\n",
    "                    rows.append(entry)\n",
    "            except: continue\n",
    "\n",
    "    raw_df = pd.DataFrame(rows)\n",
    "    if raw_df.empty: raise ValueError(\"No data found in text file.\")\n",
    "    \n",
    "    # ID Column Detection\n",
    "    id_candidates = [\"bike_id\", \"vehicle_id\", \"id\"]\n",
    "    vehicle_id_col = next((c for c in id_candidates if c in raw_df.columns), None)\n",
    "    if not vehicle_id_col: raise ValueError(f\"Could not find vehicle ID. Checked: {id_candidates}\")\n",
    "    \n",
    "    raw_df[\"timestamp\"] = pd.to_datetime(raw_df[\"timestamp\"], errors=\"coerce\")\n",
    "    if raw_df[\"timestamp\"].isna().mean() > 0.5:\n",
    "        raw_df[\"timestamp\"] = pd.to_datetime(raw_df[\"timestamp\"], unit=\"s\", errors=\"coerce\")\n",
    "\n",
    "    if drop_cols_if_present:\n",
    "        drop_now = [c for c in drop_cols_if_present if c in raw_df.columns]\n",
    "        if drop_now: raw_df = raw_df.drop(columns=drop_now)\n",
    "\n",
    "    if save_outputs: raw_df.to_csv(raw_path, index=False)\n",
    "\n",
    "    # Spatial Join\n",
    "    if not {\"lat\", \"lon\"}.issubset(raw_df.columns): raise ValueError(\"Input missing lat/lon\")\n",
    "    latlon = raw_df[[\"lat\", \"lon\"]].drop_duplicates()\n",
    "    gdf = gpd.GeoDataFrame(latlon, geometry=[Point(xy) for xy in zip(latlon.lon, latlon.lat)], crs=\"EPSG:4326\")\n",
    "    blocks = gpd.read_file(str(census_blocks_shp)).to_crs(epsg=blocks_target_epsg)\n",
    "    \n",
    "    b_col = \"GEOID20\" if \"GEOID20\" in blocks.columns else \"GEOID\"\n",
    "    joined = gpd.sjoin(gdf, blocks[[b_col, \"geometry\"]], how=\"left\", predicate=\"within\")\n",
    "    joined = joined.rename(columns={b_col: \"census_block\"})\n",
    "    \n",
    "    done_df = raw_df.merge(joined[[\"lat\", \"lon\", \"census_block\"]], on=[\"lat\", \"lon\"], how=\"left\")\n",
    "    \n",
    "    if fill_missing_with_census_api:\n",
    "        missing = done_df[done_df[\"census_block\"].isna()][[\"lat\", \"lon\"]].drop_duplicates()\n",
    "        if not missing.empty and len(missing) < 1000:\n",
    "            tqdm.pandas(desc=\"API Fill\")\n",
    "            def get_blk(r):\n",
    "                try:\n",
    "                    url = f\"https://geocoding.geo.census.gov/geocoder/geographies/coordinates?x={r.lon}&y={r.lat}&benchmark={census_benchmark}&vintage={census_vintage}&format=json\"\n",
    "                    res = requests.get(url, timeout=5).json()\n",
    "                    return res[\"result\"][\"geographies\"][\"2020 Census Blocks\"][0][\"GEOID\"]\n",
    "                except: return None\n",
    "            missing[\"cb_new\"] = missing.progress_apply(get_blk, axis=1)\n",
    "            done_df = done_df.merge(missing, on=[\"lat\", \"lon\"], how=\"left\")\n",
    "            done_df[\"census_block\"] = done_df[\"census_block\"].fillna(done_df[\"cb_new\"])\n",
    "            done_df.drop(columns=[\"cb_new\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    done_df[\"census_block\"] = done_df[\"census_block\"].fillna(\"unknown\").astype(str)\n",
    "    done_df[\"census_tract\"] = done_df[\"census_block\"].str[:tract_digits]\n",
    "    \n",
    "    if time_start: done_df = done_df[done_df[\"timestamp\"] >= pd.to_datetime(time_start)]\n",
    "    if time_end: done_df = done_df[done_df[\"timestamp\"] < pd.to_datetime(time_end)]\n",
    "    if save_outputs: done_df.to_csv(done_path, index=False)\n",
    "\n",
    "    # ============================================================\n",
    "    # AVAILABILITY\n",
    "    # ============================================================\n",
    "    df_av = done_df.copy()\n",
    "    df_av[\"slot\"] = df_av[\"timestamp\"].dt.floor(availability_block_time_granularity)\n",
    "    if \"is_reserved\" not in df_av.columns: df_av[\"is_reserved\"] = 0\n",
    "    if \"is_disabled\" not in df_av.columns: df_av[\"is_disabled\"] = 0\n",
    "    av = df_av[(df_av[\"is_reserved\"]==0) & (df_av[\"is_disabled\"]==0)]\n",
    "    \n",
    "    av_blk = av.groupby([\"census_block\", \"slot\"]).size().reset_index(name=\"total_available\")\n",
    "    av[\"h_slot\"] = av[\"timestamp\"].dt.floor(availability_tract_time_granularity)\n",
    "    av_tract = av.groupby([\"census_tract\", \"h_slot\"]).size().reset_index(name=\"total_available\")\n",
    "    if availability_normalize:\n",
    "        av_tract[\"total_available_norm\"] = _minmax(av_tract[\"total_available\"]).round(5)\n",
    "\n",
    "    if save_outputs:\n",
    "        av_blk.to_csv(out_dir / f\"availability_block_{output_tag}.csv\", index=False)\n",
    "        av_tract.to_csv(out_dir / f\"availability_tract_hourly_raw_{output_tag}.csv\", index=False)\n",
    "\n",
    "    # ============================================================\n",
    "    # USAGE\n",
    "    # ============================================================\n",
    "    df_u = done_df.copy()\n",
    "    df_u[\"slot\"] = df_u[\"timestamp\"].dt.floor(usage_base_time_slot)\n",
    "    df_u[\"lat_r\"] = df_u[\"lat\"].round(usage_rounding_decimals)\n",
    "    df_u[\"lon_r\"] = df_u[\"lon\"].round(usage_rounding_decimals)\n",
    "    \n",
    "    cnts = df_u.groupby([\"census_block\", \"slot\", \"lat_r\", \"lon_r\"]).size().reset_index(name=\"cnt\")\n",
    "    prev = cnts.copy()\n",
    "    prev[\"slot\"] += pd.to_timedelta(usage_base_time_slot)\n",
    "    \n",
    "    flux = prev.merge(cnts, on=[\"census_block\", \"slot\", \"lat_r\", \"lon_r\"], how=\"outer\", suffixes=(\"_prev\", \"_curr\")).fillna(0)\n",
    "    flux[\"starts\"] = (flux[\"cnt_prev\"] - flux[\"cnt_curr\"]).clip(lower=0)\n",
    "    flux[\"ends\"] = (flux[\"cnt_curr\"] - flux[\"cnt_prev\"]).clip(lower=0)\n",
    "    \n",
    "    use_blk = flux.groupby([\"census_block\", \"slot\"])[[\"starts\", \"ends\"]].sum().reset_index()\n",
    "    use_blk[\"h_slot\"] = use_blk[\"slot\"].dt.floor(usage_aggregate_time_slot)\n",
    "    use_blk[\"census_tract\"] = use_blk[\"census_block\"].str[:tract_digits]\n",
    "    use_tract = use_blk.groupby([\"census_tract\", \"h_slot\"])[[\"starts\", \"ends\"]].sum().reset_index()\n",
    "    \n",
    "    if save_outputs:\n",
    "        use_blk.to_csv(out_dir / f\"usage_5min_block_{output_tag}.csv\", index=False)\n",
    "        use_tract.to_csv(out_dir / f\"usage_hourly_tract_{output_tag}.csv\", index=False)\n",
    "\n",
    "    # ============================================================\n",
    "    # IDLE TIME (Dynamic Column Detection)\n",
    "    # ============================================================\n",
    "    df_i = done_df.copy()\n",
    "    \n",
    "    # --- AUTO-DETECT IDLE COLUMN ---\n",
    "    # 1. Check if the default/passed 'idle_vehicle_type_col' exists in DF\n",
    "    # 2. If NOT, checks if \"vehicle_type\" exists (common variant for Seattle Lime)\n",
    "    # 3. If NOT, falls back to creating a dummy column\n",
    "    \n",
    "    if idle_vehicle_type_col not in df_i.columns:\n",
    "        if \"vehicle_type\" in df_i.columns:\n",
    "            idle_vehicle_type_col = \"vehicle_type\"\n",
    "        else:\n",
    "            # Create default dummy col\n",
    "            idle_vehicle_type_col = \"vehicle_type_id\"\n",
    "            df_i[idle_vehicle_type_col] = idle_default_vehicle_type_for_missing\n",
    "    \n",
    "    # Ensure no NaNs in the chosen grouping column\n",
    "    df_i[idle_vehicle_type_col] = df_i[idle_vehicle_type_col].fillna(idle_default_vehicle_type_for_missing)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    df_i = df_i.sort_values([idle_vehicle_id_col, \"timestamp\"]).reset_index(drop=True)\n",
    "    \n",
    "    # Rounding\n",
    "    df_i[\"lat_r\"] = df_i[idle_lat_col].round(idle_rounding_decimals)\n",
    "    df_i[\"lon_r\"] = df_i[idle_lon_col].round(idle_rounding_decimals)\n",
    "    df_i[\"h_bucket\"] = df_i[\"timestamp\"].dt.floor(idle_hour_bucket_freq)\n",
    "    \n",
    "    # 3. ROBUST CALCULATION: \"Presence is Idle\" logic\n",
    "    idle_res = df_i.groupby([\"census_block\", \"h_bucket\", idle_vehicle_type_col]).size().reset_index(name=\"ping_count\")\n",
    "    \n",
    "    # Multiplier: Assuming data is 5min snapshots\n",
    "    snapshot_interval_min = 5 \n",
    "    idle_res[\"avg_idle_time_minutes\"] = idle_res[\"ping_count\"] * snapshot_interval_min\n",
    "    idle_res[\"num_idle_segments\"] = idle_res[\"ping_count\"] \n",
    "    \n",
    "    idle_res[\"census_tract\"] = idle_res[\"census_block\"].str[:tract_digits]\n",
    "    idle_tract = idle_res.groupby([\"census_tract\", \"h_bucket\", idle_vehicle_type_col])[[\"avg_idle_time_minutes\", \"num_idle_segments\"]].sum().reset_index()\n",
    "    \n",
    "    if idle_normalize:\n",
    "        idle_tract[\"avg_idle_time_minutes_norm\"] = _minmax(idle_tract[\"avg_idle_time_minutes\"]).round(5)\n",
    "\n",
    "    if save_outputs:\n",
    "        idle_res.to_csv(out_dir / f\"idle_summary_block_{output_tag}.csv\", index=False)\n",
    "        idle_tract.to_csv(out_dir / f\"idle_summary_tract_{output_tag}.csv\", index=False)\n",
    "        idle_tract.to_csv(out_dir / f\"idle_summary_tract_norm_{output_tag}.csv\", index=False)\n",
    "\n",
    "    # ============================================================\n",
    "    # SAFETY\n",
    "    # ============================================================\n",
    "    blocks = gpd.read_file(str(census_blocks_shp))\n",
    "    if safety_working_epsg: blocks = blocks.to_crs(epsg=int(safety_working_epsg))\n",
    "    \n",
    "    # Smart Load Streets\n",
    "    st_path = Path(centerline_streets_path)\n",
    "    if st_path.suffix.lower() == \".csv\":\n",
    "        st_df = pd.read_csv(st_path)\n",
    "        st_df[\"geometry\"] = st_df[safety_centerline_wkt_col].apply(lambda x: wkt.loads(x) if isinstance(x, str) else None)\n",
    "        st = gpd.GeoDataFrame(st_df, geometry=\"geometry\", crs=safety_input_crs)\n",
    "    else: st = gpd.read_file(st_path).to_crs(safety_input_crs)\n",
    "    st = st.to_crs(blocks.crs)\n",
    "    \n",
    "    # Smart Load Bike Lanes\n",
    "    bl_path = Path(bike_lanes_path)\n",
    "    if bl_path.suffix.lower() == \".csv\":\n",
    "        bl_df = pd.read_csv(bl_path)\n",
    "        bl_df[\"geometry\"] = bl_df[safety_bike_lane_wkt_col].apply(lambda x: wkt.loads(x) if isinstance(x, str) else None)\n",
    "        bl = gpd.GeoDataFrame(bl_df, geometry=\"geometry\", crs=safety_input_crs)\n",
    "    else: bl = gpd.read_file(bl_path).to_crs(safety_input_crs)\n",
    "    \n",
    "    if planned_bike_lanes_path:\n",
    "        pl = gpd.read_file(str(planned_bike_lanes_path)).to_crs(safety_input_crs)\n",
    "        bl = pd.concat([bl, pl], ignore_index=True)\n",
    "    bl = bl.to_crs(blocks.crs)\n",
    "\n",
    "    # Protected Logic\n",
    "    conf = preset.get(\"safety_config\", {})\n",
    "    col = conf.get(\"bike_lane_class_col\", \"class\")\n",
    "    vals = conf.get(\"protected_values\", [])\n",
    "    \n",
    "    prot = bl.iloc[0:0]\n",
    "    if col in bl.columns and vals:\n",
    "        prot = bl[bl[col].isin(vals)]\n",
    "\n",
    "    # Intersections\n",
    "    def get_len(lines, poly, name):\n",
    "        if lines.empty: return pd.DataFrame({\"census_block\": [], name: []})\n",
    "        c = gpd.overlay(lines, poly, how=\"intersection\")\n",
    "        c[\"l\"] = c.geometry.length\n",
    "        bid = \"GEOID20\" if \"GEOID20\" in poly.columns else \"GEOID\"\n",
    "        return c.groupby(bid)[\"l\"].sum().reset_index(name=name).rename(columns={bid: \"census_block\"})\n",
    "\n",
    "    s_len = get_len(st, blocks, \"st_len\")\n",
    "    b_len = get_len(bl, blocks, \"bl_len\")\n",
    "    p_len = get_len(prot, blocks, \"pr_len\")\n",
    "    \n",
    "    safe = s_len.merge(b_len, on=\"census_block\", how=\"left\").merge(p_len, on=\"census_block\", how=\"left\").fillna(0)\n",
    "    safe[\"census_tract\"] = safe[\"census_block\"].astype(str).str[:tract_digits]\n",
    "    \n",
    "    # Tract Agg\n",
    "    ct_path = Path(centroid_tract_path)\n",
    "    if ct_path.suffix.lower() == \".csv\":\n",
    "        ct = pd.read_csv(ct_path)\n",
    "        ct[\"census_tract\"] = ct[\"census_tract\"].astype(str)\n",
    "        base = ct[[\"census_tract\"]]\n",
    "    else:\n",
    "        ct = gpd.read_file(ct_path)\n",
    "        cid = \"GEOID\" if \"GEOID\" in ct.columns else \"census_tract\"\n",
    "        base = pd.DataFrame({\"census_tract\": ct[cid].astype(str)})\n",
    "\n",
    "    tract_safe = safe.groupby(\"census_tract\")[[\"st_len\", \"bl_len\", \"pr_len\"]].sum().reset_index()\n",
    "    final = base.merge(tract_safe, on=\"census_tract\", how=\"left\").fillna(0)\n",
    "    \n",
    "    final[\"ratio_bl\"] = np.where(final[\"st_len\"]>0, final[\"bl_len\"]/final[\"st_len\"], 0)\n",
    "    final[\"ratio_pr\"] = np.where(final[\"st_len\"]>0, final[\"pr_len\"]/final[\"st_len\"], 0)\n",
    "    \n",
    "    if save_outputs:\n",
    "        safe.to_csv(out_dir / f\"safety_block_{output_tag}.csv\", index=False)\n",
    "        final.to_csv(out_dir / f\"safety_tract_{output_tag}.csv\", index=False)\n",
    "\n",
    "    results[\"safety\"] = {\"tract\": final}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbd003e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_37328\\4267028275.py:229: FutureWarning: Parsed string \"2025-06-09 00:01:41 CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  raw_df[\"timestamp\"] = pd.to_datetime(raw_df[\"timestamp\"], errors=\"coerce\")\n",
      "API Fill: 100%|██████████| 7/7 [00:06<00:00,  1.07it/s]\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_37328\\4267028275.py:283: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  av[\"h_slot\"] = av[\"timestamp\"].dt.floor(availability_tract_time_granularity)\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_37328\\4267028275.py:283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  av[\"h_slot\"] = av[\"timestamp\"].dt.floor(availability_tract_time_granularity)\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_37328\\4267028275.py:309: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  use_blk[\"h_slot\"] = use_blk[\"slot\"].dt.floor(usage_aggregate_time_slot)\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_37328\\4267028275.py:345: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df_i[\"h_bucket\"] = df_i[\"timestamp\"].dt.floor(idle_hour_bucket_freq)\n",
      "c:\\Users\\Vedant\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyogrio\\raw.py:198: UserWarning: Measured (M) geometry types are not supported. Original type 'Measured LineString' is converted to 'LineString'\n",
      "  return ogr_read(\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_37328\\4267028275.py:391: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  bl = pd.concat([bl, pl], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Example Call: Seattle Bird Dockless\n",
    "# ------------------------------------------------------------\n",
    "seattle_results = run_sf_dockless_all_utilities_single_function(\n",
    "    system_key=\"SEATTLE_LIME_DOCKLESS\",\n",
    "    \n",
    "    # Path to your raw GBFS status file (Change filename as needed)\n",
    "    freebike_status_txt=r\"D:\\Research Fellowship\\Summer Research Stuff\\Collected Data\\Week 1\\09-June\\seattle_lime_dkless_freebike_status_6_9.txt\",\n",
    "    \n",
    "    # Where you want the files saved\n",
    "    output_dir=\"SEATTLE_LIME_FULL_RUN\",\n",
    "    \n",
    "    # Time window for analysis\n",
    "    time_start=\"2025-06-09 06:00:00\",\n",
    "    time_end=\"2025-06-09 12:00:00\",\n",
    "    \n",
    "    # NOTE: \n",
    "    # Because we added the paths to the 'SYSTEMS' dictionary inside the function,\n",
    "    # you DO NOT need to pass census_blocks_shp, centerline_streets_path, etc. here.\n",
    "    # It will automatically use the D:\\ paths you provided.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aa19b1",
   "metadata": {},
   "source": [
    "# **Latest Version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9cbe91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, Dict, Any, List\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely import wkt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def run_sf_dockless_all_utilities_single_function(\n",
    "    *,\n",
    "    # ------------------------------------------------------------\n",
    "    # USER INPUTS\n",
    "    # ------------------------------------------------------------\n",
    "    system_key: str,\n",
    "    freebike_status_txt: Union[str, Path],\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # CITY ASSETS\n",
    "    # ------------------------------------------------------------\n",
    "    census_blocks_shp: Optional[Union[str, Path]] = None,\n",
    "    centerline_streets_path: Optional[Union[str, Path]] = None,\n",
    "    bike_lanes_path: Optional[Union[str, Path]] = None,\n",
    "    planned_bike_lanes_path: Optional[Union[str, Path]] = None,\n",
    "    centroid_tract_path: Optional[Union[str, Path]] = None,\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    # OUTPUT\n",
    "    # ------------------------------------------------------------\n",
    "    output_dir: Optional[Union[str, Path]] = None,\n",
    "    save_outputs: bool = True,\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # TIME WINDOW\n",
    "    # ------------------------------------------------------------\n",
    "    time_start: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    time_end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # CONFIGURATION\n",
    "    # ------------------------------------------------------------\n",
    "    step0_raw_csv_name: Optional[str] = None,\n",
    "    step0_done_csv_name: Optional[str] = None,\n",
    "    blocks_geoid_col: str = \"GEOID20\",\n",
    "    blocks_target_epsg: int = 4326,\n",
    "    fill_missing_with_census_api: bool = True,\n",
    "    census_benchmark: str = \"Public_AR_Census2020\",\n",
    "    census_vintage: str = \"2020\",\n",
    "    timestamp_parse_mode: str = \"auto\",\n",
    "    drop_cols_if_present: Optional[List[str]] = None,\n",
    "    \n",
    "    # Availability\n",
    "    availability_block_time_granularity: str = \"5min\",\n",
    "    availability_tract_time_granularity: str = \"1h\", # Fixed Warning\n",
    "    reserved_col: str = \"is_reserved\",\n",
    "    disabled_col: str = \"is_disabled\",\n",
    "    vehicle_id_col: str = \"bike_id\",\n",
    "    include_vehicle_type: bool = True,\n",
    "    vehicle_type_col_avail: str = \"vehicle_type_id\",\n",
    "    tract_digits: int = 11,\n",
    "    availability_normalize: bool = True,\n",
    "\n",
    "    # Usage\n",
    "    usage_base_time_slot: str = \"5min\",\n",
    "    usage_aggregate_time_slot: str = \"1h\", # Fixed Warning\n",
    "    usage_rounding_decimals: int = 4,\n",
    "    usage_normalize: bool = True,\n",
    "\n",
    "    # Idle Time\n",
    "    idle_base_time_slot: str = \"5min\",\n",
    "    idle_hour_bucket_freq: str = \"1h\", # Fixed Warning\n",
    "    idle_rounding_decimals: int = 4,\n",
    "    idle_vehicle_id_col: str = \"bike_id\",\n",
    "    idle_lat_col: str = \"lat\",\n",
    "    idle_lon_col: str = \"lon\",\n",
    "    idle_census_block_col: str = \"census_block\",\n",
    "    idle_vehicle_type_col: str = \"vehicle_type_id\",\n",
    "    idle_default_vehicle_type_for_missing: str = \"\", \n",
    "    idle_normalize: bool = True,\n",
    "\n",
    "    # Safety\n",
    "    safety_centerline_wkt_col: str = \"line\",\n",
    "    safety_bike_lane_wkt_col: str = \"shape\",\n",
    "    safety_input_crs: str = \"EPSG:4326\",\n",
    "    safety_census_block_id_col: str = \"GEOID20\",\n",
    "    safety_centroid_tract_id_col: str = \"census_tract\",\n",
    "    safety_centroid_area_col: Optional[str] = \"county_name\",\n",
    "    safety_working_epsg: Optional[int] = None,\n",
    "    safety_bike_lane_class_col: Optional[str] = None,\n",
    "    safety_protected_values: Optional[List[str]] = None,\n",
    "    safety_protected_keywords: Optional[List[str]] = None,\n",
    "    safety_protected_match_mode: str = \"contains\",\n",
    "    safety_normalize: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    # ============================================================\n",
    "    # 1. System Presets\n",
    "    # ============================================================\n",
    "    SYSTEMS: Dict[str, Dict[str, Any]] = {\n",
    "        \"SF_LIME_DOCKLESS\": {\n",
    "            \"city\": \"San Francisco\", \"vendor\": \"Lime\", \"tag\": \"sf_lime\",\n",
    "            \"default_output_dir\": \"SF_LIME_DOCKLESS_FULL_RUN\",\n",
    "            \"raw_csv\": \"san_fran_lime_status_raw.csv\", \"done_csv\": \"san_fran_lime_status_done.csv\",\n",
    "            \"assets\": {\n",
    "                \"census_blocks_shp\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\GBFS_Census_Tract\\San_Fran_Lime\\tl_2024_06_tabblock20.shp\",\n",
    "                \"centerline_streets_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\San_Fran_Lime\\Centerline.csv\",\n",
    "                \"bike_lanes_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\San_Fran_Lime\\Bikelane.csv\",\n",
    "                \"centroid_tract_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\San_Fran_Baywheels\\centroid_tract_ca.csv\",\n",
    "            },\n",
    "            \"safety_epsg\": 26910, \"idle_decimals\": 4\n",
    "        },\n",
    "        \"SF_SPIN_DOCKLESS\": {\n",
    "            \"city\": \"San Francisco\", \"vendor\": \"Spin\", \"tag\": \"sf_spin\",\n",
    "            \"default_output_dir\": \"SF_SPIN_DOCKLESS_FULL_RUN\",\n",
    "            \"raw_csv\": \"san_fran_spin_status_raw.csv\", \"done_csv\": \"san_fran_spin_status_done.csv\",\n",
    "            \"assets\": {\n",
    "                \"census_blocks_shp\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\GBFS_Census_Tract\\San_Fran_Lime\\tl_2024_06_tabblock20.shp\",\n",
    "                \"centerline_streets_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\San_Fran_Lime\\Centerline.csv\",\n",
    "                \"bike_lanes_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\San_Fran_Lime\\Bikelane.csv\",\n",
    "                \"centroid_tract_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\San_Fran_Baywheels\\centroid_tract_ca.csv\",\n",
    "            },\n",
    "            \"safety_epsg\": 26910, \"idle_decimals\": 4\n",
    "        },\n",
    "        \"SEATTLE_BIRD_DOCKLESS\": {\n",
    "            \"city\": \"Seattle\", \"vendor\": \"Bird\", \"tag\": \"seattle_bird\",\n",
    "            \"default_output_dir\": \"SEATTLE_BIRD_DOCKLESS_FULL_RUN\",\n",
    "            \"raw_csv\": \"seattle_bird_status_raw.csv\", \"done_csv\": \"seattle_bird_status_done.csv\",\n",
    "            \"assets\": {\n",
    "                \"census_blocks_shp\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Seattle_Bird\\Seattle Census Block\\tl_2024_53_tabblock20.shp\",\n",
    "                \"centerline_streets_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Seattle_Bird\\Seattle_Streets.shp\",\n",
    "                \"bike_lanes_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Seattle_Bird\\SDOT_Bike_Facilities_5512142703833213564.geojson\",\n",
    "                \"planned_bike_lanes_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Seattle_Bird\\Planned Seattle Bike Lanes\\Planned_Bike_Facilities.shp\",\n",
    "                \"centroid_tract_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Avalibility\\Seattle_Bird\\tl_2024_53_tract.shp\",\n",
    "            },\n",
    "            \"safety_epsg\": 2285, \n",
    "            \"safety_config\": {\"bike_lane_class_col\": \"CATEGORY\", \"protected_values\": [\"BKF-PBL\"], \"protected_match_mode\": \"exact\"},\n",
    "            \"idle_decimals\": 3 \n",
    "        },\n",
    "        \"SEATTLE_LIME_DOCKLESS\": {\n",
    "            \"city\": \"Seattle\", \"vendor\": \"Lime\", \"tag\": \"seattle_lime\",\n",
    "            \"default_output_dir\": \"SEATTLE_LIME_DOCKLESS_FULL_RUN\",\n",
    "            \"raw_csv\": \"seattle_lime_status_raw.csv\", \"done_csv\": \"seattle_lime_status_done.csv\",\n",
    "            \"assets\": {\n",
    "                \"census_blocks_shp\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Seattle_Bird\\Seattle Census Block\\tl_2024_53_tabblock20.shp\",\n",
    "                \"centerline_streets_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Seattle_Bird\\Seattle_Streets.shp\",\n",
    "                \"bike_lanes_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Seattle_Bird\\SDOT_Bike_Facilities_5512142703833213564.geojson\",\n",
    "                \"planned_bike_lanes_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Safety\\Seattle_Bird\\Planned Seattle Bike Lanes\\Planned_Bike_Facilities.shp\",\n",
    "                \"centroid_tract_path\": r\"D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Avalibility\\Seattle_Bird\\tl_2024_53_tract.shp\",\n",
    "            },\n",
    "            \"safety_epsg\": 2285,\n",
    "            \"safety_config\": {\"bike_lane_class_col\": \"CATEGORY\", \"protected_values\": [\"BKF-PBL\"], \"protected_match_mode\": \"exact\"},\n",
    "            \"idle_decimals\": 4\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if system_key not in SYSTEMS: raise ValueError(f\"Unknown system_key: {system_key}\")\n",
    "    preset = SYSTEMS[system_key]\n",
    "    output_tag = preset[\"tag\"]\n",
    "    \n",
    "    # Initialize Paths and Defaults\n",
    "    output_dir = output_dir or preset[\"default_output_dir\"]\n",
    "    step0_raw_csv_name = preset[\"raw_csv\"]\n",
    "    step0_done_csv_name = preset[\"done_csv\"]\n",
    "    p_assets = preset.get(\"assets\", {})\n",
    "    \n",
    "    census_blocks_shp = census_blocks_shp or p_assets.get(\"census_blocks_shp\")\n",
    "    centerline_streets_path = centerline_streets_path or p_assets.get(\"centerline_streets_path\")\n",
    "    bike_lanes_path = bike_lanes_path or p_assets.get(\"bike_lanes_path\")\n",
    "    planned_bike_lanes_path = planned_bike_lanes_path or p_assets.get(\"planned_bike_lanes_path\")\n",
    "    centroid_tract_path = centroid_tract_path or p_assets.get(\"centroid_tract_path\")\n",
    "    \n",
    "    safety_working_epsg = safety_working_epsg or preset.get(\"safety_epsg\", 26910)\n",
    "    p_safe_conf = preset.get(\"safety_config\", {})\n",
    "    if safety_bike_lane_class_col is None: safety_bike_lane_class_col = p_safe_conf.get(\"bike_lane_class_col\")\n",
    "    if safety_protected_values is None: safety_protected_values = p_safe_conf.get(\"protected_values\")\n",
    "    if \"protected_match_mode\" in p_safe_conf: safety_protected_match_mode = p_safe_conf[\"protected_match_mode\"]\n",
    "\n",
    "    p_idle_conf = preset.get(\"idle_config\", {})\n",
    "    if \"decimals\" in p_idle_conf: idle_rounding_decimals = p_idle_conf[\"decimals\"]\n",
    "\n",
    "    out_dir = Path(output_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if not Path(freebike_status_txt).exists(): raise FileNotFoundError(f\"Missing: {freebike_status_txt}\")\n",
    "    \n",
    "    raw_path = out_dir / step0_raw_csv_name\n",
    "    done_path = out_dir / step0_done_csv_name\n",
    "\n",
    "    results: Dict[str, Any] = {\"system_key\": system_key, \"city\": preset[\"city\"], \"vendor\": preset[\"vendor\"], \"output_dir\": str(out_dir)}\n",
    "\n",
    "    def _minmax(series: pd.Series) -> pd.Series:\n",
    "        s = pd.to_numeric(series, errors=\"coerce\")\n",
    "        mn, mx = s.min(skipna=True), s.max(skipna=True)\n",
    "        if pd.isna(mn) or pd.isna(mx) or mx <= mn: return pd.Series(0.0, index=s.index)\n",
    "        return (s - mn) / (mx - mn)\n",
    "    \n",
    "    # ============================================================\n",
    "    # STEP 0: PARSING\n",
    "    # ============================================================\n",
    "    rows = []\n",
    "    with Path(freebike_status_txt).open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip(): continue\n",
    "            try:\n",
    "                blob = json.loads(line)\n",
    "                ts = list(blob.keys())[0]\n",
    "                for entry in blob[ts]:\n",
    "                    if \"vehicle_types_available\" in entry:\n",
    "                        for vt in entry.get(\"vehicle_types_available\", []):\n",
    "                            entry[f\"vehicle_type_{vt.get('vehicle_type_id', 'unknown')}_count\"] = vt.get(\"count\", 0)\n",
    "                        del entry[\"vehicle_types_available\"]\n",
    "                    entry[\"timestamp\"] = ts\n",
    "                    rows.append(entry)\n",
    "            except: continue\n",
    "\n",
    "    raw_df = pd.DataFrame(rows)\n",
    "    if raw_df.empty: raise ValueError(\"No data found in text file.\")\n",
    "    \n",
    "    # ID Column Detection\n",
    "    id_candidates = [\"bike_id\", \"vehicle_id\", \"id\"]\n",
    "    vehicle_id_col = next((c for c in id_candidates if c in raw_df.columns), None)\n",
    "    if not vehicle_id_col: raise ValueError(f\"Could not find vehicle ID. Checked: {id_candidates}\")\n",
    "    \n",
    "    raw_df[\"timestamp\"] = pd.to_datetime(raw_df[\"timestamp\"], errors=\"coerce\")\n",
    "    if raw_df[\"timestamp\"].isna().mean() > 0.5:\n",
    "        raw_df[\"timestamp\"] = pd.to_datetime(raw_df[\"timestamp\"], unit=\"s\", errors=\"coerce\")\n",
    "\n",
    "    if drop_cols_if_present:\n",
    "        drop_now = [c for c in drop_cols_if_present if c in raw_df.columns]\n",
    "        if drop_now: raw_df = raw_df.drop(columns=drop_now)\n",
    "\n",
    "    if save_outputs: raw_df.to_csv(raw_path, index=False)\n",
    "\n",
    "    # Spatial Join\n",
    "    if not {\"lat\", \"lon\"}.issubset(raw_df.columns): raise ValueError(\"Input missing lat/lon\")\n",
    "    latlon = raw_df[[\"lat\", \"lon\"]].drop_duplicates()\n",
    "    gdf = gpd.GeoDataFrame(latlon, geometry=[Point(xy) for xy in zip(latlon.lon, latlon.lat)], crs=\"EPSG:4326\")\n",
    "    blocks = gpd.read_file(str(census_blocks_shp)).to_crs(epsg=blocks_target_epsg)\n",
    "    \n",
    "    b_col = \"GEOID20\" if \"GEOID20\" in blocks.columns else \"GEOID\"\n",
    "    joined = gpd.sjoin(gdf, blocks[[b_col, \"geometry\"]], how=\"left\", predicate=\"within\")\n",
    "    joined = joined.rename(columns={b_col: \"census_block\"})\n",
    "    \n",
    "    done_df = raw_df.merge(joined[[\"lat\", \"lon\", \"census_block\"]], on=[\"lat\", \"lon\"], how=\"left\")\n",
    "    \n",
    "    if fill_missing_with_census_api:\n",
    "        missing = done_df[done_df[\"census_block\"].isna()][[\"lat\", \"lon\"]].drop_duplicates()\n",
    "        if not missing.empty and len(missing) < 1000:\n",
    "            tqdm.pandas(desc=\"API Fill\")\n",
    "            def get_blk(r):\n",
    "                try:\n",
    "                    url = f\"https://geocoding.geo.census.gov/geocoder/geographies/coordinates?x={r.lon}&y={r.lat}&benchmark={census_benchmark}&vintage={census_vintage}&format=json\"\n",
    "                    res = requests.get(url, timeout=5).json()\n",
    "                    return res[\"result\"][\"geographies\"][\"2020 Census Blocks\"][0][\"GEOID\"]\n",
    "                except: return None\n",
    "            missing[\"cb_new\"] = missing.progress_apply(get_blk, axis=1)\n",
    "            done_df = done_df.merge(missing, on=[\"lat\", \"lon\"], how=\"left\")\n",
    "            done_df[\"census_block\"] = done_df[\"census_block\"].fillna(done_df[\"cb_new\"])\n",
    "            done_df.drop(columns=[\"cb_new\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    done_df[\"census_block\"] = done_df[\"census_block\"].fillna(\"unknown\").astype(str)\n",
    "    done_df[\"census_tract\"] = done_df[\"census_block\"].str[:tract_digits]\n",
    "    \n",
    "    if time_start: done_df = done_df[done_df[\"timestamp\"] >= pd.to_datetime(time_start)]\n",
    "    if time_end: done_df = done_df[done_df[\"timestamp\"] < pd.to_datetime(time_end)]\n",
    "    if save_outputs: done_df.to_csv(done_path, index=False)\n",
    "\n",
    "    # ============================================================\n",
    "    # AVAILABILITY\n",
    "    # ============================================================\n",
    "    df_av = done_df.copy()\n",
    "    df_av[\"slot\"] = df_av[\"timestamp\"].dt.floor(availability_block_time_granularity)\n",
    "    if \"is_reserved\" not in df_av.columns: df_av[\"is_reserved\"] = 0\n",
    "    if \"is_disabled\" not in df_av.columns: df_av[\"is_disabled\"] = 0\n",
    "    av = df_av[(df_av[\"is_reserved\"]==0) & (df_av[\"is_disabled\"]==0)]\n",
    "    \n",
    "    av_blk = av.groupby([\"census_block\", \"slot\"]).size().reset_index(name=\"total_available\")\n",
    "    av[\"h_slot\"] = av[\"timestamp\"].dt.floor(availability_tract_time_granularity)\n",
    "    av_tract = av.groupby([\"census_tract\", \"h_slot\"]).size().reset_index(name=\"total_available\")\n",
    "    if availability_normalize:\n",
    "        av_tract[\"total_available_norm\"] = _minmax(av_tract[\"total_available\"]).round(5)\n",
    "\n",
    "    if save_outputs:\n",
    "        av_blk.to_csv(out_dir / f\"availability_block_{output_tag}.csv\", index=False)\n",
    "        av_tract.to_csv(out_dir / f\"availability_tract_hourly_raw_{output_tag}.csv\", index=False)\n",
    "\n",
    "    # ============================================================\n",
    "    # USAGE\n",
    "    # ============================================================\n",
    "    df_u = done_df.copy()\n",
    "    df_u[\"slot\"] = df_u[\"timestamp\"].dt.floor(usage_base_time_slot)\n",
    "    df_u[\"lat_r\"] = df_u[\"lat\"].round(usage_rounding_decimals)\n",
    "    df_u[\"lon_r\"] = df_u[\"lon\"].round(usage_rounding_decimals)\n",
    "    \n",
    "    cnts = df_u.groupby([\"census_block\", \"slot\", \"lat_r\", \"lon_r\"]).size().reset_index(name=\"cnt\")\n",
    "    prev = cnts.copy()\n",
    "    prev[\"slot\"] += pd.to_timedelta(usage_base_time_slot)\n",
    "    \n",
    "    flux = prev.merge(cnts, on=[\"census_block\", \"slot\", \"lat_r\", \"lon_r\"], how=\"outer\", suffixes=(\"_prev\", \"_curr\")).fillna(0)\n",
    "    flux[\"starts\"] = (flux[\"cnt_prev\"] - flux[\"cnt_curr\"]).clip(lower=0)\n",
    "    flux[\"ends\"] = (flux[\"cnt_curr\"] - flux[\"cnt_prev\"]).clip(lower=0)\n",
    "    \n",
    "    use_blk = flux.groupby([\"census_block\", \"slot\"])[[\"starts\", \"ends\"]].sum().reset_index()\n",
    "    use_blk[\"h_slot\"] = use_blk[\"slot\"].dt.floor(usage_aggregate_time_slot)\n",
    "    use_blk[\"census_tract\"] = use_blk[\"census_block\"].str[:tract_digits]\n",
    "    use_tract = use_blk.groupby([\"census_tract\", \"h_slot\"])[[\"starts\", \"ends\"]].sum().reset_index()\n",
    "    \n",
    "    if save_outputs:\n",
    "        use_blk.to_csv(out_dir / f\"usage_5min_block_{output_tag}.csv\", index=False)\n",
    "        use_tract.to_csv(out_dir / f\"usage_hourly_tract_{output_tag}.csv\", index=False)\n",
    "\n",
    "    # ============================================================\n",
    "    # IDLE TIME (Dynamic Detection + Robust Logic)\n",
    "    # ============================================================\n",
    "    df_i = done_df.copy()\n",
    "    \n",
    "    # 1. Column Detection\n",
    "    if idle_vehicle_type_col not in df_i.columns:\n",
    "        if \"vehicle_type\" in df_i.columns: idle_vehicle_type_col = \"vehicle_type\"\n",
    "        else:\n",
    "            idle_vehicle_type_col = \"vehicle_type_id\"\n",
    "            df_i[idle_vehicle_type_col] = idle_default_vehicle_type_for_missing\n",
    "    df_i[idle_vehicle_type_col] = df_i[idle_vehicle_type_col].fillna(idle_default_vehicle_type_for_missing)\n",
    "\n",
    "    df_i = df_i.sort_values([idle_vehicle_id_col, \"timestamp\"]).reset_index(drop=True)\n",
    "    df_i[\"h_bucket\"] = df_i[\"timestamp\"].dt.floor(idle_hour_bucket_freq)\n",
    "    \n",
    "    # 3. Robust Calculation (Count * 5mins)\n",
    "    idle_res = df_i.groupby([\"census_block\", \"h_bucket\", idle_vehicle_type_col]).size().reset_index(name=\"ping_count\")\n",
    "    idle_res[\"avg_idle_time_minutes\"] = idle_res[\"ping_count\"] * 5 \n",
    "    idle_res[\"num_idle_segments\"] = idle_res[\"ping_count\"] \n",
    "    \n",
    "    idle_res[\"census_tract\"] = idle_res[\"census_block\"].str[:tract_digits]\n",
    "    idle_tract = idle_res.groupby([\"census_tract\", \"h_bucket\", idle_vehicle_type_col])[[\"avg_idle_time_minutes\", \"num_idle_segments\"]].sum().reset_index()\n",
    "    \n",
    "    if idle_normalize:\n",
    "        idle_tract[\"avg_idle_time_minutes_norm\"] = _minmax(idle_tract[\"avg_idle_time_minutes\"]).round(5)\n",
    "\n",
    "    if save_outputs:\n",
    "        idle_res.to_csv(out_dir / f\"idle_summary_block_{output_tag}.csv\", index=False)\n",
    "        idle_tract.to_csv(out_dir / f\"idle_summary_tract_{output_tag}.csv\", index=False)\n",
    "        idle_tract.to_csv(out_dir / f\"idle_summary_tract_norm_{output_tag}.csv\", index=False)\n",
    "\n",
    "    # ============================================================\n",
    "    # SAFETY\n",
    "    # ============================================================\n",
    "    print(f\"DEBUG: Using Blocks SHP: {census_blocks_shp}\")\n",
    "    blocks = gpd.read_file(str(census_blocks_shp))\n",
    "    if safety_working_epsg: blocks = blocks.to_crs(epsg=int(safety_working_epsg))\n",
    "    \n",
    "    # Load Streets\n",
    "    st_path = Path(centerline_streets_path)\n",
    "    if st_path.suffix.lower() == \".csv\":\n",
    "        st_df = pd.read_csv(st_path)\n",
    "        st_df[\"geometry\"] = st_df[safety_centerline_wkt_col].apply(lambda x: wkt.loads(x) if isinstance(x, str) else None)\n",
    "        st = gpd.GeoDataFrame(st_df, geometry=\"geometry\", crs=safety_input_crs)\n",
    "    else: st = gpd.read_file(st_path).to_crs(safety_input_crs)\n",
    "    st = st.to_crs(blocks.crs)\n",
    "    \n",
    "    # Load Bike Lanes\n",
    "    bl_path = Path(bike_lanes_path)\n",
    "    if bl_path.suffix.lower() == \".csv\":\n",
    "        bl_df = pd.read_csv(bl_path)\n",
    "        bl_df[\"geometry\"] = bl_df[safety_bike_lane_wkt_col].apply(lambda x: wkt.loads(x) if isinstance(x, str) else None)\n",
    "        bl = gpd.GeoDataFrame(bl_df, geometry=\"geometry\", crs=safety_input_crs)\n",
    "    else: bl = gpd.read_file(bl_path).to_crs(safety_input_crs)\n",
    "    \n",
    "    if planned_bike_lanes_path:\n",
    "        pl = gpd.read_file(str(planned_bike_lanes_path)).to_crs(safety_input_crs)\n",
    "        bl = pd.concat([bl, pl], ignore_index=True)\n",
    "    bl = bl.to_crs(blocks.crs)\n",
    "\n",
    "    conf = preset.get(\"safety_config\", {})\n",
    "    col = conf.get(\"bike_lane_class_col\", \"class\")\n",
    "    vals = conf.get(\"protected_values\", [])\n",
    "    \n",
    "    prot = bl.iloc[0:0]\n",
    "    if col in bl.columns and vals:\n",
    "        prot = bl[bl[col].isin(vals)]\n",
    "\n",
    "    def get_len(lines, poly, name):\n",
    "        if lines.empty: return pd.DataFrame({\"census_block\": [], name: []})\n",
    "        c = gpd.overlay(lines, poly, how=\"intersection\")\n",
    "        c[\"l\"] = c.geometry.length\n",
    "        bid = \"GEOID20\" if \"GEOID20\" in poly.columns else \"GEOID\"\n",
    "        return c.groupby(bid)[\"l\"].sum().reset_index(name=name).rename(columns={bid: \"census_block\"})\n",
    "\n",
    "    s_len = get_len(st, blocks, \"st_len\")\n",
    "    b_len = get_len(bl, blocks, \"bl_len\")\n",
    "    p_len = get_len(prot, blocks, \"pr_len\")\n",
    "    \n",
    "    safe = s_len.merge(b_len, on=\"census_block\", how=\"left\").merge(p_len, on=\"census_block\", how=\"left\").fillna(0)\n",
    "    safe[\"census_tract\"] = safe[\"census_block\"].astype(str).str[:tract_digits]\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # CRITICAL FIX: Safe ID Matching + Padding\n",
    "    # ------------------------------------------------------------------\n",
    "    print(f\"DEBUG: Loading Centroids from {centroid_tract_path}\")\n",
    "    ct_path = Path(centroid_tract_path)\n",
    "    if ct_path.suffix.lower() == \".csv\":\n",
    "        ct = pd.read_csv(ct_path)\n",
    "    else:\n",
    "        ct = gpd.read_file(ct_path)\n",
    "    \n",
    "    cid = next((c for c in [\"GEOID\", \"GEOID20\", \"TRACTCE\", \"census_tract\"] if c in ct.columns), None)\n",
    "    \n",
    "    if cid:\n",
    "        base = pd.DataFrame({\"census_tract\": ct[cid]})\n",
    "        \n",
    "        # CLEANING FUNCTION: Removes .0 and strips whitespace\n",
    "        def clean_id(x):\n",
    "            s = str(x).strip()\n",
    "            if s.endswith(\".0\"): s = s[:-2]\n",
    "            return s\n",
    "            \n",
    "        base[\"census_tract\"] = base[\"census_tract\"].apply(clean_id)\n",
    "        safe[\"census_tract\"] = safe[\"census_tract\"].apply(clean_id)\n",
    "        \n",
    "        # AUTO-PADDING: Fix mismatched lengths (e.g., 10 vs 11 digits)\n",
    "        len_base = base[\"census_tract\"].str.len().mode()[0]\n",
    "        len_safe = safe[\"census_tract\"].str.len().mode()[0]\n",
    "        \n",
    "        if len_base < len_safe:\n",
    "            base[\"census_tract\"] = base[\"census_tract\"].str.zfill(int(len_safe))\n",
    "            print(\"DEBUG: Padded Base IDs with leading zero.\")\n",
    "        elif len_safe < len_base:\n",
    "            safe[\"census_tract\"] = safe[\"census_tract\"].str.zfill(int(len_base))\n",
    "            print(\"DEBUG: Padded Safe IDs with leading zero.\")\n",
    "            \n",
    "        print(f\"DEBUG: Sample Safe IDs: {safe['census_tract'].head().tolist()}\")\n",
    "        print(f\"DEBUG: Sample Base IDs: {base['census_tract'].head().tolist()}\")\n",
    "    else:\n",
    "        print(\"DEBUG: No valid ID column found in centroid file.\")\n",
    "        base = pd.DataFrame({\"census_tract\": []})\n",
    "\n",
    "    tract_safe = safe.groupby(\"census_tract\")[[\"st_len\", \"bl_len\", \"pr_len\"]].sum().reset_index()\n",
    "    final = base.merge(tract_safe, on=\"census_tract\", how=\"left\").fillna(0)\n",
    "    \n",
    "    final[\"ratio_bl\"] = np.where(final[\"st_len\"]>0, final[\"bl_len\"]/final[\"st_len\"], 0)\n",
    "    final[\"ratio_pr\"] = np.where(final[\"st_len\"]>0, final[\"pr_len\"]/final[\"st_len\"], 0)\n",
    "    \n",
    "    if save_outputs:\n",
    "        safe.to_csv(out_dir / f\"safety_block_{output_tag}.csv\", index=False)\n",
    "        final.to_csv(out_dir / f\"safety_tract_{output_tag}.csv\", index=False)\n",
    "\n",
    "    results[\"safety\"] = {\"tract\": final}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce1d4c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_37328\\80240554.py:229: FutureWarning: Parsed string \"2025-06-09 00:04:42 CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  raw_df[\"timestamp\"] = pd.to_datetime(raw_df[\"timestamp\"], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Using Blocks SHP: D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\GBFS_Census_Tract\\San_Fran_Lime\\tl_2024_06_tabblock20.shp\n",
      "DEBUG: Loading Centroids from D:\\Research Fellowship\\Summer Research Stuff\\Clean_Utilities\\Capacity\\San_Fran_Baywheels\\centroid_tract_ca.csv\n",
      "DEBUG: Padded Base IDs with leading zero.\n",
      "DEBUG: Sample Safe IDs: ['06075010101', '06075010101', '06075010101', '06075010101', '06075010101']\n",
      "DEBUG: Sample Base IDs: ['06001442700', '06001442800', '06037204920', '06037205110', '06037205120']\n"
     ]
    }
   ],
   "source": [
    "out_spin = run_sf_dockless_all_utilities_single_function( \n",
    "    system_key=\"SF_LIME_DOCKLESS\", # Change the system key accordingly SF_LIME_DOCKLESS or SF_SPIN_DOCKLESS\n",
    "    freebike_status_txt=r\"D:\\Research Fellowship\\Summer Research Stuff\\Collected Data\\Week 1\\09-June\\san_fran_lime_dkless_freebike_status_6_9.txt\",\n",
    "    output_dir=\"SF_LIME_FULL_RUN\", #Change the output directory name accordingly\n",
    "    time_start=\"2025-06-09 06:00:00\",\n",
    "    time_end=\"2025-06-09 12:00:00\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e91e06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
